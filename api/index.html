{"posts":[{"content":"查询可用网络接口 multipass networks 设置网络接口 multipass set local.bridged-network=waibu 设置镜像源 multipass set local.image.mirror=https://mirrors.cloud.tencent.com/ubuntu-cloud-images/ 安装docker 国内机安装源 bash &lt;(curl -sSL https://gitee.com/SuperManito/LinuxMirrors/raw/main/DockerInstallation.sh) 参考 https://github.com/canonical/multipass/issues/717 https://github.com/robertzhouxh/multipass-k8s-kubeedge https://www.cnblogs.com/qumogu/p/18245943 https://www.wxy97.com/archives/77 ","tags":[],"title":"multipass 命令","link":"https://toomi.pages.dev/post/multipass-ming-ling/","stats":{"text":"1 min read","time":29000,"words":91,"minutes":1},"dateFormat":"2024-06-22"},{"content":"为某些python脚本，没有requirements.txt，自动分析其依赖，进行自动安装. 下面介绍两种方法 pip工具 pigar仅打包当前目录下python文件的依赖。安装命令如下： pip install pigar # 安装工具 pigar generate #生成requirements.txt 自定义脚本 import os import ast import subprocess def dir_files(directory, extension): &quot;&quot;&quot;遍历目录内指定后缀&quot;&quot;&quot; file_list = [] for root, dirs, files in os.walk(directory): for file in files: if file.endswith(extension): if &quot;tempCodeRunnerFile&quot; not in file: # CodeRunner生成的临时文件忽略掉 file_list.append(os.path.join(root, file)) return file_list def parse_imports(filename): &quot;&quot;&quot;解析python源文件import了哪些库&quot;&quot;&quot; with open(filename, &quot;r&quot;, encoding=&quot;utf-8&quot;) as f: tree = ast.parse(f.read()) imports = set() for node in ast.iter_child_nodes(tree): if isinstance(node, ast.Import): for alias in node.names: imports.add(alias.name.split(&quot;.&quot;)[0]) elif isinstance(node, ast.ImportFrom): module_name = node.module.split(&quot;.&quot;)[0] if node.module else &quot;&quot; for alias in node.names: imports.add(module_name + &quot;.&quot; + alias.name.split(&quot;.&quot;)[0]) return list(imports) def pip_install(libraries): &quot;&quot;&quot;判断是否安装了库 没有就直接pip安装&quot;&quot;&quot; for library in libraries: library = library.split(&quot;.&quot;)[0] try: __import__(library) except ImportError: print(f&quot;{library} 没有安装，正在使用pip安装...&quot;) subprocess.call([&quot;pip&quot;, &quot;install&quot;, library]) if __name__ == &quot;__main__&quot;: # 指定项目文件夹路径 python_file_folders = [r&quot;D:\\\\project\\\\&quot;] # 查找项目中的Python文件 python_files = [] for folder in python_file_folders: python_files += dir_files(folder, &quot;.py&quot;) + dir_files(folder, &quot;.pyw&quot;) print(&quot;找到的Python文件：&quot;, python_files) # 解析Python文件中的依赖库 all_imports = [] for python_file in python_files: imports = parse_imports(python_file) all_imports += [library.split(&quot;.&quot;)[0] for library in imports] print(&quot;项目中使用的依赖库：&quot;, all_imports) # 自动安装依赖库 if all_imports: imports = set(all_imports) pip_install(imports) 参考 https://visionguide.readthedocs.io/zh-cn/latest/python/tool/requirements/ ","tags":[],"title":"python 自动安装依赖","link":"https://toomi.pages.dev/post/python-zi-dong-an-zhuang-yi-lai/","stats":{"text":"3 min read","time":132000,"words":428,"minutes":3},"dateFormat":"2024-04-21"},{"content":"拉取镜像 # gitlab-ce为稳定版本，后面不填写版本则默认pull最新latest版本 $ docker pull gitlab/gitlab-ce 配置 配置文件路径:/home/gitlab/config/gitlab.rb # 配置http协议所使用的访问地址,不加端口号默认为80 external_url 'http://192.168.1.101:8084' nginx['listen_port'] = 80 # 配置ssh协议所使用的访问地址和端口 gitlab_rails['gitlab_ssh_host'] = '192.168.1.101' gitlab_rails['gitlab_shell_ssh_port'] = 222 # 此端口是run时22端口映射的222端口 运行 $ docker run -d -p 443:443 -p 8084:80 -p 222:22 --name gitlab --restart always -v /home/gitlab/config:/etc/gitlab -v /home/gitlab/logs:/var/log/gitlab -v /home/gitlab/data:/var/opt/gitlab gitlab/gitlab-ce # -d：后台运行 # -p：将容器内部端口向外映射 # --name：命名容器名称 # -v：将容器内数据文件夹或者日志、配置等文件夹挂载到宿主机指定目录 ","tags":[],"title":"docker 安装 gitlab-ce","link":"https://toomi.pages.dev/post/docker-an-zhuang-gitlab-ce/","stats":{"text":"1 min read","time":55000,"words":209,"minutes":1},"dateFormat":"2024-01-08"},{"content":" https://www.knowprogram.com/java/java-synchronization-mcq/ https://www.sarthaks.com/2435862/which-of-the-following-is-not-an-inheritance-mapping-strategies https://www.sanfoundry.com/java-questions-answers-java8-features/ https://www.springboottutorial.com/spring-boot-interview-questions http://lucentblackboard.com/java-programming-examples/126-0-Variables%20and%20Loops https://techs4.blogspot.com/2012/04/ocjp-16-preparation-questions-and.html https://www.sanfoundry.com/advanced-java-questions-answers-design-patterns/ https://documentation.help/Spring-Framework-zh/pr01.html https://www.sanfoundry.com/advanced-java-questions-answers-annotations/ ","tags":[],"title":"hackerrank刷题资源","link":"https://toomi.pages.dev/post/hackerrank-shua-ti-zi-yuan/","stats":{"text":"1 min read","time":33000,"words":90,"minutes":1},"dateFormat":"2023-10-28"},{"content":"环境准备 用k3s搭建k8s环境 安装Docker环境，主要是用于打包项目成容器镜像（image） 在centos安装Java环境，包括maven，主要是打包spring项目，参考 2.1. Java环境安装命令 sudo yum install java-17-openjdk java-17-openjdk-devel 2.2. [java-17-openjdk] 是jre ，[java-17-openjdk-devel] 是编译工具，javac 2.3. maven 建议下载二进制包，解压，然后配置环境变量进行安装，不要用 yum install 的方式，因为这种方式会把额外安装 jre 版本，容易造成和项目java版本冲突 spring 项目代码，参考 安装docker私有仓库，后续需要把打包好的docker镜像上传到仓库后，再部署到k8s集群，使用命令docker run -d -p 5000:5000 --restart=always --name registry registry 进行安装参考 项目打包 通过拉取代码，用maven打包成jar，命令如下 $ git clone https://github.com/mkyong/spring-boot.git $ cd spring-boot-hello-world $ mvn spring-boot:run 将jar，打包成docker镜像，创建一个名为Dockerfile的文本文件，内容如下 from openjdk:17.0.2-jdk copy target/spring-boot-hello-world-1.0.jar /spring-boot-hello-world-1.0.jar CMD [&quot;java&quot;, &quot;-jar&quot;, &quot;/spring-boot-hello-world-1.0.jar&quot;] 然后再运行下面命令打包 docker build -t my-java-app . 查看生成的image镜像 docker image ls 运行docker容器，并测试外网访问 # 81是主机端口，外网访问，8080是容器端口，spring 项目监听的端口 docker run -p 81:8080 -it --rm --name my-running-app my-java-app 将镜像推送到私有仓库 docker push my-java-app 部署到k8s 构建k8s最小管理单元pod，创建名为my-java-app.yaml,内容如下 apiVersion: v1 kind: Pod metadata: name: spring-java-name # 指定 label，便于检索 ，这里要记住，后面配置外网访问service的时候需要用到 labels: app: spring-java-lable spec: containers: - name: spring-java-container # 指定镜像 image: my-java-app:latest #imagePullPolicy: Never # 指定暴露端口 ports: # 这里只是个用于标识，和容器内应用监听的端口要一致，如果不一致，以容器实际端口为准 - containerPort: 8080 运行kubectl apply -f my-java-app.yaml命令进行打包，然后运行kubectl get pods查看打包后的pod状态 创建NodePort类型 service ，将应用暴露出来供外网访问，创建一个名为java-service.yaml的文本文件，内容如下 apiVersion: v1 kind: Service metadata: name: my-java-nodeport-service spec: type: NodePort ports: - port: 8001 # Service 公开的端口，内部访问 targetPort: 8080 # Pod 中容器的端口 nodePort: 30001 # NodePort，需要在 30000-32767 范围内选择，外部访问 selector: app: spring-java-lable # 用于选择匹配的 Pod，基于pod label，而不是pod name 运行 kubectl apply -f java-service.yaml 命令，启动service服务，然后运行kubectl get svc 命令查看所有service,会看到一个TYPE 为 NodePort 的service 在k8s服务器上访问CLUSTER-IP:PORT , 如curl 10.43.90.53:8001 或者在外网访问外放ip和端口，如curl 外网ip:30001 即可测试服务访问性 ","tags":[],"title":"从零开始在linux k8s部署spring项目","link":"https://toomi.pages.dev/post/cong-ling-kai-shi-zai-linux-k8s-bu-shu-spring-xiang-mu/","stats":{"text":"4 min read","time":195000,"words":751,"minutes":4},"dateFormat":"2023-10-17"},{"content":"1. BEGIN 和BEGIN等效的命令还有“BEGIN WORK”及“START TRANSACTION”。 执行BEGIN命令并不会真的去引擎层开启一个事务，仅仅是为当前线程设定标记，表示为显式开启的事务。 当以BEGIN开启一个事务时，首先会去检查是否有活跃的事务还未提交，如果没有提交，则调用ha_commit_trans提交之前的事务，并释放之前事务持有的MDL锁。 执行 BEGIN 语句后，直到执行 查询 select 语句或者修改语句，Innodb 才会开启事务，并创建一致性视图（Read View） 2. START TRANSACTION WITH CONSISTENT SNAPSHOT 执行这个语句会立即开启事务时还会顺便创建一致性视图（Read View） 3. AUTOCOMMIT=0 关闭自动提交，则执行 查询 select 语句或者修改语句，Innodb 都会开启事务 4. 实验 在MySQL中，可以通过 SHOW ENGINE INNODB STATUS 命令，或者查询表 NFORMATION_SCHEMA.INNODB_TRX 查看事务状态，本文用的是查询NFORMATION_SCHEMA.INNODB_TRX 判断哪种命令开启了事务。 4.1 验证BEGIN 语句 步骤1：在窗口1开启会话，查询当前执行中的事务，看到当前并无执行中的事务 步骤2：在窗口2开启会话，只执行 BEGIN 语句，并返回窗口 1 查询当前执行中的事务 仍然看到当前并无执行中的事务，可以得出结果 只执行BEGIN 语句并不会立即开启事务 步骤3：执行查询语句 select * from account ; ，并返回窗口 1 查询当前执行中的事务 此时可以看到，有正在执行中的事务，可以得出结果，在执行BEGIN语句后，直到执行 查询 select 语句或者修改语句，Innodb 才会开启事务 同理，用上诉方法，可以证明START TRANSACTION WITH CONSISTENT SNAPSHOT语句执行后，会立即开启事务 参考链接 When does Innodb Start Transaction InnoDB 事务子系统介绍 一条MySQL查询语句经历了什么--事务 MySQL 如何查看当前最新事务ID ","tags":[],"title":"MySQL Innodb 开启事务时机 ","link":"https://toomi.pages.dev/post/mysql-innodb-kai-qi-shi-wu-shi-ji/","stats":{"text":"2 min read","time":116000,"words":522,"minutes":2},"dateFormat":"2023-10-03"},{"content":"Linux中有两个很容易混淆的概念,pagecache和buffercache,首先简单将一些Linux系统下内存的分布,使用free -m命令可以查看内存分布情况: [root@localhost ~]# free -m total used free shared buff/cache available Mem: 3770 1148 1252 17 1369 2377 Swap: 3967 0 3967 内存分布示意图: Page cache与Buffer Cache作用 page cache :页缓存,负责缓存逻辑数据。 buffer cache : 块缓存,负责缓存物理数据。 文件存储在磁盘上,存储最小单位是扇区,大小为0.5kb,磁盘读取数据到内存时不会一个一个扇区的读,这样效率太低,而是先将扇区组成小块,这个小块就是buffer cache，大小为1kb,然后将块组织成页(pagecache,4kb)。 读取数据具体流程是:先去读取buffer cache,如果cache空间不够，会通过一定的策略将一些过时或多次未被访问的buffer cache清空。程序在下一次访问磁盘时首先查看是否在buffer cache找到所需块，命中可减少访问磁盘时间。不命中时需重新读入buffer cache。 对buffer cache的写分为两种，一是直接写，这是程序在写buffer cache后也写磁盘，要读时从buffer cache上读，二是后台写，程序在写完buffer cache后并不立即写磁盘，因为有可能程序在很短时间内又需要写文件，如果直接写，就需多次写磁盘了。这样效率很低，而是过一段时间后由后台写，减少了多次访磁盘的时间。 Page cache在linux读写文件时，它用于缓存文件的逻辑内容，从而加快对磁盘上映像和数据的访问。具体说是加速对文件内容的访问，buffer cache缓存文件的具体内容——物理磁盘上的磁盘块，这是加速对磁盘的访问。 Buffer cache是由物理内存分配，Linux系统为提高内存使用率，会将空闲内存全分给buffer cache ，当其他程序需要更多内存时，系统会减少cache大小。 Page cache和Buffer cache的区别 磁盘的操作有逻辑级（文件系统）和物理级（磁盘块），这两种Cache就是分别缓存逻辑和物理级数据的。 假设我们通过文件系统操作文件，那么文件将被缓存到Page Cache，如果需要刷新文件的时候，Page Cache将交给Buffer Cache去完成，因为Buffer Cache就是缓存磁盘块的。 也就是说，直接去操作文件，那就是Page Cache区缓存，用dd等命令直接操作磁盘块，就是Buffer Cache缓存的东西。 Page cache实际上是针对文件系统的，是文件的缓存，在文件层面上的数据会缓存到page cache。文件的逻辑层需要映射到实际的物理磁盘，这种映射关系由文件系统来完成。当page cache的数据需要刷新时，page cache中的数据交给buffer cache，但是这种处理在2.6版本的内核之后就变的很简单了，没有真正意义上的cache操作。 Buffer cache是针对磁盘块的缓存，也就是在没有文件系统的情况下，直接对磁盘进行操作的数据会缓存到buffer cache中，例如，文件系统的元数据都会缓存到buffer cache中。 简单说来，page cache用来缓存文件数据，buffer cache用来缓存磁盘数据。在有文件系统的情况下，对文件操作，那么数据会缓存到page cache，如果直接采用dd等工具对磁盘进行读写，那么数据会缓存到buffer cache。 Buffer(Buffer Cache)以块形式缓冲了块设备的操作，定时或手动的同步到硬盘，它是为了缓冲写操作然后一次性将很多改动写入硬盘，避免频繁写硬盘，提高写入效率。 Cache(Page Cache)以页面形式缓存了文件系统的文件，给需要使用的程序读取，它是为了给读操作提供缓冲，避免频繁读硬盘，提高读取效率。 原文 https://www.cnblogs.com/Courage129/p/14311675.html ","tags":[],"title":"Linux系统中的Page cache和Buffer cache","link":"https://toomi.pages.dev/post/linux-xi-tong-zhong-de-page-cache-he-buffer-cache/","stats":{"text":"4 min read","time":233000,"words":1057,"minutes":4},"dateFormat":"2023-09-22"},{"content":"lru全称是least recently used，即最近最少使用。该算法一般应用于内存缓存场景，提供了在缓存数据规模增大时限制数据集合大小，优先保留热点数据的方案。lru在数据规模到达集合上限时会优先清理上次访问时间距当前最久的数据，这样的好处在于能够保证热点数据更长时间驻留，有利于提供系统的整体访问性能，减少从外存或磁盘读取数据的次数。 LinkedHashMap JDK自带集合工具中已经为我们提供了lru算法的解决方案，即LinkedHashMap。LinkedHashMap继承于HashMap，在原有hash数组的基础上追加了一个双向链表结构。我们知道，数组长于随机访问，短于增删数据，而链表正好相反，增删数据方便，但只能顺序访问。因此，这里其实结合了数组和链表各自的优势，实现高效的随机访问和元素增删，典型的空间换时间。 LinkedHashMap比HashMap多出两个私有属性。首先是布尔型的accessOrder，当其为true，则当前为lru算法模式，否则为传统模式，即按照插入顺序遍历读取。其次是header，作为链表的头节点构建链表。 header的声明类型Entry继承自HashMap的Entry，其主要实现逻辑是覆写了recordAccess方法，我们来看下源码: void recordAccess(HashMap&lt;K,V&gt; m) { LinkedHashMap&lt;K,V&gt; lm = (LinkedHashMap&lt;K,V&gt;)m; if (lm.accessOrder) { lm.modCount++; remove(); addBefore(lm.header); } } private void remove() { before.after = after; after.before = before; } private void addBefore(Entry&lt;K,V&gt; existingEntry) { after = existingEntry; before = existingEntry.before; before.after = this; after.before = this; } 当accessOrder设置为true，则将当前访问节点首先从原来位置移除，然后置为头节点。这一系列操作实现将最近访问的节点移动到头部，保证了在达到容量上限需要移除元素时能够最后被移除。因此，这里也是实现lru算法的核心逻辑。 在查询或插入时，会执行recordAccess方法进行访问元素位置的调整。特别的是，插入时执行父类方法put，然后执行子类重写的recordAccess方法进行元素调整，是一个模板方法模式。另外LinkedHashMap重写了addEntry方法，这个算是对HashMap逻辑的覆盖。 void addEntry(int hash, K key, V value, int bucketIndex) { createEntry(hash, key, value, bucketIndex); // Remove eldest entry if instructed, else grow capacity if appropriate Entry&lt;K,V&gt; eldest = header.after; if (removeEldestEntry(eldest)) { removeEntryForKey(eldest.key); } else { if (size &gt;= threshold) resize(2 * table.length); } } 这里createEntry将元素加入链表头部。然后检查是否需要删除最老的元素，存在两个问题：一是header.after表示的是链表尾部，这是双向链表的环形结构决定的；二是removeEldestEntry是一个空的实现，默认返回false。在使用时需要我们覆写逻辑,以决定什么时候可以进行老旧数据的删除逻辑。一个简单的实现如下: private static final int MAX_ENTRIES = 100; protected boolean removeEldestEntry(Map.Entry eldest) { return size() &gt; MAX_ENTRIES; } 原文 https://cescme.github.io/2017/03/08/LinkedHashMap%E5%AE%9E%E7%8E%B0lru%E5%8E%9F%E7%90%86/ ","tags":[],"title":"LinkedHashMap实现lru原理","link":"https://toomi.pages.dev/post/linkedhashmap-shi-xian-lru-yuan-li/","stats":{"text":"4 min read","time":189000,"words":800,"minutes":4},"dateFormat":"2023-08-28"},{"content":"持久性的数据是存储在外部磁盘上的，如果没有文件系统，访问这些数据就需要直接读写磁盘的sector，实在太不方便了。而文件系统存在的意义，就是能更有效的组织、管理和使用磁盘上的这些raw data。 所谓持久性（persistance），就是指即便面对困难、挑战，依然可以持续，对于设备来说，就是面对掉电、操作系统crash，依然可以保持数据的持久存在。 文件系统的组成 要管理，就得先划分，那按照什么粒度划分呢？因为磁盘上的数据要和内存交互，而内存通常是以4KB为单位的，所以从逻辑上，把磁盘按照4KB划分比较方便（称为一个block）。现在假设由一个文件系统管理64个blocks的一个磁盘区域： 那在这些blocks中应该存储些什么？文件系统的基础要素自然是文件，而文件作为一个数据容器的逻辑概念，本质上是一些字节构成的集合，这些字节就是文件的user data（对应下图的&quot;D&quot;）。 除了文件本身包含的数据，还有文件的访问权限、大小和创建时间等控制信息，这些信息被称为meta data。meta data可理解为是关于文件user data的data，这些meta data存储的数据结构就是inode（对应下图的&quot;I&quot;，有些文件系统称之为dnode或fnode）。 inode 不存储文件名，文件名存储在目录中，详细看下文 inode是&quot;index node&quot;的简称，在早期的Unix系统中，这些nodes是通过数组组织起来的，因此需要依靠index来索引数组中的node。假设一个inode占据256字节，那么一个4KB的block可以存放16个inodes，使用5个blocks可以存放80个inodes，也就是最多支持80个文件。 inode 的内容 inode 包含文件的元信息，具体来说有以下内容： 文件的字节数 文件拥有者的 User ID 文件用户组的 Group ID 文件的读、写、执行权限 文件的时间戳，共有三个： ctime 指 inode 上一次变动的时间 mtime 指文件内容上一次变动的时间 atime 指文件上一次打开的时间 链接数，即有多少文件名指向这个 inode 文件数据 block 的位置 可以用 stat 命令，查看某个文件的 inode 信息： [Linux]$ stat abc.txt File: 'abc.txt' Size: 7805 Blocks: 16 IO Block: 4096 regular file Device: fc09h/64521dInode: 152067 Links: 1 Access: (0664/-rw-rw-r--) Uid: ( 5000/shiyanlou) Gid: ( 5000/shiyanlou) Access: 2018-01-16 15:14:20.419663570 +0800 Modify: 2018-01-16 15:14:28.331874373 +0800 Change: 2018-01-16 15:14:28.331874373 +0800 Birth: - 总之，除了文件名以外的所有文件信息，都存在 inode 之中。至于为什么没有文件名，下文会有详细解释。 每个 inode 都有一个号码，操作系统用 inode 号码来识别不同的文件。 这里值得重复一遍，Unix/Linux 系统内部不使用文件名，而使用 inode 号码来识别文件。对于系统来说，文件名只是 inode 号码便于识别的别称或者绰号。 表面上，用户通过文件名打开文件。实际上，系统内部将这个过程分成三步： 系统找到文件名对应的 inode 号码 通过 inode 号码，获取 inode 信息 根据 inode 信息，找到文件数据所在的 block，读出数据。 使用 ls -l 命令查看文件名对应的 inode 号码： [Linux]$ ls -i abc.txt 152067 abc.txt bitmap 同内存分配一样，当有了新的数据产生时，我们需要选择一个空闲的block来存放数据，此外还需要一个空闲的inode。所以，需要追踪这些inodes和data blocks的分配和释放情况，以判断哪些是已用的，哪些是空闲的。 最简单的办法就是使用bitmap，包括记录inode使用情况的bitmap（对应下图的&quot;i&quot;），和记录data block使用情况的bitmap（对应下图的&quot;d&quot;）。空闲就标记为0，正在使用就标记为1。 superblock 因为block是最小划分单位，所以这里使用了两个blocks来分别存储inode bitmap和data block bitmap，每个block可以容纳的bits数量是4096*8。而这里我们只有80个inodes和56个data blocks，所以是绰绰有余的。 还剩下开头的一个block，这个block是留给superblock的（对应下图的&quot;S&quot;）。 这个superblock包含了一个文件系统所有的控制信息，比如文件系统中有多少个inodes和data blocks，inode的信息起始于哪个block（这里是第3个），可能还有一个区别不同文件系统类型的magic number。因此，superblock可理解为是文件系统的meta data。 文件寻址 这5个blocks中的80个inodes构成了一个inode table。假设一个inode的大小是256字节，现在我们要访问第32个文件，那就要先找到这个文件的控制信息，也就是第32个inode所在的磁盘位置。它应该在相对inode table起始地址的8KB处（32*256=8192），而inode table的起始地址是12KB，所以实际位置是20KB。 磁盘同内存不同，它在物理上不是按字节寻址的，而是按sector。一个sector的大小通常是512字节，因此换算一下就是第40个sector（20*1024/512）。 对于ext2/3/4文件系统，以上介绍的这些inode bitmap, data block bitmap和inode table，都可以通过一个名为&quot;dumpe2fs&quot;的工具来查看其在磁盘上的具体位置： 如果只需要查看inode的使用情况，那么直接使用&quot;df -i&quot;命令即可： 那通过inode如何找到对应的文件呢？根据大小的不同，一个文件需要占据若干个blocks，这些blocks可能是磁盘上连续分布的，也可能不是。所以，比较好的办法是使用指针，指针存储在inode中，一个指针指向一个block。 不过，在这个简化的示例里，并不需要C语言里那种指针，只需要一个block的编号就可以了。如果文件比较小，占有的blocks数目就比较少，那么一个256字节的inode就能存储这些指针。假设一个inode最多能包含12个指针，那么文件的大小不能超过48KB。 那如果超过了怎么办？可由inode先指向一个block，这个block再指向分散的data block，这种方法称为multi-level index。inode在指向中间block的同时，也可以直接指向data block。假设一个指针占据4个字节，那么一个中间block可存储1024个指针，二级index的寻址范围就可超过4MB，三级index则可超过4GB。 有没有觉得很像内存管理中的多级页表？事实上，它们的原理可以说是一样的，文件在磁盘上的实际block分布对等于内存的物理地址，而各级index就对等于内存的虚拟地址。 这种只使用block指针的方式（可被称为&quot;pointer-based&quot;）被ext2和ext3文件系统所采用，但它存在一个问题，对于占据多个data block的文件，需要较多的meta data。 另一种实现是使用一个block指针加上一个length来表示一组物理上连续的blocks（称为一个extent，其中length以block为单位计），一个文件则由若干个extents构成。这种&quot;extent-based&quot;的方式被后来的ext4文件系统所采用。 struct ext4_extent { __le32 ee_block; /* first logical block extent covers */ __le16 ee_len; /* number of blocks covered by extent */ ... }; 相比&quot;pointer-based&quot;而言，&quot;extent-based&quot;由于需要磁盘上连续的free space，灵活性稍差，适用于磁盘空闲空间比较充足的场景。 目录文件 Unix/Linux 系统中，目录（directory）也是一种文件。打开目录，实际上就是打开目录文件。 目录文件的结构非常简单，就是一系列目录项（dirent）的列表。每个目录项，由两部分组成：所包含文件的文件名，以及该文件名对应的 inode 号码。如果要查看文件的详细信息，就必须根据 inode 号码，访问 inode 节点，读取信息。 目录的执行权限 目录文件的读权限（r）和写权限（w）只是针对目录文件本身。由于目录文件内只有文件名和 inode 号码，所以如果只有读权限，只能获取文件名，无法获取其他信息。 [Linux]$ ls -l drw-rw-rw- 2 glenn glenn 4096 Dec 5 18:11 test/ [Linux]$ls -l test/ total 0 d????????? ? ? ? ? ? ./ d????????? ? ? ? ? ? ../ -????????? ? ? ? ? ? b.txt 要读取 inode 节点内的信息具有文件夹的执行权限（x）。 各级目录构成了访问文件的路径，不同于windows操作系统的drive分区，类Unix系统中的&quot;mount&quot;操作让所有的文件系统的挂载点都是一个路径，形成了树形结构。从抽象的角度，目录也可视作一种文件，只是这种文件比较特殊，它的user data存储的是该路径下的普通文件的inode编号。 所以，如下图所示的这样一个路径结构，假设要在&quot;/foo&quot;目录下创建一个文件&quot;bar.txt&quot;，那么需要从inode bitmap中分配一个空闲的inode，并在&quot;/foo&quot;这个目录中分配一个entry，以关联这个inode号。 接下来，我们要读取刚才创建的这个&quot;/foo/bar.txt&quot;文件，那么先得找到&quot;/&quot;这个目录文件的inode号（这必须是事先知道的，假设是2）。然后访问这个inode指向的data block，从中找到一个名为&quot;foo&quot;的entry，得到目录文件&quot;foo&quot;的inode号（假设是44）。重复此过程，按图索骥，直到找到文本文件&quot;bar.txt&quot;的inode号。 inode 的特殊作用 由于 inode 号码与文件名分离，这种机制导致了一些 Unix/Linux 系统特有的现象。 当文件名包含特殊字符，无法正常删除时。可以删除 inode 节点，就能直接删除文件。 移动文件或重命名文件，只是改变文件名，不影响 inode 号码。所以在 Linux 中移动文件不论大小基本秒成。 打开一个文件后，系统就以 inode 号码来识别文件，不再考虑文件名。因此，系统无法从 inode 号码得知文件名。 第 3 点使得在更新软件时可以不关闭、不重启软件。因为系统通过 inode 号码，识别运行中的文件，软件更新的时候，新版文件以同样的文件名，生成一个新的 inode，不会影响到运行中的文件。等到下一次运行这个软件的时候，文件名就自动指向新版文件，旧版文件的 inode 则被回收。 软链接存储 原文 https://zhuanlan.zhihu.com/p/106459445 https://gnu-linux.readthedocs.io/zh/latest/Chapter03/00_inode.html https://juejin.cn/post/7133154797468778503 http://chuquan.me/2022/05/01/understand-principle-of-filesystem/ ","tags":[],"title":"文件系统的原理","link":"https://toomi.pages.dev/post/zhuan-zai-wen-jian-xi-tong-de-yuan-li/","stats":{"text":"11 min read","time":634000,"words":2810,"minutes":11},"dateFormat":"2023-08-25"},{"content":" MyBatis作为一款ORM框架，主要通过XML定义Object，这就难免用到反射，虽然JDK自带的反射已经方便使用，但是MyBatis依然结合业务功能，将反射功能封装成一个更加易用的包，这个包就在reflection中。 一个POJO对象拥有两类属性，对象属性以及类信息，在MyBatis中分别通过通过MetaObject和MetaClass对应上述信息。 明白了上述信息，就能理解下面MyBatis反射包中几个关键类的作用： MetaClass : 保存了POJO的类相关信息，比如拥有的方法hasGetter()/hasSetter() MetaObject ： 保存了POJO对象的相关的信息，比如通过getter()获取值，通过setter()设置值 ObjectWrapper : 用来区别不同的POJO获取属性的不同的方式，比如数组通过索引获取：nums[index],Map通过key获取，POJO通过getter()获取 Relector: 这个类便是MyBatis的反射底层类，它简单的封装了JDK底层的反射，其他类都是调用此类进行反射操作。 小结 这里再次复习一下，MyBatis的反射使用入口是SystemMetaObject，它包含了整个反射包的基本配置。 在MyBatis的代码中，很少能看到静态类，静态工具方法等，基本都是通过对象“注入”到对应的其他对象中。这样当需要修改实现的时候，就能很好的维护。 在MyBatis中，首先使用MetaObject，MetaObject内部充当了一个简单工厂方法，用来分辨是处理Map还是处理List还是Bean， ObjectWrapper根据不同的实现，使用不同的方式获取底层的值，对于Bean来说，使用的是MetaClass， MetaClass包含了对应Bean的Class的各种元属性。其中MetaClass底层使用的Reflector对象 Reflector对象便是真正的反射实现者，其内部根据传入的Class，生成了各种信息，并且由于Class是通用的，因此MyBatis使用ReflectorFactory将其缓存起来。 ReflectorFactory 测试 public class Student { public Integer getId() { return 6; } public void setId(Integer id) { System.out.println(id); } public String getUserName() { return &quot;张三&quot;; } } @Test public void test02() throws Exception{ ReflectorFactory factory = new DefaultReflectorFactory(); Reflector reflector = factory.findForClass(Student.class); System.out.println(&quot;可读属性:&quot;+Arrays.toString(reflector.getGetablePropertyNames())); System.out.println(&quot;可写属性:&quot;+Arrays.toString(reflector.getSetablePropertyNames())); System.out.println(&quot;是否具有默认的构造器:&quot; + reflector.hasDefaultConstructor()); System.out.println(&quot;Reflector对应的Class:&quot; + reflector.getType()); } SystemMetaObject Student student = new Student(); student.setId(1); MetaObject metaObject = SystemMetaObject.forObject(car); Integer id = (Integer) metaObject.getValue(&quot;id&quot;); System.out.println(&quot; id: &quot; + id); 原文 https://dengchengchao.com/?p=1209 带你彻底搞懂MyBatis的底层实现之反射工具箱(reflector) https://www.cayzlh.com/wiki/mybatis-technology-insider/part11/ https://jimmy2angel.github.io/2017/04/22/mybatis-mapper/ ","tags":[],"title":"MyBatis 的反射","link":"https://toomi.pages.dev/post/mybatis-de-fan-she/","stats":{"text":"3 min read","time":161000,"words":662,"minutes":3},"dateFormat":"2023-08-22"},{"content":"简介 OGNL是一种表达式语言，用于获取对象的属性或者调用方法。它在Mybatis/Struts（以及漏洞）等涉及到模版的场景中经常使用。语法这里就不讲了，需要注意的是 OGNL大部分场景是取属性，不建议new对象 OGNL不支持类似Groovy的safe-null，比如user?.role?.name这样的问号表达式，而SpingEL是支持的 OGNL定位是XML中的胶水，调试方便程度肯定不如Java，因此尽可能不折腾 在Mybatis中封装了如下的OGNL，测试用例如下 Object a = OgnlCache.getValue(&quot;a + 1&quot;, Collections.singletonMap(&quot;a&quot;, 10)); System.out.println(&quot;a = &quot; + a);//返回 11 只要在OgnlCache.getValue中打上了断点，所有的动态SQL生成过程均可以看见细节 模板替换 package com.homejim.mybatis; import com.homejim.myabtis.GenericTokenParser; import com.homejim.myabtis.TokenHandler; import org.junit.Ignore; import org.junit.Test; import java.util.HashMap; import java.util.Map; import static org.junit.Assert.assertEquals; public class GenericTokenParserTest { public static class VariableTokenHandler implements TokenHandler { private Map&lt;String, String&gt; variables; public VariableTokenHandler(Map&lt;String, String&gt; variables) { this.variables = variables; } @Override public String handleToken(String content) { return variables.get(content); } } @Test public void simpleTest() { GenericTokenParser parser = new GenericTokenParser(&quot;${&quot;, &quot;}&quot;, new VariableTokenHandler(new HashMap&lt;String, String&gt;() { { put(&quot;driver&quot;, &quot;com.mysql.jdbc.Driver&quot;); put(&quot;url&quot;, &quot;jdbc:mysql://localhost:3306/mybatis&quot;); put(&quot;username&quot;, &quot;root&quot;); put(&quot;password&quot;, &quot;aaabbb&quot;); } })); // 测试单个解析 assertEquals(&quot;com.mysql.jdbc.Driver&quot;, parser.parse(&quot;${driver}&quot;)); // 多个一起测试 assertEquals(&quot;驱动=com.mysql.jdbc.Driver，地址=jdbc:mysql://localhost:3306/mybatis，用户名=root&quot;, parser.parse(&quot;驱动=${driver}，地址=${url}，用户名=${username}&quot;)); } @Test public void shouldDemonstrateGenericTokenReplacement() { GenericTokenParser parser = new GenericTokenParser(&quot;${&quot;, &quot;}&quot;, new VariableTokenHandler(new HashMap&lt;String, String&gt;() { { put(&quot;first_name&quot;, &quot;James&quot;); put(&quot;initial&quot;, &quot;T&quot;); put(&quot;last_name&quot;, &quot;Kirk&quot;); put(&quot;var{with}brace&quot;, &quot;Hiya&quot;); put(&quot;&quot;, &quot;&quot;); } })); assertEquals(&quot;James T Kirk reporting.&quot;, parser.parse(&quot;${first_name} ${initial} ${last_name} reporting.&quot;)); assertEquals(&quot;Hello captain James T Kirk&quot;, parser.parse(&quot;Hello captain ${first_name} ${initial} ${last_name}&quot;)); assertEquals(&quot;James T Kirk&quot;, parser.parse(&quot;${first_name} ${initial} ${last_name}&quot;)); assertEquals(&quot;JamesTKirk&quot;, parser.parse(&quot;${first_name}${initial}${last_name}&quot;)); assertEquals(&quot;{}JamesTKirk&quot;, parser.parse(&quot;{}${first_name}${initial}${last_name}&quot;)); assertEquals(&quot;}JamesTKirk&quot;, parser.parse(&quot;}${first_name}${initial}${last_name}&quot;)); assertEquals(&quot;}James{{T}}Kirk&quot;, parser.parse(&quot;}${first_name}{{${initial}}}${last_name}&quot;)); assertEquals(&quot;}James}T{Kirk&quot;, parser.parse(&quot;}${first_name}}${initial}{${last_name}&quot;)); assertEquals(&quot;}James}T{Kirk&quot;, parser.parse(&quot;}${first_name}}${initial}{${last_name}&quot;)); assertEquals(&quot;}James}T{Kirk{{}}&quot;, parser.parse(&quot;}${first_name}}${initial}{${last_name}{{}}&quot;)); assertEquals(&quot;}James}T{Kirk{{}}&quot;, parser.parse(&quot;}${first_name}}${initial}{${last_name}{{}}${}&quot;)); assertEquals(&quot;{$$something}JamesTKirk&quot;, parser.parse(&quot;{$$something}${first_name}${initial}${last_name}&quot;)); assertEquals(&quot;${&quot;, parser.parse(&quot;${&quot;)); assertEquals(&quot;${\\\\}&quot;, parser.parse(&quot;${\\\\}&quot;)); assertEquals(&quot;Hiya&quot;, parser.parse(&quot;${var{with\\\\}brace}&quot;)); assertEquals(&quot;&quot;, parser.parse(&quot;${}&quot;)); assertEquals(&quot;}&quot;, parser.parse(&quot;}&quot;)); assertEquals(&quot;Hello ${ this is a test.&quot;, parser.parse(&quot;Hello ${ this is a test.&quot;)); assertEquals(&quot;Hello } this is a test.&quot;, parser.parse(&quot;Hello } this is a test.&quot;)); assertEquals(&quot;Hello } ${ this is a test.&quot;, parser.parse(&quot;Hello } ${ this is a test.&quot;)); } @Test public void shallNotInterpolateSkippedVaiables() { GenericTokenParser parser = new GenericTokenParser(&quot;${&quot;, &quot;}&quot;, new VariableTokenHandler(new HashMap&lt;String, String&gt;())); assertEquals(&quot;${skipped} variable&quot;, parser.parse(&quot;\\\\${skipped} variable&quot;)); assertEquals(&quot;This is a ${skipped} variable&quot;, parser.parse(&quot;This is a \\\\${skipped} variable&quot;)); assertEquals(&quot;null ${skipped} variable&quot;, parser.parse(&quot;${skipped} \\\\${skipped} variable&quot;)); assertEquals(&quot;The null is ${skipped} variable&quot;, parser.parse(&quot;The ${skipped} is \\\\${skipped} variable&quot;)); } @Ignore(&quot;Because it randomly fails on Travis CI. It could be useful during development.&quot;) @Test(timeout = 1000) public void shouldParseFastOnJdk7u6() { // issue #760 GenericTokenParser parser = new GenericTokenParser(&quot;${&quot;, &quot;}&quot;, new VariableTokenHandler(new HashMap&lt;String, String&gt;() { { put(&quot;first_name&quot;, &quot;James&quot;); put(&quot;initial&quot;, &quot;T&quot;); put(&quot;last_name&quot;, &quot;Kirk&quot;); put(&quot;&quot;, &quot;&quot;); } })); StringBuilder input = new StringBuilder(); for (int i = 0; i &lt; 10000; i++) { input.append(&quot;${first_name} ${initial} ${last_name} reporting. &quot;); } StringBuilder expected = new StringBuilder(); for (int i = 0; i &lt; 10000; i++) { expected.append(&quot;James T Kirk reporting. &quot;); } assertEquals(expected.toString(), parser.parse(input.toString())); } } 参考 https://miao1007.github.io/gitbook/mybatis/dynamic-sql/ognl.html ","tags":[],"title":"Mybatis OGNL","link":"https://toomi.pages.dev/post/mybatis-ognl/","stats":{"text":"6 min read","time":301000,"words":893,"minutes":6},"dateFormat":"2023-08-22"},{"content":"第一階段：@SpringBootTest 去年產品剛開始開發時，使用 @SpringBootTest 去測試全部 Mapper 的總時間都還在可以接受的範圍。 今年年初由於產品功能慢慢變多，使用 @SpringBootTest 去測試全部 Mapper 的總時間越拉越長，於是開始研究降低總時間的方式。 第二階段：@MyBatisTest 研究的過程中，發現 MyBatis 其實自己有出測試框架，只要在測試中把 @SpringBootTest 換成 @MyBatisTest，就可以大幅降低執行時間，因為 @MyBatisTest 不會像 @SpringBootTest 一樣要準備所有 context。 第三階段：SqlSessionFactory 想針對 Mapper 做純單元測試，只要做到啟動 H2 及取得 Mapper，不需要啟動 Spring Container的 Mapper 純單元測試：使用 SqlSessionFactory！ 代码来源 数据脚本 schema.sql CREATE TABLE cars(id INT PRIMARY KEY AUTO_INCREMENT, name VARCHAR(150), price INT); data.sql INSERT INTO cars(name, price) VALUES('Audi', 52642); INSERT INTO cars(name, price) VALUES('Mercedes', 57127); INSERT INTO cars(name, price) VALUES('Skoda', 9000); INSERT INTO cars(name, price) VALUES('Volvo', 29000); INSERT INTO cars(name, price) VALUES('Bentley', 350000); INSERT INTO cars(name, price) VALUES('Citroen', 21000); INSERT INTO cars(name, price) VALUES('Hummer', 41400); INSERT INTO cars(name, price) VALUES('Volkswagen', 21600); 工具类 MybatisFactory import java.util.List; import org.apache.ibatis.mapping.Environment; import org.apache.ibatis.session.Configuration; import org.apache.ibatis.session.SqlSessionFactory; import org.apache.ibatis.session.SqlSessionFactoryBuilder; import org.apache.ibatis.transaction.jdbc.JdbcTransactionFactory; import org.springframework.jdbc.datasource.embedded.EmbeddedDatabaseBuilder; import org.springframework.jdbc.datasource.embedded.EmbeddedDatabaseType; /** * 构建mybatis测试环境 */ public class MybatisFactory { /** * h2 数据源构建 */ EmbeddedDatabaseBuilder embeddedDatabaseBuilder = new EmbeddedDatabaseBuilder(); /** * 添加启动脚本 * * @param script */ public void addInitScript(String script) { embeddedDatabaseBuilder.addScript(script); } public MybatisFactory() { // h2 数据源构建 embeddedDatabaseBuilder .setType(EmbeddedDatabaseType.H2) .setName(&quot;test&quot;); } public &lt;T&gt; SqlSessionFactory getSqlSessionFactory(List&lt;Class&lt;T&gt;&gt; mappers) { Environment environment = new Environment(&quot;development&quot;, new JdbcTransactionFactory(), embeddedDatabaseBuilder.build()); Configuration configuration = new Configuration(environment); for (Class&lt;T&gt; mapper : mappers) { configuration.addMapper(mapper); } return new SqlSessionFactoryBuilder().build(configuration); } } 测试 import java.util.List; import org.apache.ibatis.session.SqlSession; import org.apache.ibatis.session.SqlSessionFactory; import org.junit.Test; public class FactoryTest { @Test public void testCar() { MybatisFactory mybatisFactory = new MybatisFactory(); mybatisFactory.addInitScript(&quot;db/schema.sql&quot;); mybatisFactory.addInitScript(&quot;db/data.sql&quot;); SqlSessionFactory sqlSessionFactory = mybatisFactory.getSqlSessionFactory(List.of(CarMapper.class)); try (SqlSession session = sqlSessionFactory.openSession()) { CarMapper carMapper = session.getMapper(CarMapper.class); Car car = carMapper.getCarByName(&quot;Audi&quot;); assert car.getPrice() == 52642; } } } Configuration 和 Environment 当使用MyBatis时，Configuration 和 Environment 是两个重要的概念，它们之间存在特定的关系，用于配置和管理数据库连接等运行环境。 Configuration（配置）： Configuration 类是MyBatis的核心配置类，负责管理所有配置信息，包括数据库连接、映射关系、插件等。每个Configuration对象对应一个MyBatis配置，通过此对象，可以访问和修改各项配置。 Environment（环境）： Environment 类代表MyBatis的运行环境，包括数据库连接信息和事务管理器等。每个Environment对象关联一个数据库连接池（DataSource）和一个事务管理器（TransactionFactory）。 两者之间的关系如下： 每个Configuration对象可以与一个或多个Environment对象关联，表示MyBatis可以在不同环境中运行，例如开发、测试、生产等。 每个Environment对象包含数据库连接池和事务管理器。数据库连接池（DataSource）用于获取连接，事务管理器（TransactionFactory）用于管理事务。每个Environment对象可以配置一个数据库连接池和一个事务管理器，确保正确管理连接和事务。 在MyBatis的配置文件中，您可以使用以下方式配置Environment： &lt;environments default=&quot;development&quot;&gt; &lt;environment id=&quot;development&quot;&gt; &lt;transactionManager type=&quot;JDBC&quot;/&gt; &lt;dataSource type=&quot;POOLED&quot;&gt; &lt;property name=&quot;driver&quot; value=&quot;com.mysql.jdbc.Driver&quot;/&gt; &lt;property name=&quot;url&quot; value=&quot;jdbc:mysql://localhost:3306/mydb&quot;/&gt; &lt;property name=&quot;username&quot; value=&quot;username&quot;/&gt; &lt;property name=&quot;password&quot; value=&quot;password&quot;/&gt; &lt;/dataSource&gt; &lt;/environment&gt; &lt;/environments&gt; 在上述配置中，通过元素配置了名为 &quot;development&quot; 的环境，关联了一个JDBC事务管理器和一个POOLED数据库连接池。元素指定了连接数据库所需的参数。 总而言之，Configuration是MyBatis的总体配置管理类，而Environment是定义数据库连接和事务管理的环境配置类。您可以根据需求配置多个Environment，然后在Configuration中选择适合的环境来运行。 参考 Mybatis Mapper 測試速度超進化 Spring EmbeddedDatabaseBuilder教程 ","tags":[],"title":"Mybatis Mapper 測試速度超進化","link":"https://toomi.pages.dev/post/mybatis-mapper-ce-shi-su-du-chao-jin-hua/","stats":{"text":"5 min read","time":281000,"words":1037,"minutes":5},"dateFormat":"2023-08-19"},{"content":"properties 和 yml 区别 properties 和 yml 都是 Spring Boot 支持的两种配置文件，它们可以看作是 Spring Boot 在不同时期的两款“产品”。在 Spring Boot 时代已经不需要使用 XML 文件格式来配置项目了，取而代之的是 properties 或 yml 文件。 properties 配置文件属于早期，也是目前创建 Spring Boot（2.x）项目时默认的配置文件格式，而 yml 可以看做是对 properties 配置文件的升级，属于 Spring Boot 的“新版”配置文件。 properties 翻译成中文是“属性”的意思，所以它在创建之初，就是用来在 Spring Boot 中设置属性的。 yml 是 YAML 是缩写，它的全称 Yet Another Markup Language，翻译成中文是“另一种标记语言”。 所以从二者的定义可以看出：它们的定位和层次是完全不同的，properties 只是用来设置一些属性配置的，而 yml 的格局和愿景更大，它的定位是“另一种标记语言”，所以从格局上来讲 yml 就甩 properties 好几条街。 Properties 解析 参考 mybatis 测试用例 ，链接 Properties props = Resources.getResourceAsProperties(&quot;org/apache/ibatis/databases/blog/blog-derby.properties&quot;); System.out.println(props.getProperty(&quot;driver&quot;)); yml 解析 在 Spring Framework 中，解析 YAML 文件通常使用 YamlPropertiesFactoryBean 工具类。这个工具类允许您将 YAML 文件中的配置属性解析为 Spring 的 Properties 对象，以便在应用程序中使用。 import org.springframework.beans.factory.config.YamlPropertiesFactoryBean; import org.springframework.core.io.ClassPathResource; import java.util.Properties; public class YamlParser { public static Properties parseYamlFile(String yamlFilePath) { YamlPropertiesFactoryBean factory = new YamlPropertiesFactoryBean(); factory.setResources(new ClassPathResource(yamlFilePath)); // 指定 YAML 文件路径 return factory.getObject(); } public static void main(String[] args) { Properties properties = parseYamlFile(&quot;application.yml&quot;); // 传入 YAML 文件名 if (properties != null) { System.out.println(&quot;Parsed properties from YAML file:&quot;); properties.forEach((key, value) -&gt; System.out.println(key + &quot; = &quot; + value)); } else { System.out.println(&quot;Failed to parse YAML file.&quot;); } } } ","tags":[],"title":"spring 解析 yaml 配置文件","link":"https://toomi.pages.dev/post/spring-jie-xi-yaml-pei-zhi-wen-jian/","stats":{"text":"2 min read","time":119000,"words":460,"minutes":2},"dateFormat":"2023-08-18"},{"content":"SSL的握手机制和数字签名机制完全不同， 单向验证机制，只验证服务端。 第一步：Visitor给出协议版本号、一个客户端随机数（Client random），以及客户端支持的加密方法。 第二步：Server确认双方使用的加密方法，以及一个服务器生成的随机数（Server random）。 第三步：Server发送数字证书给Visitor。 第四步：Visitor确认数字证书有效（查看证书状态且查询证书吊销列表），并使用信任的CA的公钥解密数字证书获得Server的公钥，然后生成一个新的46字节随机数（称为预备主密钥Pre-master secret），并使用Server的公钥加密预备主密钥发给Server。 第五步：Server使用自己的私钥，解密Visitor发来的预备主密钥。 第六步：Visitor和Server双方都具有了(客户端随机数+服务端随机数+预备主密钥)，它们两者都根据约定的加密方法，使用这三个随机数生成对称密钥——主密钥（也称为对话密钥session key），用来加密接下来的整个对话过程。 第七步：在双方验证完session key的有效性之后，SSL握手机制就算结束了。之后所有的数据只需要使用“对话密钥”加密即可，不再需要多余的加密机制。 注意： 1.在SSL握手机制中，需要三个随机数（客户端随机数+服务端随机数+预备主密钥）； 2.至始至终客户端和服务端只有一次非对称加密动作——客户端使用证书中获得的服务端公钥加密预备主密钥。 3.上述SSL握手机制的前提单向验证，无需验证客户端，如果需要验证客户端则可能需要客户端的证书或客户端提供签名等。 参考 加密、签名和SSL握手机制细节 也许，这样理解HTTPS更容易 ","tags":[],"title":"SSL的握手机制","link":"https://toomi.pages.dev/post/ssl-de-wo-shou-ji-zhi/","stats":{"text":"2 min read","time":106000,"words":508,"minutes":2},"dateFormat":"2023-07-20"},{"content":"1 前置知识 1.1 快照读 单纯的select操作，不包括上述 select ... lock in share mode, select ... for update。 Read Committed隔离级别：每次select都生成一个快照读 Read Repeatable隔离级别：开启事务后第一个select语句才是快照读的地方，而不是一开启事务就快照读 快照读的实现方式：undolog和多版本并发控制MVCC 1.2 当前读 当前读，读取的是最新版本，并且对读取的记录加锁，阻塞其他事务同时改动相同记录，避免出现安全问题。以下形式的SQL属于当前读: select...lock in share mode (共享读锁) select...for update update , delete , insert 例如，假设要update一条记录，但是另一个事务已经delete这条数据并且commit了，如果不加锁就会产生冲突。所以update的时候肯定要是当前读，得到最新的信息并且锁定相应的记录。 此处参考 https://www.jianshu.com/p/eb3f56565b42 2. 事务隔离级别和锁 对于读操作，mysql默认的隔离级别是 可重复读（Read Repeatable） 。普通的select查询是可重复读的，底层基于多版本并发控制MVCC的快照读 ，其他事务的修改操作对当前事务不可见，当前事务读的是历史版本。但是如果加上 for update 或者 in share mode 就会切换到当前读，能读到其他事务已经提交commit的数据。 其他隔离级别如下： ISOLATION LEVEL DIRTY READ NON-REPEATABLE READ PHANTOM READ READ_UNCOMMITTED allowed allowed allowed READ_COMMITTED prevented allowed allowed REPEATABLE_READ prevented prevented allowed SERIALIZABLE prevented prevented prevented 基于多版本并发控制MVCC实现的默认隔离REPEATABLE_READ并不能解决幻读（PHANTOM READ）问题，mysql解决幻读并不是在隔离级别层面上处理，而是通过锁层面处理，这是两个层面的问题。 这里，我需要对“幻读”做一个说明，来源自mysql45讲，第20讲： 在可重复读隔离级别下，普通的查询是快照读，是不会看到别的事务插入的数据的。因此，幻读在“当前读”下才会出现。 上面session B的修改结果，被session A之后的select语句用“当前读”看到，不能称为幻读。幻读仅专指“新插入的行”。 我自己的理解是，普通的行锁是锁住已经存在的行，而在可重复读的前提下插入的是第二次读的时候读到第一次读不存在的数据，所以这才是幻读（PHANTOM READ）。PHANTOM 在英文中就有幽灵的意思，意味幻觉，不存在的。 对于“不存在的数据”和“不符合过滤条件的数据” 要分开理解，在mysql中，如果对应字段没有索引，会遍历所有行，然后对所有遍历过的行进行加行锁，即使没有符合条件的行结果。 所以，幻读就是针对插入（insert）这种情况。 而mysql在可重复读隔离级别下，为了解决幻读，在行锁（record lock）的基础上，加入了间隙锁（gap lock）。行锁和间隙锁组合起来一起，就成为了临键锁（next-key lock）。 3. 覆盖索引 对于索引中包含了查询语句所有结果的情况，查询就不需要通过主键进行回表，对应索引就是覆盖索引。因为对于锁而已，它锁住的就是其搜索过的对象。如果没有索引而走全表扫描，锁住的就是全部行，走索引锁住的就是索引对象。 加锁规则如下，来源自mysql45讲，第21讲 加锁规则里面，包含了两个“原则”、两个“优化”和一个“bug” 原则1：加锁的基本单位是next-key lock。希望你还记得，next-key lock是前开后闭区间。 原则2：查找过程中访问到的对象才会加锁。 优化1：索引上的等值查询，给唯一索引加锁的时候，next-key lock退化为行锁。 优化2：索引上的等值查询，向右遍历时且最后一个值不满足等值条件的时候，next-key lock退化为间隙锁。 一个bug：唯一索引上的范围查询会访问到不满足条件的第一个值为止。 参考 A beginner’s guide to ACID and database transactions A Comprehensive (and Animated) Guide to InnoDB Locking 图解数据库锁 ","tags":[],"title":"理解 mysql 幻读问题","link":"https://toomi.pages.dev/post/li-jie-mysql-huan-du-wen-ti/","stats":{"text":"5 min read","time":246000,"words":1108,"minutes":5},"dateFormat":"2023-06-28"},{"content":" 在对应网页，点击油猴，添加新脚本 在编辑器添加如下代码 // ==UserScript== // @name 淘宝item // @namespace http://tampermonkey.net/ // @version 0.1 // @description try to take over the world! // @author You // @match https://sf-item.taobao.com/sf_item/713690244794.htm?spm=a213w.7398504.paiList.16.13154827Z13a5f&amp;track_id=a7eb667b-ee7a-436a-ba80-e9fab9215720 // @icon https://www.google.com/s2/favicons?sz=64&amp;domain=taobao.com // @require https://cdn.bootcss.com/jquery/2.2.1/jquery.js // @grant GM_xmlhttpRequest // ==/UserScript== (function() { 'use strict'; async function sleep(ms = 0) { return new Promise(r =&gt; setTimeout(r, ms)); } let urlList = []; var obj = {}; function lp_send_data(data) { GM_xmlhttpRequest({ method: &quot;POST&quot;, url: &quot;http://127.0.0.1:8080/doData&quot;, headers: {&quot;Content-Type&quot;: &quot;application/json;charset=UTF-8&quot;}, data: data, onload: function(response) { // window.close(); } }); } function getData(){ // 房产 obj.标题 = $('h1').text().replace(/\\s*/g,&quot;&quot;); // 标的物介绍 let bdwjs = $('#J_ItemDetailContent').text() ; // 竞买公告 let jmgg = $('#NoticeDetail').text() ; // 竞买须知 let jmxz = $('#ItemNotice').text() ; // 尾款支付说明 let wkzfsm = $('#J_CasePayInfo').text(); // 竞买记录 let jmjl = $('#J_Confirmation').text(); getMianJi(bdwjs); getMianJi(jmgg) ; getShiJian(jmgg); getAddr(bdwjs); getBuyer(jmjl); getPrice(); obj.url = window.location.href; console.log(obj) lp_send_data(JSON.stringify(obj)); } function getPrice(){ $('#J_HoverShow td').each((idx,ele)=&gt;{ var bmj = $(ele).find(&quot;span.pay-mark&quot;).first().text(); if(bmj.includes(&quot;变卖价&quot;) || bmj.includes(&quot;起拍价&quot;)){ obj.起拍价 = $(ele).find(&quot;span.J_Price&quot;).contents()[0].nodeValue; } if(bmj.includes(&quot;评估价&quot;)){ obj.评估价 = $(ele).find(&quot;span.J_Price&quot;).contents()[0].nodeValue; } }) } function getAddr(source){ const regex = /位置\\s+(.*?)\\s+地图/g; const match = regex.exec(source); if (match) { obj.地址 = match[1]; } } function getBuyer(source){ var regex = /姓名(.*)通过竞买号/g; var match = regex.exec(source); if (match) { obj.成交人 = match[1]; } match = source.match(/¥(\\d+(,\\d{3})*(\\.\\d+)?)\\b/); if (match) { obj.成交价 = parseFloat(match[1].replace(/,/g, '')) } } // 开始拍卖时间 function getShiJian(source){ var regex = /(\\d{4}年\\d{1,2}月\\d{1,2}日.*?\\d{1,2}时).*?\\s*(\\d{4}年\\d{1,2}月\\d{1,2}日.*?\\d{1,2}时)/; var match = source.match(regex); if (match) { const startTime = match[1]; const endTime = match[2]; obj.起拍时间 = startTime; obj.结束时间 = endTime; }else{ regex = /(\\d{4}年\\d{1,2}月\\d{1,2}日.*?).*?\\s*(\\d{4}年\\d{1,2}月\\d{1,2}日.*?)/;; match = source.match(regex); if (match) { const startTime = match[1]; const endTime = match[2]; obj.起拍时间 = startTime; obj.结束时间 = endTime; } } regex = /看样时间.*?(\\d{4}年\\d{1,2}月\\d{1,2}日).*/; match = source.match(regex); if (match) { obj.看样时间 = match[1]; } } function getMianJi(source){ var regex = /建筑面积.*?(\\d+\\.?\\d+).*?/; var result = source.match(regex); if(result){ obj.建筑面积 = result[1];; } regex = /套内建筑面积.*?(\\d+\\.?\\d).*?/; result = source.match(regex); if(result){ obj.套内面积 = result[1]; } regex = /土地面积.*?(\\d+\\.?\\d).*?/; result = source.match(regex); if(result){ obj.土地面积 = result[1]; } // console.log(`建筑面积：${buildingArea} 平方米，套内面积：${innerArea} 平方米`); } async function doList(){ $(&quot;.tab-menu li&quot;).each((idx,item)=&gt;{ urlList.push(item); }); for (let i = 0; i &lt; urlList.length; i++) { $(urlList[i]).click(); await sleep(1000); } getData(); } // 页面加载完成后，再执行脚本 window.addEventListener('load', function (evt) { doList(); }, false); // Your code here... })(); 关键代码段 引用\bJquery @require https://cdn.bootcss.com/jquery/2.2.1/jquery.js 授权发起异步请求 @grant GM_xmlhttpRequest 参考 https://pylab.me/blog/post/python_spider_with_tampermonkey_s1 ","tags":[],"title":"油猴Tampermonkey爬虫捞取阿里法拍","link":"https://toomi.pages.dev/post/you-hou-tampermonkey-pa-chong-lao-qu-a-li-fa-pai/","stats":{"text":"4 min read","time":223000,"words":692,"minutes":4},"dateFormat":"2023-05-31"},{"content":" 在对应网页，点击油猴，添加新脚本 在编辑器添加如下代码 // ==UserScript== // @name 淘宝item // @namespace http://tampermonkey.net/ // @version 0.1 // @description try to take over the world! // @author You // @match https://sf-item.taobao.com/sf_item/713690244794.htm?spm=a213w.7398504.paiList.16.13154827Z13a5f&amp;track_id=a7eb667b-ee7a-436a-ba80-e9fab9215720 // @icon https://www.google.com/s2/favicons?sz=64&amp;domain=taobao.com // @require https://cdn.bootcss.com/jquery/2.2.1/jquery.js // @grant GM_xmlhttpRequest // ==/UserScript== (function() { 'use strict'; async function sleep(ms = 0) { return new Promise(r =&gt; setTimeout(r, ms)); } let urlList = []; var obj = {}; function lp_send_data(data) { GM_xmlhttpRequest({ method: &quot;POST&quot;, url: &quot;http://127.0.0.1:8080/doData&quot;, headers: {&quot;Content-Type&quot;: &quot;application/json;charset=UTF-8&quot;}, data: data, onload: function(response) { // window.close(); } }); } function getData(){ // 房产 obj.标题 = $('h1').text().replace(/\\s*/g,&quot;&quot;); // 标的物介绍 let bdwjs = $('#J_ItemDetailContent').text() ; // 竞买公告 let jmgg = $('#NoticeDetail').text() ; // 竞买须知 let jmxz = $('#ItemNotice').text() ; // 尾款支付说明 let wkzfsm = $('#J_CasePayInfo').text(); // 竞买记录 let jmjl = $('#J_Confirmation').text(); getMianJi(bdwjs); getMianJi(jmgg) ; getShiJian(jmgg); getAddr(bdwjs); getBuyer(jmjl); getPrice(); obj.url = window.location.href; console.log(obj) lp_send_data(JSON.stringify(obj)); } function getPrice(){ $('#J_HoverShow td').each((idx,ele)=&gt;{ var bmj = $(ele).find(&quot;span.pay-mark&quot;).first().text(); if(bmj.includes(&quot;变卖价&quot;) || bmj.includes(&quot;起拍价&quot;)){ obj.起拍价 = $(ele).find(&quot;span.J_Price&quot;).contents()[0].nodeValue; } if(bmj.includes(&quot;评估价&quot;)){ obj.评估价 = $(ele).find(&quot;span.J_Price&quot;).contents()[0].nodeValue; } }) } function getAddr(source){ const regex = /位置\\s+(.*?)\\s+地图/g; const match = regex.exec(source); if (match) { obj.地址 = match[1]; } } function getBuyer(source){ var regex = /姓名(.*)通过竞买号/g; var match = regex.exec(source); if (match) { obj.成交人 = match[1]; } match = source.match(/¥(\\d+(,\\d{3})*(\\.\\d+)?)\\b/); if (match) { obj.成交价 = parseFloat(match[1].replace(/,/g, '')) } } // 开始拍卖时间 function getShiJian(source){ var regex = /(\\d{4}年\\d{1,2}月\\d{1,2}日.*?\\d{1,2}时).*?\\s*(\\d{4}年\\d{1,2}月\\d{1,2}日.*?\\d{1,2}时)/; var match = source.match(regex); if (match) { const startTime = match[1]; const endTime = match[2]; obj.起拍时间 = startTime; obj.结束时间 = endTime; }else{ regex = /(\\d{4}年\\d{1,2}月\\d{1,2}日.*?).*?\\s*(\\d{4}年\\d{1,2}月\\d{1,2}日.*?)/;; match = source.match(regex); if (match) { const startTime = match[1]; const endTime = match[2]; obj.起拍时间 = startTime; obj.结束时间 = endTime; } } regex = /看样时间.*?(\\d{4}年\\d{1,2}月\\d{1,2}日).*/; match = source.match(regex); if (match) { obj.看样时间 = match[1]; } } function getMianJi(source){ var regex = /建筑面积.*?(\\d+\\.?\\d+).*?/; var result = source.match(regex); if(result){ obj.建筑面积 = result[1];; } regex = /套内建筑面积.*?(\\d+\\.?\\d).*?/; result = source.match(regex); if(result){ obj.套内面积 = result[1]; } regex = /土地面积.*?(\\d+\\.?\\d).*?/; result = source.match(regex); if(result){ obj.土地面积 = result[1]; } // console.log(`建筑面积：${buildingArea} 平方米，套内面积：${innerArea} 平方米`); } async function doList(){ $(&quot;.tab-menu li&quot;).each((idx,item)=&gt;{ urlList.push(item); }); for (let i = 0; i &lt; urlList.length; i++) { $(urlList[i]).click(); await sleep(1000); } getData(); } window.addEventListener('load', function (evt) { doList(); }, false); // Your code here... })(); ","tags":[],"title":"油猴Tampermonkey爬虫","link":"https://toomi.pages.dev/post/you-hou-tampermonkey-pa-chong/","stats":{"text":"4 min read","time":210000,"words":643,"minutes":4},"dateFormat":"2023-05-31"},{"content":"简单迭代器 在python中，为了让类可以迭代访问，需要满足如下所示条件： 在类中需要实现__ iter__()方法，此方法需要返回一个迭代器； 为了类成为迭代器，可通过实现next()方法，此方法可以返回下一个可用的数据，直到没有可用数据，此时抛出StopIteration异常 按照上面的理论，可以实现一个简单的迭代器如下所示： class IterableServer: services = [ {'active': False, 'protocol': 'ftp', 'port': 21}, {'active': True, 'protocol': 'ssh', 'port': 22}, {'active': True, 'protocol': 'http', 'port': 80}, ] def __init__(self): self.current_pos = 0 def __iter__(self): return self def next(self): while self.current_pos &lt; len(self.services): service = self.services[self.current_pos] self.current_pos += 1 if service['active']: return service['protocol'], service['port'] raise StopIteration 可以比较容易分析出来，类IterableServer中存在一个services的列表，希望可以迭代访问其中‘active’为True的service，在上面的代码中，实现了next()方法，因此类IterableServer已经成为了一个迭代器，因此__ iter__()方法可以直接返回self。 在实际使用中，可以直接迭代访问，类似如下： for protocol, port in IterableServer(): print (&quot;service %s on port %d&quot; % (protocol, port)) 优雅迭代器 按照上面的理论可以简单实现一个迭代器，但是需要维护自己维护迭代的状态，比如current_pos，大多数情况下，此状态是没有额外用途的，因此一种更加优雅的实现方法可以如下所示： class IterableServer: services = [ {'active': False, 'protocol': 'ftp', 'port': 21}, {'active': True, 'protocol': 'ssh', 'port': 22}, {'active': True, 'protocol': 'http', 'port': 21}, ] def __iter__(self): for service in self.services: if service['active']: yield service['protocol'], service['port'] 可以看到在新的实现方法中，没有保存任何中间状态，只是在满足条件时，通过yield将对应的值返回即可 yield使用 yield会将函数转换为generator（生成器），函数转换为generator之后，调用此函数不会导致函数执行，而是返回一个迭代器。迭代访问(也可以手动调用next()方法，与之迭代访问等价)生成的迭代器时，才真正开始执行函数，每次执行都会中断与yield语句处，并返回yield指定的值。下一次迭代则从上一次迭代终止处继续执行，直到再次碰到yield语句。当函数执行完成后，会抛出StopIteration异常 def yield_func(): for i in xrange(0, 5): yield i for d in yield_func(): # 0, 1, 2, 3, 4 print d iter = yield_func() print iter.next() # 0 print iter.next() # 1 print iter.next() # 2 可以看到上面的函数添加了yield语句，转换成了generator，调用此函数会返回迭代器，如果直接循环迭代器访问，即采用for d in yield_func() 会依次获取相应的值0，1，2，3，4，如果手动执行next()方法，那么第一次会得到执行到yield 0处就返回，因此得到迭代值0，第二次从断点处继续执行，会执行到yield 1处返回，得到迭代值1，依次继续，直到函数执行结束 此时再理解前面的IterableServer就比较方便了，在调用IterableServer时，第一次访问的是‘protocol’为ftp的service，由于’active’为False，不会执行yield语句，继续执行，访问’protocol’为’ssh’的service，此时’active’为True，会返回此service的’protocol’与’port’值。当进行下一次迭代时，从此次结束的位置继续，会访问‘protocol’为’http’的service，此时’actice’为True，返回service对应的值。再继续下一次迭代，由于函数执行结束，此时当前方法会抛出StopIteration异常，结束迭代。 原文 https://hustyichi.github.io/2018/08/14/elegant-iterator-in-python/ ","tags":[],"title":"Python优雅迭代器实现","link":"https://toomi.pages.dev/post/python-you-ya-die-dai-qi-shi-xian/","stats":{"text":"4 min read","time":220000,"words":911,"minutes":4},"dateFormat":"2023-05-18"},{"content":"设计进程和线程，操作系统需要思考分配资源。最重要的 3 种资源是：计算资源（CPU）、内存资源和文件资源。早期的 OS 设计中没有线程，3 种资源都分配给进程，多个进程通过分时技术交替执行，进程之间通过管道技术等进行通信。 但是这样做的话，设计者们发现用户（程序员），一个应用往往需要开多个进程，因为应用总是有很多必须要并行做的事情。并行并不是说绝对的同时，而是说需要让这些事情看上去是同时进行的——比如图形渲染和响应用户输入。于是设计者们想到了，进程下面，需要一种程序的执行单位，仅仅被分配 CPU 资源，这就是线程。 线程设计出来后，因为只被分配了计算资源（CPU），因此被称为轻量级进程。被分配的方式，就是由操作系统调度线程。操作系统创建一个进程后，进程的入口程序被分配到了一个主线程执行，这样看上去操作系统是在调度进程，其实是调度进程中的线程。 这种被操作系统直接调度的线程，我们也成为内核级线程。另外，有的程序语言或者应用，用户（程序员）自己还实现了线程。相当于操作系统调度主线程，主线程的程序用算法实现子线程，这种情况我们称为用户级线程。Linux 的 PThread API 就是用户级线程，KThread API 则是内核级线程。 进程和线程的状态 旧的操作系统调度进程，没有线程；现代操作系统调度线程。 一个进程（线程）运行的过程，会经历以下 3 个状态： 进程（线程）创建后，就开始排队，此时它会处在“就绪”（Ready）状态； 当轮到该进程（线程）执行时，会变成“运行”（Running）状态； 当一个进程（线程）将操作系统分配的时间片段用完后，会回到“就绪”（Ready）状态。 有时候一个进程（线程）会等待磁盘读取数据，或者等待打印机响应，此时进程自己会进入“阻塞”（Block）状态。因为这时计算机的响应不能马上给出来，而是需要等待磁盘、打印机处理完成后，通过中断通知 CPU，然后 CPU 再执行一小段中断控制程序，将控制权转给操作系统，操作系统再将原来阻塞的进程（线程）置为“就绪”（Ready）状态重新排队。 而且，一旦一个进程（线程）进入阻塞状态，这个进程（线程）此时就没有事情做了，但又不能让它重新排队（因为需要等待中断），所以进程（线程）中需要增加一个“阻塞”（Block）状态。 注意，因为一个处于“就绪”（Ready）的进程（线程）还在排队，所以进程（线程）内的程序无法执行，也就是不会触发读取磁盘数据的操作，这时，“就绪”（Ready）状态无法变成阻塞的状态，因此下图中没有从就绪到阻塞的箭头。 而处于“阻塞”（Block）状态的进程（线程）如果收到磁盘读取完的数据，它又需要重新排队，所以它也不能直接回到“运行”（Running）状态，因此下图中没有从阻塞态到运行态的箭头。 区别 我们发现进程和线程是操作系统为了分配资源设计的两个概念，进程承接存储资源，线程承接计算资源。而进程包含线程，这样就可以做到进程间内存隔离。这是一个非常巧妙的设计，概念清晰，思路明确，你以后做架构的时候可以多参考这样的设计。 如果只有进程，或者只有线程，都不能如此简单的解决我们遇到的问题。 来源 《重学操作系统》 ","tags":[],"title":"进程和线程","link":"https://toomi.pages.dev/post/jin-cheng-he-xian-cheng/","stats":{"text":"4 min read","time":212000,"words":1043,"minutes":4},"dateFormat":"2023-05-04"},{"content":"B与bit 数据存储是以“字节”（Byte）为单位，数据传输大多是以“位”（bit，又名“比特”）为单位，一个位就代表一个0或1（即二进制），每8个位（bit，简写为b）组成一个字节（Byte，简写为B），是最小一级的信息单位。 字（Word） 在计算机中，一串数码作为一个整体来处理或运算的，称为一个计算机字，简称字。字通常分为若干个字节（每个字节一般是8位）。在存储器中，通常每个单元存储一个字。因此每个字都是可以寻址的。字的长度用位数来表示。 字长 计算机的每个字所包含的位数称为字长，计算的字长是指它一次可处理的二进制数字的数目。一般地，大型计算机的字长为32-64位，小型计算机为12-32位，而微型计算机为4-16位。字长是衡量计算机性能的一个重要因素。 本质区别 字节是寻址的最小单位。内存中两个紧挨着的字节，它们的内存地址差1。但是一个字节内的位，就没有地址的概念。你当然也可以定义一种计算机，每个位对应一个内存地址，但是在现代太另类了，估计没有人为你的计算机编程。 字是计算机一次处理数据的最大单位。多数情况下，这有几个含义：CPU的寄存器的长度是一个字；CPU一个指令最多从内存中读取的数据量就是一个字；最大的寻址空间，是2^字长（如果一个字是64位，那么最大的寻址空间就是2的64次方）。 计算机一般使用字节（byte）作为最小可寻址的内存单位，内存中的每个字节都有一个唯一的数字标识，也就是它的地址了，通常使用一个十六进制的数来表示。如果是占用多个字节内存的数据，则它的地址就是所使用字节中最小的地址。 虽然字节是最小可寻址单位，但是并不是 CPU 的最小读取单位。CPU 从内存中读取数据时需要通过 cache 来作为中间层，会根据目标地址首先在 cache 中找，如果找不到，就会首先从内存加载数据到 cache 中，然后再读取。单次从内存加载到 cache 的数据大小叫做 cache line，它的大小跟硬件有关，一般是 16 到 256 字节。 概念区别 字节（Byte）是计算机信息技术用于计量存储容量的一种计量单位，也表示一些计算机编程语言中的数据类型和语言字符。字节是二进制数据的单位。一个字节通常8位长。但是，一些老型号计算机结构使用不同的长度。为了避免混乱，在大多数国际文献中，使用词代替byte。 在计算机领域, 对于某种特定的计算机设计而言，字是用于表示其自然的数据单位的术语。在这个特定计算机中，字是其用来一次性处理事务的一个固定长度的位（bit）组。一个字的位数（即字长）是计算机系统结构中的一个重要特性。 字长在计算机结构和操作的多个方面均有体现。计算机中大多数寄存器的尺寸是一个字长。计算机处理的典型数值也可能是以字长为单位。CPU和内存之间的数据传送单位也通常是一个字长。还有而内存中用于指明一个存储位置的地址也经常是以字长为单位的。 内存对齐 尽管内存以字节为单位，现代处理器的内存子系统仅限于以字的大小的力度和对齐方式访问，处理器按照字节块的方式读取内存。一般按照2, 4,8, 16 字节为粒度进行内存读取。合理的内存对齐可以高效的利用硬件性能。 以4字节存取粒度的处理器为例，读取一个int变量（32bit 系统）, 处理器只能从4的倍数的地址开始。假如没有内存对齐机制，将一个int放在地址为1的位置。现在读取该int时，需要两次内存访问。第一次从0地址读取，剔除首个字节，第二次从4地址读取，只取首个字节；最后两下的两块数据合并入寄存器，需要大量工作。 参考 https://books.innohub.top/rustinfo/info/alignment https://worktile.com/kb/p/38205 https://wmf.im/p/%E5%86%85%E5%AD%98%E5%AF%B9%E9%BD%90/ ","tags":[],"title":"字和字节的区别","link":"https://toomi.pages.dev/post/zi-he-zi-jie-de-qu-bie/","stats":{"text":"5 min read","time":260000,"words":1234,"minutes":5},"dateFormat":"2023-04-26"},{"content":"设计思想 同步器的核心方法是acquire和release操作，其背后的思想也比较简洁明确。acquire操作是这样的： while (当前同步器的状态不允许获取操作) { 如果当前线程不在队列中，则将其插入队列 阻塞当前线程 } 如果线程位于队列中，则将其移出队列 release操作是这样的： 更新同步器的状态 if (新的状态允许某个被阻塞的线程获取成功){ 解除队列中一个或多个线程的阻塞状态 } 从这两个操作中的思想中我们可以提取出三大关键操作：同步器的状态变更、线程阻塞和释放、插入和移出队列。所以为了实现这两个操作，需要协调三大关键操作引申出来的三个基本组件： 同步器状态的原子性管理 线程阻塞与解除阻塞 队列的管理 由这三个基本组件，我们来看j.u.c是怎么设计的。 同步状态 AQS类使用单个int（32位）来保存同步状态，并暴露出getState、setState以及compareAndSet操作来读取和更新这个同步状态。其中属性state被声明为volatile，并且通过使用CAS指令来实现compareAndSetState，使得当且仅当同步状态拥有一个一致的期望值的时候，才会被原子地设置成新值，这样就达到了同步状态的原子性管理，确保了同步状态的原子性、可见性和有序性。 基于AQS的具体实现类（如锁、信号量等）必须根据暴露出的状态相关的方法定义tryAcquire和tryRelease方法，以控制acquire和release操作。当同步状态满足时，tryAcquire方法必须返回true，而当新的同步状态允许后续acquire时，tryRelease方法也必须返回true。这些方法都接受一个int类型的参数用于传递想要的状态。 阻塞 直到JSR166，阻塞线程和解除线程阻塞都是基于Java的内置管程，没有其它非基于Java内置管程的API可以用来达到阻塞线程和解除线程阻塞。唯一可以选择的是Thread.suspend和Thread.resume，但是它们都有无法解决的竞态问题，所以也没法用，目前该方法基本已被抛弃。具体不能用的原因可以官方给出的答复。 j.u.c.locks包提供了LockSupport类来解决这个问题。方法LockSupport.park阻塞当前线程直到有个LockSupport.unpark方法被调用。unpark的调用是没有被计数的，因此在一个park调用前多次调用unpark方法只会解除一个park操作。另外，它们作用于每个线程而不是每个同步器。一个线程在一个新的同步器上调用park操作可能会立即返回，因为在此之前可以有多余的unpark操作。但是，在缺少一个unpark操作时，下一次调用park就会阻塞。虽然可以显式地取消多余的unpark调用，但并不值得这样做。在需要的时候多次调用park会更高效。park方法同样支持可选的相对或绝对的超时设置，以及与JVM的Thread.interrupt结合 ，可通过中断来unpark一个线程。 队列 整个框架的核心就是如何管理线程阻塞队列，该队列是严格的FIFO队列，因此不支持线程优先级的同步。同步队列的最佳选择是自身没有使用底层锁来构造的非阻塞数据结构，业界主要有两种选择，一种是MCS锁，另一种是CLH锁。其中CLH一般用于自旋，但是相比MCS，CLH更容易实现取消和超时，所以同步队列选择了CLH作为实现的基础。 CLH队列实际并不那么像队列，它的出队和入队与实际的业务使用场景密切相关。它是一个链表队列，通过AQS的两个字段head（头节点）和tail（尾节点）来存取，这两个字段是volatile类型，初始化的时候都指向了一个空节点。如下图： 入队操作：CLH队列是FIFO队列，故新的节点到来的时候，是要插入到当前队列的尾节点之后。试想一下，当一个线程成功地获取了同步状态，其他线程将无法获取到同步状态，转而被构造成为节点并加入到同步队列中，而这个加入队列的过程必须要保证线程安全，因此同步器提供了一个CAS方法，它需要传递当前线程“认为”的尾节点和当前节点，只有设置成功后，当前节点才正式与之前的尾节点建立关联。入队操作示意图大致如下： 出队操作：因为遵循FIFO规则，所以能成功获取到AQS同步状态的必定是首节点，首节点的线程在释放同步状态时，会唤醒后续节点，而后续节点会在获取AQS同步状态成功的时候将自己设置为首节点。设置首节点是由获取同步成功的线程来完成的，由于只能有一个线程可以获取到同步状态，所以设置首节点的方法不需要像入队这样的CAS操作，只需要将首节点设置为原首节点的后续节点同时断开原节点、后续节点的引用即可。出队操作示意图大致如下： 条件队列 上一节的队列其实是AQS的同步队列，这一节的队列是条件队列，队列的管理除了有同步队列，还有条件队列。AQS只有一个同步队列，但是可以有多个条件队列。AQS框架提供了一个ConditionObject类，给维护独占同步的类以及实现Lock接口的类使用。 ConditionObject类实现了Condition接口，Condition接口提供了类似Object管程式的方法，如await、signal和signalAll操作，还扩展了带有超时、检测和监控的方法。ConditionObject类有效地将条件与其它同步操作结合到了一起。该类只支持Java风格的管程访问规则，这些规则中，当且仅当当前线程持有锁且要操作的条件（condition）属于该锁时，条件操作才是合法的。这样，一个ConditionObject关联到一个ReentrantLock上就表现的跟内置的管程（通过Object.wait等）一样了。两者的不同仅仅在于方法的名称、额外的功能以及用户可以为每个锁声明多个条件。 ConditionObject类和AQS共用了内部节点，有自己单独的条件队列。signal操作是通过将节点从条件队列转移到同步队列中来实现的，没有必要在需要唤醒的线程重新获取到锁之前将其唤醒。signal操作大致示意图如下： await操作就是当前线程节点从同步队列进入条件队列进行等待，大致示意图如下： 实现这些操作主要复杂在，因超时或Thread.interrupt导致取消了条件等待时，该如何处理。await和signal几乎同时发生就会有竞态问题，最终的结果遵照内置管程相关的规范。JSR133修订以后，就要求如果中断发生在signal操作之前，await方法必须在重新获取到锁后，抛出InterruptedException。但是，如果中断发生在signal后，await必须返回且不抛异常，同时设置线程的中断状态。 原文 https://www.cnblogs.com/iou123lg/p/9464385.html ","tags":[],"title":"AQS的设计和结构","link":"https://toomi.pages.dev/post/aqs-de-she-ji-he-jie-gou/","stats":{"text":"7 min read","time":409000,"words":1961,"minutes":7},"dateFormat":"2023-04-20"},{"content":"今天咱们来谈谈Java线程创建的一些细节问题： Java线程是如何与OS线程建立联系的 Java线程与OS线程共用一套线程状态吗 Java线程是如何做到创建与启动分开的 Java线程在JVM层面为什么要有JavaThread与OSThread Java线程为什么设计的时候要将创建与启动分开 先把这五个问题搞明白吧，其他的后面的文章再分享。本篇文章的观点都聚焦于Linux平台，不适用于所有平台。因为不同OS，底层差异还是挺大的。 Linux线程创建 线程能力是操作系统才有的，固Java的线程机制一定是基于OS的线程机制实现的，加上些许JVM自身的考虑在其中。这些考虑在哪能看到？JSR规范中。 上一段Linux平台下创建线程的代码 这样创建出来的线程，一般称为原生线程，或native thread。Java的线程实现其实就是将Linux下的线程机制基于JSR规范进行设计重组。如果我们了解Linux的线程机制，并搞明白了JVM是如何进行设计重组的。那么，Java的多线程，我们就算真正学明白了。 可以发现，跟Java创建线程明显不同的是：原生线程创建与运行是一体的，即线程创建完毕马上就运行。而Java中创建归创建，调用start线程才运行。 Java线程与原生线程之间是这样的关系：JavaThread-&gt;OSThread-&gt;native thread。后面会结合hotspot源码细讲。 Java线程创建 上一段创建Java线程的代码 从研究问题的角度，这段我们习以为常的代码要分成两部分来看：一、创建一个Java对象。注意，这一步只是单纯地创建一个Java对象，并没有什么特殊处理在里面。二、调用start方法让线程运行。我们上面提的几个问题，所有的秘密都在这一步中。接下来我结合hotspot源码将上面几个问题的答案分享给大家。 在 Java 中创建一个线程分为两步： Thread thread = new Thread(() -&gt; ...); thread.start(); 其中 new 操作只是调用了 Thread::init 方法做了一些初始化的操作，此时还没有跟操作系统交互。Java 的线程是直接与操作系统线程是 1:1 的，在 Thread::start 时会调用操作系统的 API 创建 native thread（例如 Linux 下会调用 glibc 的 pthread_create 创建）。 OS线程属性 操作系统（OS）线程的分离属性(detach)和连接(join)属性分别对应线程结束后，资源释放的操作. join属性的线程，需要同一个进程中的其他线程，获取线程终止的状态，并释放资源． detach属性的线程，在线程结束后，由系统释放资源． os线程创建的默认属性为joinable,在Java中通过new Thread创建的线程于OS而言都是分离线程。那么这个时候Java如何获取线程的执行结果呢？答案就是通过修改共享内存上变量的值。 我们都知道Java创建的线程其实是调用的操作系统的能力，那么从java中的线程对象到OS的线程对象，依次经过以下对应关系，而Linux线程操作的结果，就会存储在JVM的Java Thread对象中的_VM_result_2中，这个对象就可以理解为共享内存 参考 https://developer.aliyun.com/article/922947 https://www.ninan.life/%E6%89%8B%E5%86%99JVM%E7%AC%94%E8%AE%B0/ https://lotabout.me/books/Java-Concurrency/Cost-of-Thread/index.html ","tags":[],"title":"Java线程创建过程中的各种细节","link":"https://toomi.pages.dev/post/java-xian-cheng-chuang-jian-guo-cheng-zhong-de-ge-chong-xi-jie/","stats":{"text":"4 min read","time":204000,"words":931,"minutes":4},"dateFormat":"2023-04-19"},{"content":"Mutex Lock 互斥锁 MUTual-EXclude Lock，互斥锁。 它是理解最容易，使用最广泛的一种同步机制。顾名思义，被这个锁保护的临界区就只允许一个线程进入，其它线程如果没有获得锁权限，那就只能在外面等着。它使用得非常广泛，以至于大多数人谈到锁就是mutex。mutex是互斥锁，pthread里面还有很多锁，mutex只是其中一种。 mutex锁不是万能灵药 基本上所有的问题都可以用互斥的方案去解决，大不了就是慢点儿，但不要不管什么情况都用互斥，都能采用这种方案不代表都适合采用这种方案。而且这里所说的慢不是说mutex的实现方案比较慢，而是互斥方案影响的面比较大，本来不需要通过互斥就能让线程进入临界区，但用了互斥方案之后，就使这样的线程不得不等待互斥锁的释放，所以就慢了。甚至有些场合用互斥就很蛋疼，比如多资源分配，线程步调通知等。 如果是读多写少的场合，就比较适合读写锁(reader/writter lock)，如果临界区比较短，就适合空转锁(pin lock)... 预防死锁 如果要进入一段临界区需要多个mutex锁，那么就很容易导致死锁，单个mutex锁是不会引发死锁的。要解决这个问题也很简单，只要申请锁的时候按照固定顺序，或者及时释放不需要的mutex锁就可以。这就对我们的代码有一定的要求，尤其是全局mutex锁的时候，更需要遵守一个约定。 如果是全局mutex锁，我习惯将它们写在同一个头文件里。一个模块的文件再多，都必须要有两个umbrella header file。一个是整个模块的伞，外界使用你的模块的时候，只要include这个头文件即可。另一个用于给模块的所有子模块去include，然后这个头文件里面就放一些公用的宏啊，配置啊啥的，全局mutex放在这里就最合适了。这两个文件不能是同一个，否则容易出循环include的问题。如果有人写模块不喜欢写这样的头文件的，那现在就要改了。 然后我的mutex锁的命名规则就是：作用_mutex_序号，比如LinkListMutex_mutex_1,OperationQueue_mutex_2，后面的序号在每次有新锁的时候，就都加一个1。如果有哪个临界区进入的时候需要获得多个mutex锁的，我就按照序号的顺序去进行加锁操作(pthread_mutex_lock)，这样就能够保证不会出现死锁了。 还有另一种方案也非常有效，就是用pthread_mutex_trylock函数来申请加锁，这个函数在mutex锁不可用时，不像pthread_mutex_lock那样会等待。pthread_mutex_trylock在申请加锁失败时立刻就会返回错误:EBUSY(锁尚未解除)或者EINVAL(锁变量不可用)。一旦在trylock的时候有错误返回，那就把前面已经拿到的锁全部释放，然后过一段时间再来一遍。 当然也可以使用pthread_mutex_timedlock这个函数来申请加锁，这个函数跟pthread_mutex_trylock类似，不同的是，你可以传入一个时间参数，在申请加锁失败之后会阻塞一段时间等解锁，超时之后才返回错误。 Reader-Writter Lock 读写锁 前面mutex锁有个缺点，就是只要锁住了，不管其他线程要干什么，都不允许进入临界区。设想这样一种情况：临界区foo变量在被bar1线程读着，加了个mutex锁，bar2线程如果也要读foo变量，因为被bar1加了个互斥锁，那就不能读了。但事实情况是，读取数据不影响数据内容本身，所以即便被1个线程读着，另外一个线程也应该允许他去读。除非另外一个线程是写操作，为了避免数据不一致的问题，写线程就需要等读线程都结束了再写。 因此诞生了Reader-Writter Lock，有的地方也叫Shared-Exclusive Lock，共享锁。 Reader-Writter Lock的特性是这样的，当一个线程加了读锁访问临界区，另外一个线程也想访问临界区读取数据的时候，也可以加一个读锁，这样另外一个线程就能够成功进入临界区进行读操作了。此时读锁线程有两个。当第三个线程需要进行写操作时，它需要加一个写锁，这个写锁只有在读锁的拥有者为0时才有效。也就是等前两个读线程都释放读锁之后，第三个线程就能进去写了。总结一下就是，读写锁里，读锁能允许多个线程同时去读，但是写锁在同一时刻只允许一个线程去写。 认真区分使用场合，记得避免写线程饥饿 由于读写锁的性质，在默认情况下是很容易出现写线程饥饿的。因为它必须要等到所有读锁都释放之后，才能成功申请写锁。不过不同系统的实现版本对写线程的优先级实现不同。Solaris下面就是写线程优先，其他系统默认读线程优先。 比如在写线程阻塞的时候，有很多读线程是可以一个接一个地在那儿插队的(在默认情况下，只要有读锁在，写锁就无法申请，然而读锁可以一直申请成功，就导致所谓的插队现象)，那么写线程就不知道什么时候才能申请成功写锁了，然后它就饿死了。 总的来说，这样的锁建立之后一定要设置优先级，不然就容易出现写线程饥饿。而且读写锁适合读多写少的情况，如果读、写一样多，那这时候还是用mutex锁比较合理。 spin lock 空转锁 上面在给出mutex锁的实现代码的时候提到了这个spin lock，空转锁。它是互斥锁、读写锁的基础。在其它同步机制里condition variable、barrier等都有它的身影。我先说一下其他锁申请加锁的过程，你就知道什么是spin lock了。 互斥锁和读写锁在申请加锁的时候，会使得线程阻塞，阻塞的过程又分两个阶段，第一阶段是会先空转，可以理解成跑一个while循环，不断地去申请锁，在空转一定时间之后，线程会进入waiting状态(对的，跟进程一样，线程也分很多状态)，此时线程就不占用CPU资源了，等锁可用的时候，这个线程会被唤醒。 为什么会有这两个阶段呢？主要还是出于效率因素。 如果单纯在申请锁失败之后，立刻将线程状态挂起，会带来context切换的开销，但挂起之后就可以不占用CPU资源了，原属于这个线程的CPU时间就可以拿去做更加有意义的事情。假设锁在第一次申请失败之后就又可用了，那么短时间内进行context切换的开销就显得很没效率。 如果单纯在申请锁失败之后，不断轮询申请加锁，那么可以在第一时间申请加锁成功，同时避免了context切换的开销，但是浪费了宝贵的CPU时间。假设锁在第一次申请失败之后，很久很久才能可用，那么CPU在这么长时间里都被这个线程拿来轮询了，也显得很没效率。 于是就出现了两种方案结合的情况：在第一次申请加锁失败的时候，先不着急切换context，空转一段时间。如果锁在短时间内又可用了，那么就避免了context切换的开销，CPU浪费的时间也不多。空转一段时间之后发现还是不能申请加锁成功，那么就有很大概率在将来的不短的一段时间里面加锁也不成功，那么就把线程挂起，把轮询用的CPU时间释放出来给别的地方用。 事实上，spin lock在实现的时候，有一个__pthread_spin_count限制，如果空转次数超过这个限制，线程依旧会挂起（__shed_yield）。 使用场合 了解了空转锁的特性，我们就发现这个锁其实非常适合临界区非常短的场合，或者实时性要求比较高的场合。 由于临界区短，线程需要等待的时间也短，即便轮询浪费CPU资源，也浪费不了多少，还省了context切换的开销。 由于实时性要求比较高，来不及等待context切换的时间，那就只能浪费CPU资源在那儿轮询了。 不过说实话，大部分情况你都不会直接用到空转锁，其他锁在申请不到加锁时也是会空转一定时间的，如果连这段时间都无法满足你的请求，那要么就是你扔的线程太多，或者你的临界区没你想象的那么短。 pthread_cleanup_push() &amp; pthread_cleanup_pop() 线程是允许在退出的时候，调用一些回调方法的。如果你需要做类似的事情，那么就用以下这两种方法: void pthread_cleanup_push(void (*callback)(void *), void *arg); void pthread_cleanup_pop(int execute); 正如名字所暗示的，它背后有一个stack，你可以塞很多个callback函数进去，然后调用的时候按照先入后出的顺序调用这些callback。所以你在塞callback的时候，如果是关心调用顺序的，那就得注意这一点了。 但是！你塞进去的callback只有在以下情况下才会被调用： 线程通过pthread_exit()函数退出 线程被pthread_cancel()取消 pthread_cleanup_pop(int execute)时，execute传了一个非0值 pthread_join() 在线程结束的时候，我们能通过上面的pthread_cleanup_push塞入的callback方法知道，也能通过pthread_join这个方法知道。一般情况下，如果是出于业务的需要要知道线程何时结束的，都会采用pthread_join这个方法。 它适用这样的场景： 你有两个线程，B线程在做某些事情之前，必须要等待A线程把事情做完，然后才能接着做下去。这时候就可以用join。 int pthread_join(pthread_t thread, void **value_ptr); 在B线程里调用这个方法，第一个参数传A线程的thread_id, 第二个参数你可以扔一个指针进去。当A线程调用pthread_exit(void *value_ptr)来结束的时候，A的value_ptr就会到pthread_join的value_ptr去，你可以理解成A把它计算出来的结果放到exit函数里面去，然后其他join的线程就能拿到这个数据了。 在B线程join了A线程之后，B线程会阻塞住，直到A线程跑完。A线程跑完之后，自动被detach，后续再要join的线程就会报EINVAL。 Condition Variables 条件变量 pthread_join解决的是多个线程等待同一个线程的结束。条件变量能在合适的时候唤醒正在等待的线程。具体什么时候合适由你自己决定。它必须要跟互斥锁联合起来用。原因我会在注意事项里面讲。 场景：B线程和A线程之间有合作关系，当A线程完成某件事情之前，B线程会等待。当A线程完成某件事情之后，需要让B线程知道，然后B线程从等待状态中被唤醒，然后继续做自己要做的事情。 如果不用条件变量的话，也行。那就是搞个volatile变量，然后让其他线程不断轮询，一旦这个变量到了某个值，你就可以让线程继续了。如果有多个线程需要修改这个变量，那就再加个互斥锁或者读写锁。 但是！！！这做法太特么愚蠢了，还特别浪费CPU时间，所以还在用volatile变量标记线程状态的你们也真是够了！！！ 大致的实现原理是：一个条件变量背后有一个池子，所有需要wait这个变量的线程都会进入这个池子。当有线程扔出这个条件变量的signal，系统就会把这个池子里面的线程挨个唤醒。 semaphore 信号量 semaphore事实上就是我们学《操作系统》的时候所说的PV操作。 你也可以把它理解成带有数量控制的互斥锁，当sem_init(&amp;sem, 0, 1);时，他就是一个mutex锁了。 场景：比如有3台打印机，有5个线程要使用打印机，那么semaphore就会先记录好有3台，每成功被申请一次，就减1，减到0时，后面的申请就会被拒绝。 它也可以用mutex和条件变量来实现，但实际上还是用semaphore比较方便。 semaphore下的死锁 mutex下的死锁比较好处理，因为mutex只会锁一个资源(当semaphore的值为1时，就是个mutex锁)，按照顺序来申请mutex锁就好了。但是到了semaphore这里，由于资源数量不止1个，死锁情况就显得比较复杂。 Barriers Barrier可以理解成一个mile stone。当一个线程率先跑到mile stone的时候，就先等待。当其他线程都到位之后，再从等待状态唤醒，继续做后面的事情。 场景：超大数组排序的时候，可以采用多线程的方案来排序。比如开10个线程分别排这个超大数组的10个部分。必须要这10个线程都完成了各自的排序，你才能进行后续的归并操作。先完成的线程会挂起等待，直到所有线程都完成之后，才唤醒所有等待的线程。 前面有提到过条件变量和pthread_join，前者是在做完某件事情通知其他线程，后者是在线程结束之后让其他线程能够获得执行结果。如果有多个线程同时做一件事情，用上面这两者可以有次序地进行同步。另外，用semaphore也可以实现Barrier的功能。 但是我们已经有Barrier了好吗！你们能不要把代码搞那么复杂吗！ 总结 这篇文章主要讲了pthread的各种同步机制相关的东西：mutex、reader-writter、spin、cleanup callbacks、join、condition variable、semaphore、barrier。其中cleanup callbacks不算是同步机制，但是我看到也有人拿这个作为同步机制的一部分写在程序中，这是不对的！所以我才写了一下这个。 原文 https://casatwy.com/pthreadde-ge-chong-tong-bu-ji-zhi.html ","tags":[],"title":"pthread的各种同步机制","link":"https://toomi.pages.dev/post/pthread-de-ge-chong-tong-bu-ji-zhi/","stats":{"text":"14 min read","time":798000,"words":3801,"minutes":14},"dateFormat":"2023-04-19"},{"content":"排序的是一種很常見的行為，主要是將資料由小到大、或由大到小排列，根據資料量的多寡又分為： 內部排序：資料量小，可在記憶體內完成排序 外部排序：資料量大，需透過硬碟才能完成排序 而相關的排序演算法有非常多種 初等排序 氣泡排序法 Bubble Sort 選擇排序法 Selection Sort 插入排序法 Insertion Sort 謝耳排序法 Shell Sort 高等排序 快速排序法 Quick Sort 合併排序法 Merge Sort 堆積排序法 Heap Sort ","tags":[],"title":"排序","link":"https://toomi.pages.dev/post/pai-xu/","stats":{"text":"1 min read","time":32000,"words":148,"minutes":1},"dateFormat":"2023-04-14"},{"content":" 参考 http://www.ycvs.ntpc.edu.tw/ezfiles/0/1000/img/52/116782017.pdf http://www.ycvs.ntpc.edu.tw/ezfiles/0/1000/img/52/189599641.pdf ","tags":[],"title":"树","link":"https://toomi.pages.dev/post/shu/","stats":{"text":"1 min read","time":10000,"words":28,"minutes":1},"dateFormat":"2023-04-12"},{"content":" sql修改 此文中，修改左右数值是使用sql实现，但是在初始化的时候，需要大量执行sql语句，短期间会造成数据库压力，所以现把数据读到内存中，然后计算左右数值，最后报存到数据库 代码 Node package org.example.nestset; import java.util.ArrayList; import java.util.Collections; import java.util.List; public class Node { Integer id; Integer left; Integer right; List&lt;Node&gt; children = new ArrayList&lt;&gt;(); Integer level; public Integer getLeft() { return left; } @Override public boolean equals(Object obj) { if (obj instanceof Node) { Node node = (Node) obj; return id.equals(node.id); } return false; } @Override public int hashCode() { return this.id; } public Integer getRight() { return right; } public List&lt;Node&gt; getChildren() { return children; } public Integer getLevel() { return level; } public Integer getId() { return id; } public void setLevel(Integer level) { this.level = level; } public void setLeft(Integer left) { this.left = left; } public void setRight(Integer right) { this.right = right; } public Node(Integer id) { this.id = id; } public String toString() { return String.join( &quot;&quot;, Collections.nCopies(this.level -1, &quot; &quot;)) + &quot;|--&quot; + this.getId() + &quot;(&quot; + this.getLeft() + &quot;,&quot; + this.getRight() + &quot;)&quot;; } } Tree package org.example.nestset; import java.util.HashMap; import java.util.List; import java.util.Map; public class Tree { Node root; /** * 用来设置节点right ，left */ Integer seq = 0; public Tree() { } private final Map&lt;Integer, Node&gt; elementMap = new HashMap&lt;&gt;(); /** * 初始化根节点 */ private void addRoot(Node root) { root.setLeft(1); root.setRight(2); root.setLevel(1); this.root = root; } public void addChild(Node parent, Node child) { // 重复插入则忽略 if (this.elementMap.containsKey(child.getId())) { return; } this.elementMap.put(child.getId(), child); if (parent == null) { addRoot(child); return; } child.setLevel(parent.getLevel() + 1); parent.children.add(child); } private void search(List&lt;Node&gt; nodes) { for (Node node : nodes) { search(node); } } private void search(Node node) { node.setLeft(++seq); search(node.getChildren()); node.setRight(++seq); } public void search() { search(root); } public void print() { System.out.println(root.toString()); printChild(this.root, 1); } private void printChild(Node node, int i) { for (Node child : node.getChildren()) { System.out.println(child.toString()); printChild(child, i + 2); } } } Test package org.example.nestset; import org.junit.Test; public class NodeTest { @Test public void addChild() { Node node1 = new Node(1); Node node2 = new Node(2); Node node3 = new Node(3); Node node4 = new Node(4); Node node5 = new Node(5); Node node6 = new Node(6); Node node7 = new Node(7); Node node8 = new Node(8); Node node9 = new Node(9); Node node10 = new Node(10); Node node11 = new Node(11); Tree tree = new Tree(); tree.addChild(null, node1); tree.addChild(node1, node2); tree.addChild(node1, node3); tree.addChild(node1, node4); tree.addChild(node2, node5); tree.addChild(node3, node6); tree.addChild(node4, node7); tree.addChild(node3, node8); tree.addChild(node4, node9); tree.addChild(node5, node10); tree.addChild(node1, node11); tree.search(); tree.print(); } } 参考 https://www.werc.cz/blog/2015/07/19/nested-set-model-practical-examples-part-i ","tags":[],"title":"Java使用Nested set存储树形结构","link":"https://toomi.pages.dev/post/java-shi-yong-nested-set-cun-chu-shu-xing-jie-gou/","stats":{"text":"4 min read","time":188000,"words":547,"minutes":4},"dateFormat":"2023-04-03"},{"content":"在Mybatis中提供了大量实用的工具类，这些工具类不仅能用在Mybatis中，也可以把这些工具类拷贝出来，放到我们的项目中，方便开发。 GenericTokenParser 在Mybatis中，需要频繁的对XML进行解析，在解析的过程中GenericTokenParser绝对是出现频率最高的一个类. public static class VariableTokenHandler implements TokenHandler { private final Map&lt;String, String&gt; variables; VariableTokenHandler(Map&lt;String, String&gt; variables) { this.variables = variables; } @Override public String handleToken(String content) { return variables.get(content); } } static Stream&lt;Arguments&gt; shouldDemonstrateGenericTokenReplacementProvider() { return Stream.of(arguments(&quot;James T Kirk reporting.&quot;, &quot;#{first_name} #{initial} #{last_name} reporting.&quot;) ); } @ParameterizedTest @MethodSource(&quot;shouldDemonstrateGenericTokenReplacementProvider&quot;) void shouldDemonstrateGenericTokenReplacement(String expected, String text) { GenericTokenParser parser = new GenericTokenParser(&quot;#{&quot;, &quot;}&quot;, new VariableTokenHandler(new HashMap&lt;&gt;() { private static final long serialVersionUID = 1L; { put(&quot;first_name&quot;, &quot;James&quot;); put(&quot;initial&quot;, &quot;T&quot;); put(&quot;last_name&quot;, &quot;Kirk&quot;); put(&quot;var{with}brace&quot;, &quot;Hiya&quot;); put(&quot;&quot;, &quot;&quot;); } })); System.out.println(parser.parse(text)); assertEquals(expected, parser.parse(text)); } 参考 https://blog.51cto.com/u_15651175/5540400 https://github.com/mybatis/mybatis-3/blob/master/src/test/java/org/apache/ibatis/parsing/GenericTokenParserTest.java https://ibit.tech/archives/mybatis-common-tools-class-script-sql-runner ","tags":[],"title":"mybatis 工具类替换占位符","link":"https://toomi.pages.dev/post/mybatis-gong-ju-lei-ti-huan-zhan-wei-fu/","stats":{"text":"2 min read","time":81000,"words":256,"minutes":2},"dateFormat":"2023-03-29"},{"content":"JDBC回顾 众所周知，Mybatis是对JDBC的封装，那底层肯定是JDBC,那是不是很有必要回顾一下jdbc。先写一段代码 //首先加载驱动 Class.forName(&quot;com.mysql.jdbc.Driver&quot;); //提供JDBC连接的URL String url=&quot;jdbc:mysql://0.0.0.0:3306/xxxx&quot;; String username=&quot;root&quot;; String password=&quot;root&quot;; //创建数据库的连接 Connection con = DriverManager.getConnection(url,username,password); //创建一个statement执行者 String sql=&quot;SELECT * FROM biz_spot WHERE spot_id = &quot;; PreparedStatement statement = con.prepareStatement(sql); statement.setLong(1,11L); //执行SQL语句 ResultSet result = statement.executeQuery(); //处理返回结果 while (result.next()){ System.out.println(result.getString(&quot;xxx&quot;) + &quot;---&quot; + result.getString(&quot;xxx&quot;)); } //关闭JDBC对象 con.close(); result.close(); statement.close(); 执行过程如下图所示： 先来看看JDBC的Statement，statement的重要作用就是设置sql参数然后执行sql，先来看看jdbc的三种sql处理器： 关于防止sql注入，Statement是直接发送静态sql执行，而PreparedStatement 发送的是sql（这边会带问号）以及若干参数组，参数需要转义进去，所有的转义操作都在数据库端执行，并不是在我们的应用层转义的。 Statement 中常规方法 addBatch: 批处理操作，将多个SQL合并在一起，最后调用executeBatch 一起发送至数据库执行 setFetchSize:设置从数据库每次读取的数量单位。该举措是为了防止一次性从数据库加载数据过多，导致内存溢出。 编码示例： //批量执行sql @Test public void prepareBatchTest() throws SQLException { String sql = &quot;INSERT INTO `users` (`name`,age) VALUES ('bob',18);&quot;; Statement statement = connection.createStatement(); //设置最大行数 statement.setFetchSize(100); long start = System.currentTimeMillis(); for (int i = 0; i &lt; 100; i++) { statement.addBatch(sql); } // 批处理 一次发射 statement.executeBatch(); System.out.println(System.currentTimeMillis() - start); statement.close(); } //批量设置参数然后执行 @Test public void prepareBatchTest() throws SQLException { String sql = &quot;INSERT INTO `users` (`name`,age) VALUES (?,18);&quot;; PreparedStatement preparedStatement = connection.prepareStatement(sql); preparedStatement.setFetchSize(100); long start = System.currentTimeMillis(); for (int i = 0; i &lt; 100; i++) { preparedStatement.setString(1, UUID.randomUUID().toString()); preparedStatement.addBatch(); // 添加批处理参数 } preparedStatement.executeBatch(); // 批处理 一次发射 System.out.println(System.currentTimeMillis() - start); preparedStatement.close(); } // sql注入测试 public int selectByName(String name) throws SQLException { String sql = &quot;SELECT * FROM users WHERE `name`='&quot; + name + &quot;'&quot;; System.out.println(sql); Statement statement = connection.createStatement(); statement.executeQuery(sql); ResultSet resultSet = statement.getResultSet(); int count=0; while (resultSet.next()){ count++; } statement.close(); return count; } //PreparedStatement防止sql注入测试 public int selectByName2(String name) throws SQLException { String sql = &quot;SELECT * FROM users WHERE `name`=?&quot;; PreparedStatement statement = connection.prepareStatement(sql); statement.setString(1,name); System.out.println(statement); statement.executeQuery(); ResultSet resultSet = statement.getResultSet(); int count=0; while (resultSet.next()){ count++; } statement.close(); return count; } @Test public void injectTest() throws SQLException { //正常情况下 System.out.println(selectByName(&quot;bob&quot;)); //sql注入 System.out.println(selectByName(&quot;bob' or '1'='1&quot;)); //sql注入并没作用 System.out.println(selectByName2(&quot;bob' or '1'='1&quot;)); } Mybatis执行过程 各个过程也是各个组件的作用： 接口代理（MapperProxy）: 其目的是简化对MyBatis使用，底层使用动态代理实现。 Sql会话: 提供增删改查API，其本身不作任何业务逻辑的处理，所有处理都交给执行器。采用了设计模式中的门面模式。 执行器: 核心作用是处理SQL请求、事务管理、维护缓存以及批处理等 。执行器在的角色更像是一个管理员，接收SQL请求，然后根据缓存、批处理等逻辑来决定如何执行这个SQL请求。并交给JDBC处理器执行具体SQL。 JDBC处理器：他的作用就是用于通过JDBC具体处理SQL和参数的。在会话中每调用一次CRUD，JDBC处理器就会生成一个实例与之对应（命中缓存除外）。 各个组件的功能以及注意事项如下图： Executor Executor是MyBatis执行接口,执行器的功能如下： 基本功能：改、查，没有增删的原因是所有的增删操作都可以归结到改。 缓存维护：这里的缓存主要是为一级缓存服务，功能包括创建缓存Key、清理缓存、判断缓存是否存在。 事务管理：提交、回滚、关闭、批处理刷新。（一般我们都交由spring管理，不会使用mybatis的） Executor:可包含多个statement Executor有主要的三个实现子类。分别是：SimpleExecutor(简单执行器)、ReuseExecutor(重用执行器)、BatchExecutor(批处理执行器)。 SimpleExecutor是默认执行器，它的行为是每处理一次会话当中的SQl请求都会通过对应的StatementHandler 构建一个新个Statement，这就会导致即使是相同SQL语句也无法重用Statement,所以就有了（ReuseExecutor）可重用执行器 ReuseExecutor 区别在于他会将在会话期间内的Statement进行缓存，并使用SQL语句作为Key。所以当执行下一请求的时候，不在重复构建Statement，而是从缓存中取出并设置参数，然后执行。阅读源码可以知道其实里面就包含一个statementMap，执行的时候看一下是否存在，如果有了就不需要新构建statement了，这也说明为什么执行器不能跨线程调用，这会导致两个线程给同一个Statement 设置不同场景参数。 BatchExecutor 顾名思议，它就是用来作批处理的。但会将所有SQL请求集中起来，最后调用Executor.flushStatements() 方法时一次性将所有请求发送至数据库。 三个示例的代码： public class ExecutorTest { private Configuration configuration; private Connection connection; private JdbcTransaction jdbcTransaction; private MappedStatement ms; private SqlSessionFactory factory; @Before public void init() throws SQLException { // 获取构建器 SqlSessionFactoryBuilder factoryBuilder = new SqlSessionFactoryBuilder(); // 解析XML 并构造会话工厂 factory = factoryBuilder.build(ExecutorTest.class.getResourceAsStream(&quot;/mybatis-config.xml&quot;)); configuration = factory.getConfiguration(); jdbcTransaction = new JdbcTransaction(factory.openSession().getConnection()); // 获取SQL映射 ms = configuration.getMappedStatement(&quot;xxx.xxx.xxx.UserMapper.selectByid&quot;); } // 简单执行器测试 @Test public void simpleTest() throws SQLException { SimpleExecutor executor = new SimpleExecutor(configuration, jdbcTransaction); List&lt;Object&gt; list = executor.doQuery(ms, 10, RowBounds.DEFAULT, SimpleExecutor.NO_RESULT_HANDLER, ms.getBoundSql(10)); executor.doQuery(ms, 1, RowBounds.DEFAULT, SimpleExecutor.NO_RESULT_HANDLER, ms.getBoundSql(1)); System.out.println(list.get(0)); } // 重用执行器 @Test public void ReuseTest() throws SQLException { ReuseExecutor executor = new ReuseExecutor(configuration, jdbcTransaction); List&lt;Object&gt; list = executor.doQuery(ms, 1, RowBounds.DEFAULT, SimpleExecutor.NO_RESULT_HANDLER, ms.getBoundSql(1)); // 相同的SQL 会缓存对应的 PrepareStatement--&gt;缓存周期：会话 executor.doQuery(ms, 1, RowBounds.DEFAULT, SimpleExecutor.NO_RESULT_HANDLER, ms.getBoundSql(1)); System.out.println(list.get(0)); } // 批处理执行器 @Test public void batchTest() throws SQLException { BatchExecutor executor = new BatchExecutor(configuration, jdbcTransaction); MappedStatement setName = configuration .getMappedStatement(&quot;xxx.xxx.xxx.UserMapper.setName&quot;); Map&lt;String,Object&gt; param = new HashMap&lt;&gt;(2); param.put(&quot;arg0&quot;, 1); param.put(&quot;arg1&quot;, &quot;good man&quot;); //设置 executor.doUpdate(setName, param); executor.doUpdate(setName, param); executor.doFlushStatements(false); } } 这边批处理执行器想必大家觉得和上面有所不一样吧，为什么他不写查询，其实他如果写查询的话和上面的SimpleExecutor 一样，他有批处理功能，和上面jdbc的批处理一样，他是一条sql，设置多个参数过去，然后执行。而ReuseExecutor 是设置一次参数执行一次，设置一次执行一次。还是有本质的区别 基础以及二级缓存执行器 前面我们所说Executor其中有一个职责是负责缓存维护，以及事务管理。上面三个执行器并没有涉及，这部分逻辑去哪了呢？别急，缓存和事务无论采用哪种执行器，都会涉及，这属于公共逻辑。所以就完全有必要三个类之上抽象出一个基础执行器用来处理公共逻辑。 基础执行器 BaseExecutor 基础执行器主要是用于维护缓存和事务。事务是通过会话中调用commit、rollback进行管理。重点在于缓存这块它是如何处理的? (这里的缓存是指一级缓存）,它实现了Executor中的query与update方法。会话中SQL请求，正是调用的这两个方法。Query方法中处理一级缓存逻辑，即根据SQL及参数判断缓存中是否存在数据，有就走缓存。否则就会调用子类的doQuery() 方法去查询数据库,然后在设置缓存。在doUpdate() 中主要是用于清空缓存。 二级缓存执行器 BaseExecutor 只有一级缓存，那二级缓存其实是在CachingExecutor，那为什么不把它和一级缓存一起处理呢？因为二级缓存和一级缓存相对独立的逻辑，而且二级缓存可以通过参数控制关闭，而一级缓存是不可以的。综上原因把二级缓存单独抽出来处理。抽取的方式采用了装饰者设计模式，即在CachingExecutor 对原有的执行器进行包装（明白点就是CachingExecutor 包含了一个Executor，这里为三大子类执行器，子类拥有父类的方法基本就是指向了父类BaseExecutor的方法），处理完二级缓存逻辑之后，把SQL执行相关的逻辑交给实际的Executor处理（交由BaseExecutor 以及其子类处理）。 CachingExecutor直接实现了Executor接口。 Executor执行器关系图 会话与执行器的结构关系 从Mybatis执行过程的图中我们可以知道，SqlSession是调用Executor，从源码中可看成SqlSession实现中，其实包含一个Executor（二级缓存）。这样整个流程就串起来了，咱们以sqlsession的查询方法的selectList方法为例（因为select方法最终都会调到selectList） SqlSession调用selectList方法，就调用到CachingExecutor中的query方法 如果配置了二级缓存CachingExecutor先执行完自己的方法（是否有缓存从缓存里面取值操作），第一次缓存没有或没开启缓存交由BaseExecutor 的query方法 BaseExecutor方法执行完一级缓存后，然后交由子类doQuery方法，咱们默认为SimpleExecutor执行器 SimpleExecutor执行doQuery方法 如何创建会话以及如何将执行器包装成CachingExecutor又是另一个话题了 SqlSessionFactory.openSession，这里需要一个configuration 里面会有个方法configuration.newExecutor(tx, execType);这里会创建Executor 这里首先会根据executorType判断用三大执行器的哪个默认为SimpleExecutor 再使用CachingExecutor 对其包装：new CachingExecutor(executor) 会话与重用执行器以及批量执行器的关系 主要为了说明一下statement 这里为jdbc的statement, 重用执行器 假如我们用会话调用两个不同的方法，然后里面的sql是一样的，这里说的一样只是参数不同，那我们会预编译几次呢？下面代码示例 public interface EmployeeMapper { List&lt;Employee&gt; getAll(); @Select(&quot;select * from employee where id= #{id}&quot;) List&lt;Employee&gt; getById(@Param(&quot;id&quot;) Long id); @Select(&quot;select * from employee where id= #{id}&quot;) List&lt;Employee&gt; selectById(@Param(&quot;id&quot;) Long id); } @Test public void reuseExecutorTest() throws IOException { String resource = &quot;mybatis-config.xml&quot;; InputStream inputStream = Resources.getResourceAsStream(resource); SqlSessionFactory sqlSessionFactory = new SqlSessionFactoryBuilder().build(inputStream); //使用ExecutorType.REUSE设置重用执行器ReuseExecutor SqlSession sqlSession = sqlSessionFactory.openSession(ExecutorType.REUSE); try { EmployeeMapper employeeMapper = sqlSession.getMapper(EmployeeMapper.class); employeeMapper.selectById(1L); employeeMapper.getById(2L); } finally { sqlSession.close(); } } 打印结果 ==&gt; Preparing: select * from employee where id= ? ==&gt; Parameters: 1(Long) &lt;== Columns: id, name &lt;== Row: 1, zhangsan &lt;== Total: 1 ==&gt; Parameters: 2(Long) &lt;== Columns: id, name &lt;== Row: 2, lisi &lt;== Total: 1 从代码证明也就执行一次预编译。会话期间内所有的相同sql都只预编译一次即可 批量执行器 上面是重用执行器，预编译一次，那我们试一下批量执行器，小编前面说过，批量执行器需要使用修改的方法，那我们换一下代码： public class DemoTest { SqlSessionFactory sqlSessionFactory; @Before public void init() throws IOException { String resource = &quot;mybatis-config.xml&quot;; InputStream inputStream = Resources.getResourceAsStream(resource); sqlSessionFactory = new SqlSessionFactoryBuilder().build(inputStream); } @Test public void batchExecutorTest() { SqlSession sqlSession = sqlSessionFactory.openSession(ExecutorType.BATCH,true); try { EmployeeMapper employeeMapper = sqlSession.getMapper(EmployeeMapper.class); employeeMapper.updateById(&quot;wangwu&quot;, 1L); employeeMapper.updateById(&quot;zhaoliu&quot;,2L); //需要走flushStatements才会提交，即便上面opensession中设置自动提交为true List&lt;BatchResult&gt; batchResults = sqlSession.flushStatements(); System.out.println(batchResults.size()); } finally { sqlSession.close(); } } } public interface EmployeeMapper { @Update(&quot;update employee set name = #{name} where id=#{id}&quot;) void updateById(@Param(&quot;name&quot;)String name,@Param(&quot;id&quot;) Long id); @Insert(&quot;insert into emplyee id = #{employee.id}, name=#{employee.name}&quot;) void insertEmployee(@Param(&quot;employee&quot;) Employee employee); } 执行结果表明，这边statement预编译一次即可 ==&gt; Preparing: update employee set name = ? where id=? ==&gt; Parameters: wangwu(String), 1(Long) ==&gt; Parameters: zhaoliu(String), 2(Long) 1 紧接着我们继续修改一下测试用例： @Test public void batchExecutorTest() { SqlSession sqlSession = sqlSessionFactory.openSession(ExecutorType.BATCH,true); try { EmployeeMapper employeeMapper = sqlSession.getMapper(EmployeeMapper.class); //更新 employeeMapper.updateById(&quot;wangwu&quot;, 1L); Employee employee = new Employee(3L,&quot;tom&quot;); Employee employee2 = new Employee(4L,&quot;jerry&quot;); //插入两次 employeeMapper.insertEmployee(employee); employeeMapper.insertEmployee(employee2); //再次更新 employeeMapper.updateById(&quot;zhaoliu&quot;,2L); List&lt;BatchResult&gt; batchResults = sqlSession.flushStatements(); System.out.println(batchResults.size()); } finally { sqlSession.close(); } } 打印结果 ==&gt; Preparing: update employee set name = ? where id=? ==&gt; Parameters: wangwu(String), 1(Long) ==&gt; Preparing: insert into employee (id,name) values (?, ?) ==&gt; Parameters: 3(Long), tom(String) ==&gt; Parameters: 4(Long), jerry(String) ==&gt; Preparing: update employee set name = ? where id=? ==&gt; Parameters: zhaoliu(String), 2(Long) 从上面可有看出，相同sql语句在一起的与分开的是不一样的，只有连续相同的SQL语句并且相同的SQL映射声明，才会重用Statement，并利用其批处理功能。否则会构建一个新的Satement然后在flushStatements() 时一次执行。这么做的原因是它要保证执行顺序。跟调用顺序一致性。 Mybatis插件 MyBatis⽀持⽤插件对四⼤核⼼对象(Executor、StatementHandler、ParameterHandler、ResultSetHandler)进⾏拦截，对mybatis来说插件就是拦截器，⽤来增强核⼼对象的功能，增强功能本质上是借助于底层的 动态代理实现的，换句话说，MyBatis中的四⼤对象都是代理对象。 MyBatis所允许拦截的⽅法如下： 执⾏器Executor (update、query、commit、rollback等⽅法)； SQL语法构建器StatementHandler (prepare、parameterize、batch、updates query等⽅ 法)； 参数处理器ParameterHandler (getParameterObject、setParameters⽅法)； 结果集处理器ResultSetHandler (handleResultSets、handleOutputParameters等⽅法)； 核心组件 核心组件概述 组件 相关描述 Configuration Mybatis 的主要配置。 包含属性、设置、类型别名、类型处理器、对象工厂、环境配置和映射器等信息。 MappedStatement 用于描述 Mapper 中的 SQL 配置信息。 对 Mapper XML 配置文件中 &quot;&lt;select update SqlSession Mybatis 提供的面向用户的 API，可通过它来执行命令（增、删、改、查），获取映射器示例和管理事务。 Executor Mybatis 的 SQL 执行器，Mybatis 中对数据库所有的增、删、改、查操作都是由它完成的。 StatementHandler 封装了对 JDBC Statement 对象的操作。 ParameterHandler 用于为 PreparedStatement 和 CallableStatement 对象参数占位符设置值。 ResultSetHandler 封装了对 JDBC 中的 ResultSet 对象操作，将 SELECT 查询结果抓换成 Java 对象。 TypeHandler MyBatis 中的类型处理器，用于处理 Java 类型和 JDBC 类型之间的映射。 SqlSourc SqlSource接口很简单，只有一个getBound方法: public interface SqlSource { BoundSql getBoundSql(Object parameterObject); } 它有很多实现，需要我们重点关注的是StaticSqlSource，RawSqlSource和DynamicSqlSource。在正式学习他们前，我们先了解一下Mybatis动态SQL和静态SQL的区别。 动态SQL表示这个SQL节点中含有${} (不是#{}) 或是其他动态的标签（比如，if，trim，foreach，choose，bind节点等），需要在运行时根据传入的条件才能确定SQL，因此对于动态SQL的MappedStatement的解析过程应该是在运行时。 而静态SQL是不含以上这个节点的SQL，能直接解析得到含有占位符形式的SQL语句，而不需要根据传入的条件确定SQL，因此可以在加载时就完成解析。所在在执行效率上要高于动态SQL。 而DynamicSqlSource和RawSqlSource就分别对应了动态SQL和静态SQL，它们都封装了StaticSqlSource。即前两个中getBoundSql方法是委托给StaticSqlSource对象。 参考 https://www.cnblogs.com/insaneXs/p/9083003.html https://www.bmabk.com/index.php/post/13564.html https://wch853.github.io/posts/mybatis/MyBatis%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90%EF%BC%88%E4%B8%89%EF%BC%89%EF%BC%9A%E5%9F%BA%E7%A1%80%E6%94%AF%E6%8C%81%E6%A8%A1%E5%9D%97.html#%E7%B1%BB%E5%9E%8B%E8%BD%AC%E6%8D%A2 ","tags":[],"title":"JDBC 与 mybatis 处理过程","link":"https://toomi.pages.dev/post/mybatis-mappedstatement/","stats":{"text":"17 min read","time":993000,"words":3911,"minutes":17},"dateFormat":"2023-03-29"},{"content":"前期准备 需要安装jq 命令 ubuntu@gooderver:~$ ip -j -4 addr show | jq '.[]|.addr_info|.[]|.local' &quot;127.0.0.1&quot; &quot;172.24.64.162&quot; ubuntu@gooderver:~$ ip -j -6 addr show | jq '.[]|.addr_info|.[]|.local' &quot;::1&quot; &quot;2403:ac00:xxxx&quot; &quot;fe80::e654:e8ff:fea3:6a01&quot; ubuntu@gooderver:~$ ip -j -6 addr show | jq -r '.[]|.addr_info|.[]|.local' ::1 2403:ac00:xxxx fe80::e654:e8ff:fea3:6a01 参考 作者：wangbingbing 出处：https://www.cnblogs.com/wangbingbing/p/17185709.html 版权：本作品采用「署名-非商业性使用-相同方式共享 4.0 国际」许可协议进行许可。 ","tags":[],"title":"shell获取所有ipv4/ipv6地址","link":"https://toomi.pages.dev/post/shell-huo-qu-suo-you-ipv4ipv6-di-zhi/","stats":{"text":"1 min read","time":39000,"words":126,"minutes":1},"dateFormat":"2023-03-27"},{"content":"在学习计算机编程的时候，堆栈是我们最经常使用到的概念，也是2种最基本的内存管理方式。可是在数据结构算法也有一个堆，还有叫二叉堆的。数据结构中的堆和二叉堆是同一个概念，它们和内存管理中的堆没有任何关系。 堆（heap）这个叫法最早还是用在数据结构中，比用在内存管理的时间早。为了区别说法，本文我们称前者是二叉堆，后者叫内存堆。 内存堆 内存堆通常和栈出现在一起叫内存堆栈： 栈：是指函数调用是参数和局部变量的存储方式。在函数调用的时候由系统分配一块连续内存，局部变量都存在在栈中，在函数结束后统一释放。函数间的调用关系也是由函数栈通过先进先出的规则管理。 堆：是动态分配的，需要程序主动控制。 二叉堆 二叉堆满足以下条件: 全二叉树（即：一定要先填满上面的） 父节点比子节点重(二叉搜索树是父节点比左子节点重，但比右子节点轻) 二叉堆看着是二叉树，实际上在存储中还是使用数组，由于是全二叉树，子节点可以通过2k和2k + 1来定位。 参考 https://www.jianshu.com/p/00fac80c1b25 https://writings.sh/post/data-structure-heap-and-common-problems ","tags":[],"title":"二叉堆","link":"https://toomi.pages.dev/post/er-cha-dui/","stats":{"text":"2 min read","time":79000,"words":379,"minutes":2},"dateFormat":"2023-03-23"},{"content":"抽象数据类型是指一个数学模型以及定义在此数学模型上的一组操作，简称ADT,(Abstract Data Type)。 ADT与具体的物理存储无关，不论ADT内部如何变化，都不影响外部使用。抽象数据类型可以使我们更容易描述现实世界，而数据结构的本质就是抽象数据类型的物理实现。 抽象数据类型的定义格式： ADT&lt;抽象数据类型名&gt; { 数据对象D:&lt;数据对象的定义&gt; 数据关系R:&lt;数据关系的定义&gt; 基本操作P:&lt;基本操作的定义&gt; }ADT&lt;抽象数据类型名&gt; D是数据对象，R是D上的关系集，P是对D的基本操作集。 抽象数据类型抽象出了数据结构本质的特征，所能完成的功能以及它和外部用户的接口。同时，将实体的外部特性和其内部实现细节分离，并且对外部用户隐藏其内部实现细节。 参考 https://www.cnblogs.com/liuruiliang/p/16006760.html https://www.coursera.org/lecture/shuju-jiegou-suanfa/about-weQQs ","tags":[],"title":"抽象数据类型","link":"https://toomi.pages.dev/post/chou-xiang-shu-ju-lei-xing/","stats":{"text":"2 min read","time":65000,"words":293,"minutes":2},"dateFormat":"2023-03-23"},{"content":"什么是 hash Hash（哈希），又称“散列”。散列（hash）英文原意是“混杂”、“拼凑”、“重新表述”的意思。 在某种程度上，散列是与排序相反的一种操作，排序是将集合中的元素按照某种方式比如字典顺序排列在一起，而散列通过计算哈希值，打破元素之间原有的关系，使集合中的元素按照散列函数的分类进行排列。 在介绍一些集合时，我们总强调需要重写某个类的 equlas() 方法和 hashCode() 方法，确保唯一性。这里的 hashCode() 表示的是对当前对象的唯一标示。计算 hashCode 的过程就称作 哈希。 为什么要有 Hash 我们通常使用数组或者链表来存储元素，一旦存储的内容数量特别多，需要占用很大的空间，而且在查找某个元素是否存在的过程中，数组和链表都需要挨个循环比较，而通过 哈希 计算，可以大大减少比较次数。 举个栗子：现在有 4 个数 {2,5,9,13}，需要查找 13 是否存在。 使用数组存储，需要新建个数组 new int[]{2,5,9,13}，然后需要写个循环遍历查找。这样需要遍历 4 次才能找到，时间复杂度为 O(n)。 而假如存储时先使用哈希函数进行计算，这里我随便用个函数： H[key] = key % 3; // 四个数 {2,5,9,13} 对应的哈希值为： H[2] = 2 % 3 = 2; H[5] = 5 % 3 = 2; H[9] = 9 % 3 = 0; H[13] = 13 % 3 = 1; 然后把它们存储到对应的位置。当要查找 13 时，只要先使用哈希函数计算它的位置，然后去那个位置查看是否存在就好了，本例中只需查找一次(暂不考虑hash函数冲突)，时间复杂度为 O(1)。 因此可以发现，哈希其实是随机存储的一种优化，先进行分类，然后查找时按照这个对象的分类去找。哈希通过一次计算大幅度缩小查找范围，自然比从全部数据里查找速度要快。 哈希函数(算法) 哈希函数（算法）不是指某种特定的函数（算法），而是一类函数，它有各种各样的实现。例如 MD4、MD5、SHA。哈希函数（散列函数）能够将任意长度的输入值转变成固定长度的值输出，该值称为散列值，输出值通常为字母与数字组合。好的哈希算法发生碰撞的几率非常小，碰撞主要取决于： 散列函数，一个好的散列函数的值应尽可能平均分布。 处理碰撞方法。 负载因子的大小。太大不一定就好，并且浪费空间严重，负载因子和散列函数是联动的。 JDK 1.7 之前的 HashMap 我们先大概瞥一眼 JDK 1.7 之前的 HashMap 结构： 简而言之，HashMap 是由数组组成的一定数量的桶（bucket）。在进行存储时，使用 key 的 hashcode() 通过 hash 函数计算得到 hash 值，然后通过 (hash值 % 数组长度)来确定将 Entry(key + value) 放入数组的哪个桶里。 为了高效的定位元素在数组中的位置，以及使放入的元素尽可能均匀的分布在数组中。 正确实现 hashcode() 方法返回的 hash 值可以达到散列分布的目的，同样的键也会返回相同的 hash 值，因此我们使用 hash 信息来确定元素在数组中的下标信息以达到快速访问。 如果 hash 函数足够完美，将能实现数据的均匀分配，此时时间复杂度为 O(1)。但是开发者通常会编写较差的哈希函数，这将导致分布不均。 Hash碰撞及解决（Collision） Hash碰撞表示对于同一个hash函数，对不同的key进行hash计算得出对结果相同。此时数组将会有很大一部分被浪费。 负载因子与容量 为了解决这个问题，一方面，我们可以增大哈希值的取值空间来减少冲突的可能性，比如使 hash 表(数组，桶bucket)大于所需的总数据量。期望只要有哈希表的 70 % 被占用就足够。存储元素的个数和哈希表桶的数量的比值就叫做负载因子（Load factor）。 负载因子=所有存储的元素数量数组桶大小 负载因子 = \\frac{所有存储的元素数量}{数组桶大小} 负载因子=数组桶大小所有存储的元素数量​ 负载因子的值通常是可配置的，并在时间和空间成本之间进行权衡，下图是负载因子值调低调高时的影响。 Java 中 HashMap 的默认负载因子是 0.75，也就是说只要哈希表的 75 % 被占用就足够了。Java HashMap 的默认初始容量为 16，即存储区数组在首次插入时被延迟初始化。当插入的元素到达一定的阈值，HashMap 将会扩容来重新计算哈希，该阈值的计算公式为： 阈值(Threshold)=当前存储的元素数量×负载因子阈值(Threshold) = 当前存储的元素数量 \\times 负载因子 阈值(Threshold)=当前存储的元素数量×负载因子 以 HashMap 的默认值计算，则为：阈值=16×0.75=12阈值 = 16 \\times 0.75 = 12阈值=16×0.75=12, 也就是说当插入第 13 个元素后会进行扩容为之前的两倍 oldThreshold &lt;&lt; 1，此时将发生重新哈希（Rehashing），由于重新哈希处理增加了存储桶的数量，因此降低了负载因子。 为什么 HashMap 加载因子默认是 0.75？ 这个跟一个统计学里很重要的原理——泊松分布有关。泊松分布是统计学和概率学常见的离散概率分布，适用于描述单位时间内随机事件发生的次数的概率分布。 等号的左边，P 表示概率，N 表示某种函数关系，t 表示时间，n 表示数量。等号的右边，λ 表示事件的频率。在理想情况下，使用随机哈希码，在扩容阈值（加载因子）为 0.75 的情况下，节点出现在频率在 Hash 桶（表）中遵循参数平均为 0.5 的泊松分布。忽略方差，即 X = λt，P(λt = k)，其中 λt = 0.5的情况。所以我们可以知道，其实常数 0.5 是作为参数代入泊松分布来计算的，而加载因子 0.75 是作为一个条件，当 HashMap 长度为length/size ≥ 0.75 时就扩容，在这个条件下，冲突后的拉链(链表)长度和概率结果为： 0: 0.60653066 1: 0.30326533 2: 0.07581633 3: 0.01263606 4: 0.00157952 5: 0.00015795 6: 0.00001316 7: 0.00000094 8: 0.00000006 计算结果如上述的列表所示，当一个桶中的链表长度达到 8 个元素的时候，概率为 0.00000006，几乎是一个不可能事件。选择0.75作为默认的加载因子，完全是时间和空间成本上寻求的一种折衷选择。 冲突元素寻址 为了给冲突元素一个合适的位置存储，我们将解决方案从寻址方向上分为两个大类： 开放式寻址（Open Addressing） 闭合式寻址（Closed Addressing） 通过在哈希表数组本身中搜索另一个空存储桶来处理冲突。 键始终存储在散列到的桶中。在每个桶的基础上使用单独的数据结构来处理冲突。 每个桶中最多存放一个键。 每个桶存储任意键数。 理论最大负载系数为1。 没有理论上的最大负载系数。 哈希表数组的大小必须始终至少与哈希表中键的数量一样大。 性能随着负载系数的增长而降低。 开放式寻址相关技术 线性探测（Linear Probing） 二次方探测（Quadratic Probing） 再哈希（Double hashing） 罗宾汉哈希（Robin Hood hashing） 闭合式寻址相关技术 使用链表单独存储 hash 相同的键值（拉链法） 使用动态数组单独存储 hash 相同的键值 使用自平衡二叉树 \bJDK 1.8后，Java HashMap 是结合闭合式寻址中第一种和第三种的实现。 HashMap 详解 先总体了解下 HashMap 作为集合类的特性： HashMap 的键值都不能存储基本类型,要存储基本类型提高性能，可以使用 Eclipse Collection 的原始类型集合类。 支持一个 null 键和多个 null value 键必须唯一，重复的键值将被后面的值替代 使用哈希技术存储索引，所以不保证插入顺序 非线程安全，需自己保证同步,可以使用 Collections.synchronizedMap(new HashMap(...)); 包裹，或使用 HashTable，ConcurrentHashMap，后者性能更高 快速失败机制 在 Java 非线程安全的集合类中，遍历集合中，对集合做额外的操作比如调用新增、删除会立即停止当前操作并抛出 ConcurrentModificationException，在 HashMap 中使用 modCount 记录修改次数，如果遍历中该记录和开始时不相同，则报错。 可以使用 Iterator 接口安全的移除元素，一般集合会实现安全的移除操作。但是多线程环境下得保证 Iterator 实现类的线程安全。 final HashMap&lt;String, String&gt; hashMap = new HashMap&lt;&gt;(20, 0.75f); final Iterator&lt;Map.Entry&lt;String, String&gt;&gt; iterator = hashMap.entrySet().iterator(); while (iterator.hasNext()) { if (iterator.next().getKey().equals(&quot;test&quot;)) { iterator.remove(); // ok! } } // java 8 removeIf hashMap.entrySet().removeIf(stringStringEntry -&gt; stringStringEntry.getKey().equals(&quot;test&quot;)); 数组桶(bucket) 在代码中，桶数组用如下变量表示： transient Node&lt;K,V&gt;[] table; 这里需要注意几点： 桶数组并没有在构造方法中初始化，而是在第一次使用时才会分配内存，比如 put、compute、merge 等。 当分配内存时，长度总是 2 的幂次方。（为什么选择 2 的幂次方？由于为了达到高效处理性能，很多操作都是通过位运算完成。比如其中的 hash 方法，计算桶索引等，后面会详细说明。） 如果初始化时传入的桶的容量 capacity 不是 2 的幂次方，将会使用该位运算算法增加到最近的 2 次幂。 单链表 每个桶内部由链表组成，在代码中为类 Node&lt;K, V&gt; 的实例，此类是 HashMap 类的静态内部类，并且实现 Map.Entry&lt;K, V&gt; 接口，此节点的表示形式为： static class Node&lt;K,V&gt; implements Map.Entry&lt;K,V&gt; { final int hash; // 使用 hash 定位桶 final K key; // 节点存放的 key V value; // 节点存放的 值 Node&lt;K,V&gt; next; // 指向链表的下一个节点 } HashMap 如何计算桶索引 之前我们提到计算索引是使用哈希函数计算的 hash 值和桶数组长度求余，但是求余的效率并没有直接按位运算的高。同样我们需要借助一些位运算的技巧，这里 n 为桶数组的长度： index = hash(key) &amp; (n-1) // 相当于求 hash(key) % n，当 n 为 2 的次幂且不为 0 时成立 // hash 函数求 hash 值 static final int hash(Object key) { int h; return (key == null) ? 0 : (h = key.hashCode()) ^ (h &gt;&gt;&gt; 16); } 注意，为什么这里 hash 函数需要将高位数据移位到低位进行异或运算呢？这是因为有些数据计算出的哈希值差异主要在高位，而 HashMap 里的哈希寻址是忽略容量以上的高位的，那么这种处理就可以有效避免类似情况下的哈希碰撞。 计算桶索引并不只是发生在 put 方法时，在调用 get、contains、remove 时都会调用 hash 方法重新计算 hash 值并在计算桶索引。 为什么桶内不使用 ArrayList 或 LinkedList HashMap 内部使用单向链表来维护哈希冲突的元素，但为什么不用数组或双向链表，这其实是一个平衡后的考虑： ArrayList 使用较少的空间，检索速度快，但是最坏的情况下插入和删除元素的时间复杂度可能为 O(n) LinkedList 双向链表使用更多空间维护前后节点信息，但是插入或删除元素的时间复杂度为 O(1) 使用单向链表的好处在于，其既可以使空间相对较少，也能保证删除和插入的时间复杂度为 O(1)。但是如果链表过长，最坏的可能是所有元素都放入一个桶里，此时时间复杂度将变为 O(n)。为了优化这一点，JDK 1.8 使用了红黑树来优化链表过长的情况。 当桶数组的长度超过 MIN_TREEIFY_CAPACITY 且桶中的元素超过 TREEIFY_THRESHOLD 值时，链表转为红黑树。 当桶中元素减少至 UNTREEIFY_THRESHOLD 时，红黑树退回到链表。 为什么桶内元素过长时用红黑树 因为当大量哈希冲突的时候会导致节点链表越来越长从而降低 HashMap 性能。而红黑树为自平衡二叉搜索树，重新平衡并不完美，但保证在 O(logN) 时间内搜索，其中 n 是树的节点数。插入和删除操作，以及树的重排和重新着色，也在 O(logN) 时间内执行。 因此在数据量大的且桶中冲突较大的散列表中红黑树比单向链表更有优势。 本质上这是个安全问题 因为在元素放置过程中，如果一个对象哈希冲突，都被放置到同一个桶里，则会形成一个很长的链表，我们知道链表查询是线性的，会严重影响存取的性能。而在现实世界，构造哈希冲突的数据并不是非常复杂的事情，恶意代码就可以利用这些数据大量与服务器端交互，导致服务器端 CPU 大量占用，这就构成了哈希碰撞拒绝服务攻击，国内一线互联网公司就发生过类似攻击事件。 为什么选择红黑树而不是二叉树或绝对平衡二叉树呢 首先，二叉树在极端情况下依然会形成链表。例如 1,2,3,4 的 hashCode 相同时，二叉树退化成链表； 再是，绝对平衡就好像有强迫症一样把精力消耗在如何达到平衡上，因此造成不必要的性能开销； 而红黑树它是一棵自平衡树但不是绝对平衡树，优点有以下： - 树属于折半查找，于较长的链表相比查询效率要高 - 自平衡树解决了二叉树的计算情况问题（二叉树退化成链表） - 非绝对平衡树比绝对平衡树在增删节点时要高效一些 因此红黑树是综合性能较强的树型数据结构。 LinkedHashMap 我们知道 HashMap 使用 hash 函数定位桶，桶内部使用单向链表存储冲突元素，不能保证插入的顺序。LinkedHashMap 继承了 HashMap，在原有的数据结构基础上，将所有桶内元素通过双向链表链接起来，该链表定义了迭代的顺序，默认是数据插入的顺序，也可以配置为访问顺序。转换为红黑树时也维护链表。 如果将键重新插入映射中，则插入顺序不会受到影响。 该实现提供了顺序保证，但是并没有增加时间复杂度，和 HashMap 一样还是 O(1)。TreeMap 由于使用了红黑树来提供顺序保证，所以时间复杂度为 O(logN)。 TreeMap 比较 以下是 TreeMap、HashMap 和 LinkedHashMap 之间的重要区别。 No. 关键点 TreeMap HashMap LinkedHashMap 1 元素的顺序 插入到 TreeMap 中的元素根据其键的自然顺序进行排序，或者通过在 Map 创建时提供的 Comparator 进行排序，具体取决于使用的构造函数。 HashMap 不保证 Map 的顺序，也不保证顺序随时间保持不变。 LinkedHashMap 默认遵循元素的插入顺序，并维护插入元素的顺序。 2 内部实现 TreeMap 的内部实现不允许存储 null 键，但允许 null 值。 HashMap 允许存储一个 null 键以及多个 null 值。 LinkedHashmap 的内部实现与 HashMap 相似，因此允许存储一个 null 键和多个 null 值。 3 时间复杂度 TreeMap 的 get、put 和 remove 操作的时间复杂度都是 O(logN)，比 HashMap 大。 HashMap 在 get、put 和 remove 操作的情况下具有 O(1) 的复杂性。 LinkedHashMap 具有与 HashMap 相同的复杂度，即 O(1)。 4 继承 TreeMap 实现了 Collection 框架的 SortedMap 接口，它是 Map 的子代。TreeMap 内部实现了红黑树（一种自平衡二叉搜索树）。 HashMap 实现了简单的 Map 接口，并在内部使用散列来存储和检索其元素。 LinkedHashMap 扩展了 HashMap 并在内部像 HashMap 一样使用散列。 5 索引性能 与 HashMap 和 LinkedHashMap 相比，TreeMap 维护其元素的顺序，因此性能指标较低，并且需要更多内存。 HashMap 不维护其元素的任何插入顺序，因此与 TreeMap 相比更快，也不根据其值对其元素进行排序，因此也比 LinkedHashMap 更快。 LinkedHashMap 比 TreeMap 快，但比 HashMap 慢。 6 比较 TreeMap 中的元素通过使用 compareTo() 方法进行比较，其中也可以提供自定义实现。 HashMap 使用 Object 类的 compare() 方法进行元素比较。 LinkedHashMap 也使用 Object 类的 compare() 方法进行元素比较。 Java 7基于分段锁的ConcurrentHashMap 注：本章的代码均基于JDK 1.7.0_67 Java 7中的ConcurrentHashMap的底层数据结构仍然是数组和链表。与HashMap不同的是，ConcurrentHashMap最外层不是一个大的数组，而是一个Segment的数组。每个Segment包含一个与HashMap数据结构差不多的链表数组。整体数据结构如下图所示。 ConcurrentHashMap与HashMap相比，有以下不同点 ConcurrentHashMap线程安全，而HashMap非线程安全 HashMap允许Key和Value为null，而ConcurrentHashMap不允许 HashMap不允许通过Iterator遍历的同时通过HashMap修改，而ConcurrentHashMap允许该行为，并且该更新对后续的遍历可见 HashTable（jdk 1.2 时期的，后续没没太多优化，不建议使用，尽做向后兼容）容器在竞争激烈的并发环境下表现出效率低下的原因是所有访问HashTable的线程都必须竞争同一把锁，那假如容器里有多把锁，每一把锁用于锁容器其中一部分数据，那么当多线程访问容器里不同数据段的数据时，线程间就不会存在锁竞争，从而可以有效的提高并发访问效率，这就是ConcurrentHashMap所使用的锁分段技术，首先将数据分成一段一段的存储，然后给每一段数据配一把锁，当一个线程占用锁访问其中一个段数据的时候，其他段的数据也能被其他线程访问。 Java 8基于CAS的ConcurrentHashMap 注：本章的代码均基于JDK 1.8.0_111 Java 7为实现并行访问，引入了Segment这一结构，实现了分段锁，理论上最大并发度与Segment个数相等。Java 8为进一步提高并发性，摒弃了分段锁的方案，而是直接使用一个大的数组。同时为了提高哈希碰撞下的寻址性能，Java 8在链表长度超过一定阈值（8）时将链表（寻址时间复杂度为O(N)）转换为红黑树（寻址时间复杂度为O(long(N))）。 即JDK1.8中，ConcurrentHashMap的数据结构与1.8中的HashMap结构类似，都是：数组+单向链表/红黑树。 其数据结构如下图所示： Java 8的ConcurrentHashMap同样是通过Key的哈希值与数组长度取模确定该Key在数组中的索引。同样为了避免不太好的Key的hashCode设计，它通过如下方法计算得到Key的最终哈希值 //计算key的哈希值 h = key.hashCode(); //ConcurrentHashMap中的hash算法，其中HASH_BITS为0x7fffffff，即Integer.MAX_VALUE的值 h = (h ^ (h &gt;&gt;&gt; 16)) &amp; HASH_BITS //HashMap中的hash算法 h = h ^ (h &gt;&gt;&gt; 16) //计算table数组位置 (n - 1) &amp; h 不同的是，Java 8的ConcurrentHashMap作者认为引入红黑树后，即使哈希冲突比较严重，寻址效率也足够高，所以作者并未在哈希值的计算上做过多设计，只是将Key的hashCode值与其高16位作异或并保证最高位为0（从而保证最终结果为正整数）。相比HashMap，多了一步与运算：保证最高位为0，从而保证最终结果为正整数。 put 大体思路 要想对链表或红黑树进行put操作，必须拿到表头或根节点，所以，锁住了表头或根节点，就相当于锁住了整个链表或者整棵树。这个设计与分段锁的思想一致，只是其他读取操作需要用cas来保证。 如果table数组还没有初始化，则使用CAS进行初始化 如果table数组中i位置处元素为空，则使用CAS将table[i]的值设置为value 如果其他线程正在对table数组进行扩容，则当前线程去协助其进行扩容 其他情况，则使用synchronized锁住table[i]这个元素（链表表头或红黑树根节点），并将元素追加插入到链表或红黑树中；插入后，检查是否需要将该桶的数据结构由链表转化为红黑树。 成功设置&lt;key, value&gt;后，检查是否需要进行扩容 相比JDK1.7的ConcurrentHashMap，JDK1.8中的改进之处主要体现在： 更小的锁粒度：1.8中摒弃了segment锁，直接将hash桶的头结点当做锁。旧版本的一个segment锁，保护了多个hash桶，而1.8版本的一个锁只保护一个hash桶，由于锁的粒度变小了，写操作的并发性得到了极大的提升。 更高效的扩容 更多的扩容线程：扩容时，需要锁的保护。旧版本最多可以同时扩容的线程数是segment锁的个数。而1.8中理论上最多可以同时扩容的线程数是table数组的长度。但是为了防止扩容线程过多，ConcurrentHashMap规定了扩容线程每次最少迁移16个hash桶，因此1.8中实际上最多可以同时扩容的线程数是：hash桶的个数/16 扩容期间，依然保证较高的并发度：旧版本的segment锁，锁定范围太大，导致扩容期间，写并发度，严重下降。而新版本的采用更加细粒度的hash桶级别锁，扩容期间，依然可以保证写操作的并发度。 参考 https://www.cnblogs.com/goloving/p/15242372.html https://www.zeral.cn/data-structure/java-hashmap-%E5%AE%9E%E7%8E%B0/ http://www.jasongj.com/java/concurrenthashmap/ ","tags":[],"title":"hash 和 hashmap","link":"https://toomi.pages.dev/post/hash-he-hashmap/","stats":{"text":"22 min read","time":1292000,"words":5886,"minutes":22},"dateFormat":"2023-03-18"},{"content":"MyBatis Plus支持自定义通用方法，自定义通用方法与MP自带的通用方法一样，都会在应用启动时将方法注入到全局中。自定义通用方法的步骤： 在XxxMapper中定义想要自定义的通用方法，XxxMapper要继承BaseMapper 创建一个类继承AbstractMethod，重写injectMappedStatement()方法，该方法中输入写自定义通用方法的SQL语句 创建一个XxxInjector类继承DefaultInjector，重写getMethodList()方法，该方法中将包含了自定义SQL语句的类的实例化对象添加到methodList中 在配置文件中通过bean标签配置自定义的XxxInjector，并在全局配置中通过属性sqlInjector引入 步骤4中，配置自定义注入器范例代码如下 @Bean public SqlSessionFactory sqlSessionFactory(DriverManagerDataSource dataSource) throws Exception { MybatisSqlSessionFactoryBean factoryBean = new MybatisSqlSessionFactoryBean(); GlobalConfig globalConfig = GlobalConfigUtils.defaults(); globalConfig.setSqlInjector(new LzcLogicSqlInjector()); factoryBean.setGlobalConfig(globalConfig); factoryBean.setDataSource(dataSource); return factoryBean.getObject(); } 参考 https://juejin.cn/post/7093825235393773605 ","tags":[],"title":"MyBatis Plus自定义通用方法","link":"https://toomi.pages.dev/post/mybatis-plus-zi-ding-yi-tong-yong-fang-fa/","stats":{"text":"1 min read","time":54000,"words":230,"minutes":1},"dateFormat":"2023-03-17"},{"content":"Mybatis执行SQL的完整过程 一切的执行从MapperProxy开始，MapperProxy是MapperProxyFactory使用SqlSession创建出来的。所以MapperProxy中包含SqlSession 可以看到MapperProxy调用invoke方法，进而调用MapperMethod的execute()，这些MapperMethod 就是和你要执行的命令相关，比如执行select语句，则会通过 SqlSession的select()方法，最终调用到Executor的query方法。Executor会再协调另外三个核心组件。 MapperProxyFactory用来创建MapperProxy，这个factory其实主要就是完成了InvokeHandler的bindTarget的功能。而MapperProxy只需要完成invoke方法的功能。 MapperProxy包含SqlSession SqlSesion包含四大组件Executor，StatementHandler，ParameterHandler，ResultHandler。还包含Configuration Configuration可以创建四大组件，同时Configuration还包含InterceptorChain，通过调用interceptorChain的pluginAll()方法，完成针对四大组件的插件的动态代理链的创建。 MapperProxy 因为Mapper接口不能直接被实例化，Mybatis利用JDK动态代理，创建MapperProxy间接实例化Mapper对象。 MapperProxy还可以缓存MapperMethod对象 MapperMethod 负责解析Mapper接口的方法，并封装成MapperMethod对象 将Sql命令的执行路由到恰当的SqlSesison方法上 插件的构建 谈原理首先要知道StatementHandler，ParameterHandler，Result Handler都是代理，他们是Configuration创建，在创建过程中会调用interceptorChain.pluginAll()方法，为四大组件组装插件（再底层是通过Plugin.wrap(target,XX, new Plugin( interceptor))来来创建的）。 插件链是何时构建的 在执行SqlSession的query或者update方法时，SqlSession会通过Configuration创建Executor代理，在创建过程中就调用interceptor的pluginAll方法组装插件。然后executor在调用doQuery（）方法的时候，也会调用Configuration的newStatementHandler方法创建StatemenHandler（和上面描述的一样，这个handler就是个代理，也是通过interceptorChain的pluginAll方法构建插件） 插件如何执行 以statementhandler的prepare方法的插件为例，正如前面所说，statementhandler是一个proxy，执行他的prepare方法，将调用invokeHandler的invoke方法，而invokeHandler就是Plugin.wrap(target, xxx, new Plugin(interceptor))中的第三个参数，所以很自然invokeHanlder的invoke的方法最终就会调用interceptor对象的intercept方法。 例子 package org.example; import javax.sql.DataSource; import org.apache.ibatis.mapping.Environment; import org.apache.ibatis.session.Configuration; import org.apache.ibatis.session.SqlSession; import org.apache.ibatis.session.SqlSessionFactory; import org.apache.ibatis.session.SqlSessionFactoryBuilder; import org.apache.ibatis.transaction.TransactionFactory; import org.apache.ibatis.transaction.jdbc.JdbcTransactionFactory; import org.example.mapper.StudentMapper; import org.springframework.jdbc.datasource.DriverManagerDataSource; public class WithoutSpringRunner { public static void main(String[] args) { DriverManagerDataSource dataSource = new DriverManagerDataSource(); dataSource.setDriverClassName(&quot;com.mysql.cj.jdbc.Driver&quot;); dataSource.setUrl(&quot;jdbc:mysql://129.1.1.194:3386/erp?useSSL=false&amp;serverTimezone=GMT%2B8&quot;); dataSource.setUsername(&quot;erp&quot;); dataSource.setPassword(&quot;xxxxxx&quot;); TransactionFactory transactionFactory = new JdbcTransactionFactory(); Environment environment = new Environment(&quot;development&quot;, transactionFactory, dataSource); Configuration configuration = new Configuration(environment); configuration.addMapper(StudentMapper.class); SqlSessionFactory sqlSessionFactory = new SqlSessionFactoryBuilder().build(configuration); try (SqlSession session = sqlSessionFactory.openSession()) { StudentMapper userMapper = session.getMapper(StudentMapper.class); //这里能够执行findAll说明userMapper是一个实例，那一定是在getMapper中发生了实例化 userMapper.getAllStudent().forEach(System.out::println); } } } 从上面的例子的getMapper()方法中点进去，一直往里跟，最后会走到org.apache.ibatis.binding.MapperRegistry#getMapper这个方法。 public &lt;T&gt; T getMapper(Class&lt;T&gt; type, SqlSession sqlSession) { final MapperProxyFactory&lt;T&gt; mapperProxyFactory = (MapperProxyFactory&lt;T&gt;) knownMappers.get(type); if (mapperProxyFactory == null) { throw new BindingException(&quot;Type &quot; + type + &quot; is not known to the MapperRegistry.&quot;); } try { return mapperProxyFactory.newInstance(sqlSession); } catch (Exception e) { throw new BindingException(&quot;Error getting mapper instance. Cause: &quot; + e, e); } } 这里可以看到我们的UserMapper是从knownMappers这个map里拿出来的，那必然是有地方放进去的，发现是通过configuration.addMapper(UserMapper.class);这行代码将UserMapper放到knownMappers中。这里实际调用的是org.apache.ibatis.binding.MapperRegistry#addMapper public &lt;T&gt; void addMapper(Class&lt;T&gt; type) { if (type.isInterface()) { if (hasMapper(type)) { throw new BindingException(&quot;Type &quot; + type + &quot; is already known to the MapperRegistry.&quot;); } boolean loadCompleted = false; try { knownMappers.put(type, new MapperProxyFactory&lt;&gt;(type)); // It's important that the type is added before the parser is run // otherwise the binding may automatically be attempted by the // mapper parser. If the type is already known, it won't try. MapperAnnotationBuilder parser = new MapperAnnotationBuilder(config, type); parser.parse(); loadCompleted = true; } finally { if (!loadCompleted) { knownMappers.remove(type); } } } } 可以看到mybatis为我们的mapper封装了一个MapperProxyFactory对象，我们的mapper接口信息最终存在了这个对象的mapperInterface属性中，在getMapper()时，通过jdk动态代理生成代理对象，这个代理对象里面完成了对JDBC的封装，执行了真正的数据库操作。 原文 https://www.cnblogs.com/baichunyu/p/11208524.html https://github.com/oneone1995/blog/issues/12 ","tags":[],"title":"spring mybatis","link":"https://toomi.pages.dev/post/spring-mybatis/","stats":{"text":"5 min read","time":281000,"words":1039,"minutes":5},"dateFormat":"2023-03-16"},{"content":"重要类 BeanFactory： org.springframework.beans.factory.BeanFactory，是一个非常纯粹的 bean 容器，它是 IoC 必备的数据结构，其中 BeanDefinition 是它的基本结构。BeanFactory 内部维护着一个 BeanDefinition map ，并可根据 BeanDefinition 的描述进行 bean 的创建和管理。 BeanDefinition： org.springframework.beans.factory.config.BeanDefinition ，用来描述 Spring 中的 Bean 对象，如保存对象的类路径、对象的属性。 BeanFactoryPostProcessor, 当spring启动的时候，要执行容器的初始化，而BeanFactoryPostProcessor翻译过来就是bean工厂的后置处理器。简单来说，就是容器初始化完成之后，可以对容器做的一些后置处理。 BeanDefinitionRegistryPostProcessor , 此类为 BeanFactoryPostProcessor 子类，只不过BeanDefinitionRegistryPostProcessor支持动态注册beanDefinition，BeanFactoryPostProcessor原则上不支持 ApplicationContext： org.springframework.context.ApplicationContext ，这个是 Spring 容器，它叫做应用上下文，与我们应用息息相关。 生命周期 这里先说总结再讲解源码，先理清脉络再深入细节才不会迷失在细节当中。Spring的启动过程主要可以分为两部分： 第一步：解析成BeanDefinition：将bean定义信息解析为BeanDefinition类，不管bean信息是定义在xml中，还是通过@Bean注解标注，都能通过不同的BeanDefinitionReader转为BeanDefinition类。 这里分两种BeanDefinition，RootBeanDefintion和BeanDefinition。RootBeanDefinition这种是系统级别的，是启动Spring必须加载的6个Bean。BeanDefinition是我们定义的Bean。 第二步：参照BeanDefintion定义的类信息，通过BeanFactory生成bean实例存放在缓存中。 入口 // 创建ioc容器, sMainConfig为配置类 ApplicationContext context = new AnnotationConfigApplicationContext(MainConfig.class); // AnnotationConfigApplicationContext public class AnnotationConfigApplicationContext extends GenericApplicationContext implements AnnotationConfigRegistry { // 读取注解的Bean定义读取器 private final AnnotatedBeanDefinitionReader reader; // 扫描指定类路径中注解Bean定义的扫描器 private final ClassPathBeanDefinitionScanner scanner; // 无参构造函数 public AnnotationConfigApplicationContext() { // 查看new AnnotatedBeanDefinitionReader(this)源码可以发现 // 此时往容器中注入了5个处理器 // ConfigurationClassPostProcessor 这个内置的，唯一的实现了BeanDefinitionRegistryPostProcessor的后置处理器是解析其他通过注解注入的bean的类。 // AutowiredAnnotationBeanPostProcessor 用来实现依赖注入的功能 // CommonAnnotationBeanPostProcessor // EventListenerMethodProcessor // DefaultEventListenerFactory this.reader = new AnnotatedBeanDefinitionReader(this); this.scanner = new ClassPathBeanDefinitionScanner(this); } // 最常用的构造函数，通过将涉及的配置类传递给该构造函数，实现将相应配置类中的Bean自动注册到容器中 public AnnotationConfigApplicationContext(Class&lt;?&gt;... annotatedClasses) { // (1) 调用无参构造函数 this(); // (2) 将配置类信息封装成BeanDefinition对象保存到beanDefinitionMap容器中 register(annotatedClasses); // (3)刷新容器，触发容器对注解Bean的载入、解析和注册 refresh(); } } 初始化的过程可以看到初始化beanFactory为DefaultListableBeanFactory。这里可以看到AnnotationConfigApplicationContext虽然本身是一个beanFactory（实现了BeanFactory接口），但是依赖查找，依赖注入的过程是依赖内部的beanFactory来实现的（典型的代理模式） 另外需要注意的一点是，在容器初始化的过程中注册了6个Bean ConfigurationClassPostProcessor（实现了BeanFactoryPostProcessor，处理@Configuration） AutowiredAnnotationBeanPostProcessor（实现了BeanPostProcessor，处理@Autowired，@Value等） CommonAnnotationBeanPostProcessor（实现了BeanPostProcessor，用来处理JSR-250规范的注解，如@Resource，@PostConstruct等） PersistenceAnnotationBeanPostProcessor（实现了BeanFactoryPostProcessor，用来支持JPA，在我们这个Demo中不会注册，因为路径中没有JPA相关的类） EventListenerMethodProcessor（实现了BeanFactoryPostProcessor） DefaultEventListenerFactory 手动添加 BeanFactoryPostProcessor 首先我们要知道，给 Spring 容器提供 BeanFactoryPostProcessor 或者 BeanDefinitionRegistryPostProcessor 的 Bean 对象的方式有两种： 通过加注解，让 Spring 扫描到类（把一个类交给 Spring 管理，整个类的创建过程是交给 Spring 来处理的），主要依赖上文提到的内置的 ConfigurationClassPostProcessor 进行扫描 手动添加对象到 Spring 中（自己控制对象的实例化过程，然后再交给 Spring 管理） 手动注册BeanFactoryPostProcessor 并提供给 Spring 管理，是利用 ApplicationContext#addBeanFactoryPostProcessor api 进行 添加 ，例如 public class MyApplication { public static void main(String[] args) throws IOException { AnnotationConfigApplicationContext ctx = new AnnotationConfigApplicationContext(); ctx.addBeanFactoryPostProcessor(new ManB()); ctx.addBeanFactoryPostProcessor(new ManSubY()); ctx.register(AppConfig.class); ctx.refresh(); } } BeanFactoryPostProcessors执行顺序 Spring容器注册bean方式 常见注册bean对象到spring容器的方式: @Component、@Controller、@Service、@Repository 方式，此为自动创建 @Bean 方式，配置类方法加上 @Bean 注解 @Import 方式 @Import 普通类 @Import ImportSelector @Import ImportBeanDefinitionRegistrar 实现 BeanDefinitionRegistryPostProcessor#postProcessBeanDefinitionRegistry FactoryBean接口 以上，都是需要ConfigurationClassPostProcessor 这个内置 BeanDefinitionRegistryPostProcessor 进行扫描，然后加载 XML配置文件：通过在XML配置文件中使用标签来定义和注册bean。 番外 动态地给Java对象添加字段并赋值，并不是在已经实例化的对象上去完成的。基于cglib，commons-beanutils库实现思路为： 将原对象和扩展字段封装为字段map 基于字段map和原对象创建其子类对象 重新将原字段值和扩展字段值赋给子类对象 返回子类对象 相当于是复制类定义，然后新建个类定义处理。和 spring 的在 BeanFactoryPostProcessor 中添加属性是相通的，因为此时还没对bean进行实例化。 参考 https://www.cnblogs.com/JaxYoun/p/13923703.html https://juejin.cn/post/7238200443581677625 https://learnku.com/articles/42527 https://www.cnblogs.com/kendoziyu/p/springframework-bean-factory-post-processor.html https://juejin.cn/post/7039308644768284679 ","tags":[],"title":"Spring 容器启动","link":"https://toomi.pages.dev/post/beanfactorypostprocessor-he-beanpostprocessor-de-qu-bie/","stats":{"text":"5 min read","time":294000,"words":1253,"minutes":5},"dateFormat":"2023-03-07"},{"content":"排查方向 cpu使用率 内存使用率 磁盘队列 磁盘 通过【扩展事件】查看历史事件, 路径为 管理 --&gt; 扩展事件 --&gt; system_health --&gt; package0.event_file 查看 通过上方工具栏进行事件名称显示，或者筛选 查看当前执行的语句，以及其等待时间 SELECT r.session_id, r.wait_type, r.wait_time as wait_time_ms ,t.* FROM sys.dm_exec_requests r JOIN sys.dm_exec_sessions s ON r.session_id = s.session_id cross apply sys.dm_exec_sql_text(r.sql_handle) t WHERE wait_type in ('PAGEIOLATCH_SH', 'PAGEIOLATCH_EX', 'WRITELOG', 'IO_COMPLETION', 'ASYNC_IO_COMPLETION', 'BACKUPIO') AND is_user_process = 1 10-15 毫秒/传输数字是我们根据 Windows 和 SQL Server 工程师多年来的集体经验选择的非常近似的阈值。 通常，当数字超过此大致阈值时，SQL Server用户开始看到其工作负载中的延迟并报告这些延迟。 最终，I/O 子系统的预期吞吐量由制造商、型号、配置、工作负载以及可能的其他多个因素定义。 查看历史io数据 SELECT TOP 100 qs.last_execution_time, SUBSTRING(qt.TEXT, (qs.statement_start_offset/2)+1, ((CASE qs.statement_end_offset WHEN -1 THEN DATALENGTH(qt.TEXT) ELSE qs.statement_end_offset END - qs.statement_start_offset)/2)+1), qs.execution_count, qs.total_logical_reads, qs.last_logical_reads, qs.total_logical_writes, qs.last_logical_writes, qs.total_worker_time, qs.last_worker_time, qs.total_elapsed_time/1000000 total_elapsed_time_in_S, qs.last_elapsed_time/1000000 last_elapsed_time_in_S, qs.last_execution_time, qp.query_plan FROM sys.dm_exec_query_stats qs CROSS APPLY sys.dm_exec_sql_text(qs.sql_handle) qt CROSS APPLY sys.dm_exec_query_plan(qs.plan_handle) qp where qs.last_execution_time &lt;= '2023-03-04 13:54:35.523' -- ORDER BY qs.total_logical_reads DESC -- logical reads ORDER BY qs.last_execution_time DESC -- ORDER BY qs.total_worker_time DESC -- CPU time 有关 I/O 相关等待类型的信息 以下是在报告磁盘 I/O 问题时SQL Server观察到的常见等待类型的说明。 PAGEIOLATCH_EX 当任务在 I/O 请求中等待数据或索引页的闩锁 (缓冲区) 时发生。 闩锁请求处于独占模式。 当缓冲区写入磁盘时，将使用独占模式。 长时间等待可能表示磁盘子系统存在问题。 PAGEIOLATCH_UP 当任务在 I/O 请求中等待缓冲区的闩锁时发生。 闩锁请求处于更新模式。 长时间等待可能表示磁盘子系统存在问题。 WRITELOG 在任务等待事务日志刷新完成时发生。 当日志管理器将其临时内容写入磁盘时，会发生刷新。 导致日志刷新的常见操作是事务提交和检查点。 长时间等待的 WRITELOG 常见原因包括： 事务日志磁盘延迟：这是最常见的等待原因 WRITELOG 。 通常，建议将数据和日志文件保存在单独的卷上。 事务日志写入是连续写入，而从数据文件读取或写入数据是随机的。 在一个驱动器卷上混合数据和日志文件 (尤其是传统的旋转磁盘驱动器) 将导致磁盘头移动过多。 VLF 过多：) VLF (虚拟日志文件过多，可能会导致 WRITELOG 等待。 过多的 VLF 可能会导致其他类型的问题，例如长时间恢复。 过多的小事务：虽然大型事务可能会导致阻塞，但过多的小事务可能会导致另一组问题。 如果不显式开始事务，则任何插入、删除或更新都将导致事务 (我们调用此自动事务) 。 如果在一个循环中执行 1，000 次插入，则会生成 1，000 个事务。 此示例中的每个事务都需要提交，这会导致事务日志刷新和 1，000 个事务刷新。 如果可能，请将单个更新、删除或插入到更大的事务中，以减少事务日志刷新并提高 性能。 此操作可以减少 WRITELOG 等待时间。 计划问题会导致日志编写器线程无法以足够快的速度进行计划：在 2016 SQL Server 之前，单个日志编写器线程执行所有日志写入。 如果线程计划 (例如 CPU) 过高时出现问题，则日志编写器线程和日志刷新都可能会延迟。 在 2016 SQL Server，最多添加了四个日志编写器线程，以提高日志写入吞吐量。 请参阅 SQL 2016 - 它只是运行得更快：多个日志编写器辅助角色。 在 2019 SQL Server，最多添加了 8 个日志编写器线程，这进一步提高了吞吐量。 此外，在 2019 SQL Server，每个常规工作线程可以直接执行日志写入，而不是发布到日志编写器线程。 通过这些改进， WRITELOG 计划问题很少触发等待。 ASYNC_IO_COMPLETION 发生以下某些 I/O 活动时发生： 大容量插入提供程序 (“插入批量”) 在执行 I/O 时使用此等待类型。 读取 LogShipping 中的撤消文件，并定向异步 I/O 进行日志传送。 在数据备份期间从数据文件读取实际数据。 IO_COMPLETION 在等待 I/O 操作完成时发生。 此等待类型通常涉及与数据页无关的 I/O， (缓冲区) 。 示例包括： 在溢出期间从/向磁盘读取和写入排序/哈希结果 (检查 tempdb 存储) 的性能。 将预先的假脱机读取和写入磁盘 (检查 tempdb 存储) 。 在导致从磁盘读取日志的任何操作（例如，恢复) ）期间， (从事务日志读取日志块。 尚未设置数据库时，从磁盘读取页面。 将页面复制到数据库快照 (写入时复制) 。 关闭数据库文件和文件解压缩。 BACKUPIO 当备份任务正在等待数据或正在等待缓冲区存储数据时发生。 此类型并不常见，除非任务正在等待磁带装载。 参考文档 https://learn.microsoft.com/zh-cn/troubleshoot/sql/database-engine/performance/troubleshoot-sql-io-performance#io_completion ","tags":[],"title":"sql server 性能排查","link":"https://toomi.pages.dev/post/sql-server-xing-neng-pai-cha/","stats":{"text":"6 min read","time":327000,"words":1430,"minutes":6},"dateFormat":"2023-03-03"},{"content":"思路 查看当前连接端口数最多的ip地址排序，然后调用第三方接口查询ip归属地，然后利用ipset进行封禁 所需软件 ipset SHELL=/bin/sh PATH=/usr/local/sbin:/usr/local/bin:/sbin:/bin:/usr/sbin:/usr/bin target_sheng_fen=&quot;Henan&quot; for line in `netstat -anlp|grep 80|grep tcp|awk '{print $5}'|awk -F: '{print $1}'|sort|uniq -c|sort -nr|head -n20|awk '{print $2}'`; do shengfen=`curl -s https://ipinfo.io/$line |awk -F\\&quot; '/region/{print $(NF-1)}'` echo &quot;省份为 $shengfen&quot; if [ &quot;$shengfen&quot; = &quot;$target_sheng_fen&quot; ]; then echo &quot;--&gt;正在封禁 $line&quot; ipset add banlist $line ; fi done 参考 https://www.itbunan.xyz/index.php/liu/314.html https://aglzg.com/index/archives/info/id/80.html ","tags":[],"title":"根据地区封禁ip","link":"https://toomi.pages.dev/post/gen-ju-di-qu-feng-jin-ip/","stats":{"text":"1 min read","time":50000,"words":159,"minutes":1},"dateFormat":"2023-03-02"},{"content":"iptables 是 Linux 上常用的防火墙软件 netfilter 项目的一部分，所以要讲清楚 iptables，我们先理一理什么是防火墙？ 什么是防火墙 简单来说，防火墙是一种网络隔离工具，部署于主机或者网络的边缘，目标是对于进出主机或者本地网络的网络报文根据事先定义好的规则做匹配检测，规则匹配成功则对相应的网络报文做定义好的处理（允许，拒绝，转发，丢弃等）。防火墙根据其管理的范围来分可以将其划分为主机防火墙和网络防火墙；根据其工作机制来区分又可分为包过滤型防火墙（netfilter）和代理服务器（Proxy）。我们接下来在这篇笔记中主要说说包过滤型防火墙（netfilter）。 包过滤型防火墙 包过滤型防火墙主要依赖于 Linux 内核软件 netfilter，它是一个 Linux 内核“安全框架”，而 iptables 是内核软件 netfilter 的配置工具，工作于用户空间。iptables/netfilter 组合就是 Linux 平台下的过滤型防火墙，并且这个防火墙软件是免费的，可以用来替代商业防火墙软件，来完成网络数据包的过滤、修改、重定向以及网络地址转换（nat）等功能。 在有些 Linux 发行版上，我们可以使用 systemctl start iptables 来启动 iptables 服务，但需要指出的是，iptables 并不是也不依赖于守护进程，它只是利用 Linux 内核提供的功能。 iptables 服务 可以理解成机器重启后，自动应用防火墙规则。如果无此项服务，需要手动保存规则，并重启后重新手动导入规则。 其中规则可以包括匹配数据报文的源地址、目的地址、传输层协议（TCP/UDP/ICMP/..）以及应用层协议（HTTP/FTP/SMTP/..）等，处理逻辑就是根据规则所定义的方法来处理这些数据包，如放行（accept），拒绝（reject）返回拒绝信息，丢弃（drop）即抛弃数据包，不返回任何信息 等。 在数据包经过网络协议栈的不同位置时做相应的由 iptables 配置好的处理逻辑。 netfilter 中的五个钩子（这里也称为五个关卡）分别为：PRE_ROUTING/INPUT/FORWARD/OUTPUT/POST_ROUTING，网络数据包的流向图如下图所示： 当主机/网络服务器网卡收到一个数据包之后进入内核空间的网络协议栈进行层层解封装 刚刚进入网络层的数据包通过 PRE_ROUTING 关卡时，要进行一次路由选择，当目标地址为本机地址时，数据进入 INPUT，非本地的目标地址进入 FORWARD（需要本机内核支持 IP_FORWARD），所以目标地址转换通常在这个关卡进行 INPUT 经过路由之后送往本地的数据包经过此关卡，所以过滤 INPUT 包在此点关卡进行 FORWARD 经过路由选择之后要转发的数据包经过此关卡，所以网络防火墙通常在此关卡配置 OUTPUT 由本地用户空间应用进程产生的数据包过此关卡，所以 OUTPUT 包过滤在此关卡进行 POST_ROUTING 刚刚通过 FORWARD 和 OUTPUT 关卡的数据包要通过一次路由选择由哪个接口送往网络中，经过路由之后的数据包要通过 POST_ROUTING 此关卡，源地址转换通常在此点进行 上面提到的这些处于网络协议栈的“关卡”，在 iptables 的术语里叫做“链（chain）”，内置的链包括上面提到的5个： PreRouting Forward Input Output PostRouting 一般的场景里面，数据包的流向基本是： 到本主机某进程的报文：PreRouting -&gt; Input -&gt; Process -&gt; Output -&gt; PostRouting 由本主机转发的报文：PreRouting -&gt; Forward -&gt; PostRouting iptables 的四表五链 iptables 默认有五条链（chain），分别对应上面提到的五个关卡：PRE_ROUTING/INPUT/FORWARD/OUTPUT/POST_ROUTING，这五个关卡分别由 netfilter 的五个钩子函数来触发。但是，为什么叫做“链”呢？ 我们知道，iptables/netfilter 防火墙对经过的数据包进行“规则”匹配，然后执行相应的“处理”。当报文经过某一个关卡时，这个关卡上的“规则”不止一条，很多条规则会按照顺序逐条匹配，将在此关卡的所有规则组织称“链”就很适合，对于经过相应关卡的网络数据包按照顺序逐条匹配“规则”。 另外一个问题是，每一条“链”上的一串规则里面有些功能是相似的，比如，A 类规则都是对 IP 或者端口进行过滤，B 类规则都是修改报文，我们考虑能否将这些功能相似的规则放到一起，这样管理 iptables 规则会更方便。iptables 把具有相同功能的规则集合叫做“表”，并且定一个四种表： filter 表：负责过滤功能；与之对应的内核模块是 iptables_filter nat(Network Address Translation) 表：网络地址转换功能，典型的比如 SNAT、DNAT，与之对应的内核模块是 iptables_nat mangle 表：解包报文、修改并封包，与之对应的内核模块是 iptables_mangle raw 表：关闭 nat 表上启用的连接追踪机制；与之对应的内核模块是 iptables_raw Linux 网络管理员所定义的所有 iptables “规则”都存在于这四张表中。 但是，需要注意的是，并不是所有的“链”都具有所有类型的“规则”，也就是说，某个特定表中的“规则”注定不能应用到某些“链”中，比如，用作地址转换功能的 nat 表里面的“规则”据不能存在于 FORWARD “链”中。下面，我们就详细的列举 iptables 的“链表”关系： 我们先说一下，每个“链”都拥有哪些功能的规则： 链 表 PreRouting raw, mangle, nat Forward mangle, filter Input mangle, filter, nat Output raw, mangle, filter, nat PostRouting mangle, nat 在实际使用 iptables 配置规则时，我们往往是以“表”为入口制定“规则”，所以我们将“链表”关系转化成“表链”关系： 表 链 raw PreRouting, Output mangle PreRouting, Forward, Input, Output, PostRouting filter Forward, Input, Output nat PreRouting, Input, Output, PostRouting 还需要注意的一点是，因为数据包经过一个关卡的时候，会将“链”中所有的“规则”都按照顺序逐条匹配，为相同功能的“规则”属于同一个“表”。那么，哪些“表”中的规则会放到“链”的最前面执行呢？这时候就涉及一个优先级的问题。 iptables 为我们提供了四张“表”，当它们处于同一条“链”的时候，它们的执行优先级关系如下： raw -&gt; mangle -&gt; nat -&gt; filter 实际上，网络管理员还可以使用 iptables 创建自定义的“链”，将针对某个应用层序所设置的规则放到这个自定义“链”中，但是自定义的“链”不能直接使用，只能被某个默认的“链”当作处理动作 action 去调用。可以这样说，自定义链是“短”链，这些“短”链并不能直接使用，而是需要和 iptables上 的内置链一起配合，被内置链“引用”。 数据包经过过滤型防火墙的流程 有了前面的介绍我们可以总结出如下图所示的数据包在经过过滤型防火墙的流程图： iptables规则 前面我们提到过，iptables 规则由两部分组成：报文的匹配条件和匹配到之后的处理动作。 匹配条件：根据协议报文特征指定匹配条件，基本匹配条件和扩展匹配条件 处理动作：内建处理机制由 iptables 自身提供的一些处理动作 同时，网络管理员还可以使用 iptables 创建自定义的链，附加到 iptables 的内置的五个链。 设置 iptables 规则时需要考量的要点： 根据要实现哪种功能，判断添加在哪张“表”上 根据报文流经的路径，判断添加在哪个“链”上 到本主机某进程的报文：PreRouting -&gt; Input -&gt; Process -&gt; Output -&gt; PostRouting 由本主机转发的报文：PreRouting -&gt; Forward -&gt; PostRouting 对于每一条“链”上其“规则”的匹配顺序，排列好检查顺序能有效的提高性能，因此隐含一定的法则： 同类规则（访问同一应用），匹配范围小的放上面 不同类规则（访问不同应用），匹配到报文频率大的放上面 将那些可由一条规则描述的多个规则合并为一个 设置默认策略 同时，也一定要注意，在远程连接主机配置防火墙时注意： 不要把“链”的默认策略修改为拒绝，因为有可能配置失败或者清除所有策略后无法登陆到服务器，而是尽量使用规则条目配置默认策略 为防止配置失误策略把自己也拒掉，可在配置策略时设置计划任务定时清除策略，当确定无误后，关闭该计划任务 原文 https://morven.life/posts/iptables-wiki/ ","tags":[],"title":"从零开始认识 iptables","link":"https://toomi.pages.dev/post/cong-ling-kai-shi-ren-shi-iptables/","stats":{"text":"9 min read","time":499000,"words":2336,"minutes":9},"dateFormat":"2023-02-28"},{"content":"对每个ip限速 iptables -A INPUT -i eth0 -m hashlimit --hashlimit-above 128kb/s --hashlimit-mode srcip --hashlimit-name in -j DROP iptables -A OUTPUT -o eth0 -m hashlimit --hashlimit-above 128kb/s --hashlimit-mode dstip --hashlimit-name out -j DROP iptables的命令参数说明 下面关于iptables命令的详细参数简单说明一下： -t(table)：指定表 -A(append)： 追加，一般将新加的规则（策略）是追加到表中链上的末尾规则 -I(insert) 插入链 后面加数字可以指定讲规则加到相应的行，默认是所有规则最前面 -L(list)查看规则 可以加-n、-v参数效果查看更明显 -F(flush) 清除所有规则 -D(delete)删除某个规则 可以指定相应链上的规则顺序号去删除，--line-numbers可以查看 匹配数据包的一些条件参数： -i 进入的网卡 -o 出去的网卡 -s ip源地址IP -d ip 目的地址IP --dport 端口号 目的端口号 --sdport 端口号 源端口号 限制并发数 iptables的connlimit模块可限制全局的并发连接数。其用法如下： iptables -I INPUT -p tcp --syn --dport 8081 -m connlimit --connlimit-above 2 --connlimit-mask 0 -j DROP 命令要点： -p tcp：针对tcp协议过滤； --dport 8081：对8081端口起作用； -m connlimit：使用connlimit模块 --connlimit-above 2：最多不超过2个并发连接 --connlimit-mask 0： 默认ip掩码是32，即限制单ip并发连接数，设置为0则限制全局连接。 用上述命令配置好防火墙后，可用以下方法测试效果： 用nc监听端口：nc -l 8081 -k; 打开另外几个终端，测试连接：nc localhost 8081。 正确配置情形下，第一个和第二个nc连接是正常的，第三个连接起会出现 broken pipe 错误,即无法连接，说明防火墙达到了预期效果（也可以用netstat -nt | grep 8081查看已建立连接）。 允许单个IP的最大连接数为 3 iptables -I INPUT -p tcp --dport 80 -m connlimit --connlimit-above 3 -j REJECT 删除规则 列出防火墙所有规则，并显示行数 iptables -L -n --line-number 删除INPUT下行数为 3 的规则 iptables -D INPUT 3 保存设置及重启自动恢复 iptables设定的规则，掉电重启会清空。若需要保存，最好是使用iptables-save/restore配合重定向，示例如下： iptables-save &gt; /etc/iptables-rules iptables-restore &lt; /etc/iptables-rules ","tags":[],"title":"iptables 限速 ，限制并发数","link":"https://toomi.pages.dev/post/iptables-xian-su/","stats":{"text":"3 min read","time":148000,"words":599,"minutes":3},"dateFormat":"2023-02-27"},{"content":"提供与 xml 中 &lt;import/&gt;等效的功能, 允许去导入@Configuration类, ImportSelector 和 ImportBeanDefinitionRegistrar 的具体实现, 以及常规组件类 。 类似于AnnotationConfigApplicationContext.register(java.lang.Class&lt;?&gt;...) 这种操作。 可以在类级别声明或作为元注释声明。如果需要导入XML或其他非@Configuration bean定义资源，请改用@ImportResource注释 1. 注册具体类 将 @Import 标记的类注册成 bean。 // Test 类 这里不需要任何注解(@Component、@Service)这些都不需要 public class Test { } // MyConfig 类 @Configuration @Import({Test.class}) public class AppConfig { } 2. 导入@Configuration 配置类下的bean定义 @Configuration public class AppConfigAux { // 假设这里内部由很多使用了@Bean注解的方法 } // AppConfig 类 @Configuration @Import({AppConfigAux.class}) public class AppConfig { } // 开始测试的Test类 public class Test { public static void main(String[] args) { new AnnotationConfigApplicationContext(AppConfig.class); } } 3. 导入ImportBeanDefinitionRegistrar的具体实现类 通过方法 registerBeanDefinitions进行注入 // AppConfigAux 类 不需要任何注解 public class AppConfigAux implements ImportBeanDefinitionRegistrar { @Override public void registerBeanDefinitions(AnnotationMetadata importingClassMetadata, BeanDefinitionRegistry registry) { registry.registerBeanDefinition(&quot;simple&quot;,BeanDefinitionBuilder.rootBeanDefinition(Simple.class).getBeanDefinition()); } } // AppConfig 类 @Configuration @Import({AppConfigAux.class}) public class AppConfig { } // Simple 类 不需要注解 pulic class Simple { } // 开始测试的Test类 public class Test { public static void main(String[] args) { new AnnotationConfigApplicationContext(AppConfig.class); } } 4. 导入ImportSelector的具体实现 // AppConfigAux 类 不需要任何注解 public class AppConfigAux implements ImportSelector { @Override public String[] selectImports(AnnotationMetadata importingClassMetadata) { //返回String[]含义：多个类的完全限定名，表示注入这些类到容器中，bean名称就是类的完全限定名（保证唯一性） return new String[] { Simple.class.getName() }; } } // AppConfig 类 @Configuration @Import({AppConfigAux.class}) public class AppConfig { } // Simple 类 不需要注解 pulic class Simple { } // 开始测试的Test类 public class Test { public static void main(String[] args) { new AnnotationConfigApplicationContext(AppConfig.class); } } ","tags":[],"title":"Spring @Import 注解介绍","link":"https://toomi.pages.dev/post/spring-import-zhu-jie-jie-shao/","stats":{"text":"2 min read","time":109000,"words":404,"minutes":2},"dateFormat":"2023-02-24"},{"content":" 创建脚本rename-files.sh #!/bin/bash #print usage if [ -z $1 ];then echo &quot;Usage :$(basename $0) parent-directory&quot; exit 1 fi #process all subdirectories and files in parent directory all=&quot;$(find $1 -depth)&quot; for name in ${all}; do #set new name in lower case for files and directories new_name=&quot;$(dirname &quot;${name}&quot;)/$(basename &quot;${name}&quot; | tr '[A-Z]' '[a-z]')&quot; #check if new name already exists if [ &quot;${name}&quot; != &quot;${new_name}&quot; ]; then [ ! -e &quot;${new_name}&quot; ] &amp;&amp; mv -T &quot;${name}&quot; &quot;${new_name}&quot;; echo &quot;${name} was renamed to ${new_name}&quot; || echo &quot;${name} wasn't renamed!&quot; fi done echo echo #list directories and file new names in lowercase echo &quot;Directories and files with new names in lowercase letters&quot; find $(echo $1 | tr 'A-Z' 'a-z') -depth exit 0 执行脚本，将Files目录以及子目录和文件全都改成小写 chmod +x rename-files.sh rename-files.sh Files #Files 为目录名 原文链接 https://www.tecmint.com/rename-all-files-and-directory-names-to-lowercase-in-linux/ ","tags":[],"title":"linux 子目录以及文件名改成小写","link":"https://toomi.pages.dev/post/linux-zi-mu-lu-yi-ji-wen-jian-ming-gai-cheng-xiao-xie/","stats":{"text":"2 min read","time":70000,"words":203,"minutes":2},"dateFormat":"2023-02-23"},{"content":"以前在linux中如果要部署.net运行环境，必须安装mono，xsp等等。非常耗时耗力且繁琐。 偶尔有一次发现宇内大神的jexus推出了独立版，无法安装mono即可完美支持asp.net webform, mvc等程序，安装过程也非常方便快捷。只需要几行命令即可，大大增加减少了小白的操作步骤。 安装步骤 运行下述安装命令，需要操作者有root权限。 X86_64系统 安装jexus独立版的命令是： curl https://jexus.org/release/x64/install.sh sudo sh ./install.sh ARM64系统 安装jexus独立版的命令是： curl https://jexus.org/release/arm64/install.sh sudo sh ./install.sh 配置和运行 配置目录，默认网站路径 /usr/jexus/siteconf/default , 在里面配置端口port，和网站路径 root 将网站上传到 root对应的路径，Jexus包括如下操作命令（首先 cd /usr/jexus）： 启动：sudo ./jws start 停止：sudo ./jws stop 重启：sudo ./jws restart 参考网站 https://www.jexus.org/ ","tags":[],"title":"jexus 运行 asp.net","link":"https://toomi.pages.dev/post/jexus-yun-xing-aspnet/","stats":{"text":"2 min read","time":61000,"words":255,"minutes":2},"dateFormat":"2023-02-21"},{"content":"在開發應用的過程中，會使用一些程式庫，例如，應用程式也許使用了 OWASP 的 HTML Sanitizer、H2 JDBC 驅動程式、Java Mail 等，然而，應用程式程式主要流程一直在你的控制之內，你決定了何時要處理請求參數、取得模型物件、轉發請求、顯示頁面等各式流程。 控制、熟悉應用程式的流程走向，是很重要的一部份，在打算套用框架之前，你必須先認清應用程式的流程走向，至少，你應該寫個原型，試著摸出、掌握大致的流程走向，這對後續框架的選擇、使用會有很大的幫助。 因為在開始使用框架之後，會發現框架將主導程式運行的某個流程，你必須在框架的規範下定義某些類別，框架會在適當時候調用你實作的程式元件，也就是說，對應用程式的流程控制權會被反轉，框架會規範、限制應用程式流程，框架會呼叫你的程式元件，而不是由你來呼叫框架。 例如，使用程式庫的話，主要流程的控制是這樣的： 灰色部份是可以自行掌控的流程，並在過程中必要時機，引用各種程式庫，當然，執行程式庫的過程中，會暫時進入程式庫的流程，不過，絕大多數的情況下，你對應用程式的主要流程，還是擁有很大的控制權。 使用框架的話，主要流程的控制是這樣的： 這就是控制權反轉（Inverse of Control, IoC）的解讀之一，使用框架的話必須注意，流程控制權會從你的手中反轉至框架身上！ 當然，框架本質上也是個程式庫，不過會被定位為框架，表示它對程式主要流程擁有更多的控制權，然而，框架本身是個半成品，想要完成整個流程，必須在框架的流程規範下，實現自定義元件，如上圖表示的，灰色部份是可以自行掌控的部份，與使用程式庫相比，對流程控制的自由度少了許多。 應用程式開發時是否需要使用框架，有很多考量點，然而簡單來說，使用程式庫時，開發者會擁有較高的自由度；使用框架時，開發者會受到較大的限制。 若應用程式本身不複雜，不需要套用任何框架就可以完成這項任務，那麼就不要使用框架，因為若要使用框架，往往必須按照框架的規範實作定義檔、設定相關數值、依一定方式取得元件等，若應用程式本身並不複雜，在享用到框架的益處之前，就會被一堆繁瑣規範或設定給困擾，產生「有必要這麼複雜嗎？」的疑惑，若框架本身又很龐大，光是學習、掌握到熟練框架，也會付出很大的成本。 然而，如果應用程式在主要流程、組合元件甚至管理元件生命週期管理等需求上，自行撰寫程式碼完成任務已成沉重負擔之時，而框架對這些需求，若能大致符合既有之需求，套用框架可省去自行撰寫、維護元件生命週期等程式碼的麻煩，這時換取而來的益處超越了犧牲掉的流程自由度，才會使得使用框架具有意義。 類似地，想要套用 Spring MVC 之類的 Web 框架嗎？那要先問自己，應用程式打算遵照或已經是 MVC 流程架構了嗎？如果不是，那使用 Spring MVC 並不會為你帶來益處，反而會感到處處受限，正如股市有句名言：「好的老師帶你上天堂，不好的老師帶你住套房。」用框架時可以這麼想：「好的框架帶你上天堂，不好的框架帶你住套房。」 在程式設計相關的領域中，名詞多半沒有嚴謹的定義，這邊談到的 Inversion of Control，是在使用任何框架前必須要有的認知，然而，並不是 Spring IoC 核心中的 Inversion of Control 概念，由於經常有人將兩者混淆在一起，後來 Martin Fowler 建議使用 Dependency Injection 來取代 Spring 中 對 IoC 的稱呼。 原文 https://openhome.cc/Gossip/Spring/IoC.html ","tags":[],"title":"框架和函数库","link":"https://toomi.pages.dev/post/kuang-jia-he-han-shu-ku/","stats":{"text":"4 min read","time":237000,"words":1152,"minutes":4},"dateFormat":"2023-02-21"},{"content":"SqlSessionFactoryBean的主要作用便是用来创建SqlSessionFactory，在它的构建过程中会解析MyBatis所需要的配置文件，将所有配置都封装到Configuration类中，最后返回SqlSessionFactory实例。SqlSessionFactoryBean实现了Spring中两个用于初始化Bean的接口FactoryBean,InitializingBean InitializingBean定义如下: public interface InitializingBean { /** * 此方法在BeanFactory设置了Bean配置里的所有属性后执行。 */ void afterPropertiesSet() throws Exception; } 我们一般在XML中都会有这么一段配置，如下: &lt;bean id=&quot;sqlSessionFactory&quot; class=&quot;org.mybatis.spring.SqlSessionFactoryBean&quot;&gt; &lt;!-- 注入连接池 --&gt; &lt;property name=&quot;dataSource&quot; ref=&quot;dataSource&quot;/&gt; &lt;!--主配置 --&gt; &lt;property name=&quot;configuration&quot;&gt; &lt;bean class=&quot;org.apache.ibatis.session.Configuration&quot;&gt; &lt;property name=&quot;mapUnderscoreToCamelCase&quot; value=&quot;true&quot;/&gt; &lt;/bean&gt; &lt;/property&gt; &lt;!-- 加载mybatis映射文件 --&gt; &lt;property name=&quot;mapperLocations&quot; value=&quot;classpath:mappers/*.xml&quot;/&gt; &lt;/bean&gt; 上述配置的作用就是让容器创建一个ID为sqlSessionFactory的SqlSessionFactoryBean实例，并且为实例指定了datasource属性，手动指定了主配置对象，开启MyBatis下划线转驼峰的属性配置，并且指定映射文件所在的位置。 虽然指定的是SqlSessionFactoryBean实例，但是因为继承了FactoryBean接口，因此我们从容器拿到的对象实际上是getObject方法返回的对象，即SqlSessionFactory实例。 创建流程 首先自不用说，Spring会创建SqlSessionFactoryBean实例，设置配置的所有属性，这是第一步，接下来便会走afterPropertiesSet()方法。 @Override public void afterPropertiesSet() throws Exception { // 检查数据源 notNull(dataSource, &quot;Property 'dataSource' is required&quot;); // 检查SqlSessionFactoryBuilder, 已经在构造方法里初始化 notNull(sqlSessionFactoryBuilder, &quot;Property 'sqlSessionFactoryBuilder' is required&quot;); // configuration实例与configLocation只能有一个 state((configuration == null &amp;&amp; configLocation == null) || !(configuration != null &amp;&amp; configLocation != null), &quot;Property 'configuration' and 'configLocation' can not specified with together&quot;); // 解析配置文件，创建SqlSessionFactory。这属于MyBatis本身的内容，不再展开说明。 this.sqlSessionFactory = buildSqlSessionFactory(); } 创建SqlSessionFactory的过程与MyBatis不同的点主要在与transactionFactory，在创建事务工厂时不再使用MyBatis自带的JdbcTransaction类，而是使用SpringManagedTransactionFactory这个新的事务工厂类，这个事务获取到的连接全部来自于Spring管理，这样把事务全权交给Spring管理，而MyBatis本身不再涉及事务管理。如果在使用Spring时没有配置(使用)事务，那么获取到的连接取决于DataSource，无论拿到的连接是自动提交还是手动提交，MyBatis每一次方法调用都会视情况提交或回滚。当然对于自动提交而言，MyBatis的commit或者rollbakc是不会调用conn.commit()或conn.rollback。 在经过afterPropertiesSet方法后，SqlSessionFactory实例便创建完毕。需要使用SqlSessionFactory实例便会调用getObject方法。 @Override public SqlSessionFactory getObject() throws Exception { if (this.sqlSessionFactory == null) { afterPropertiesSet(); } return this.sqlSessionFactory; } 原文 https://www.cnblogs.com/wt20/p/10470872.html ","tags":[],"title":"SqlSessionFactoryBean的构建流程","link":"https://toomi.pages.dev/post/sqlsessionfactorybean-de-gou-jian-liu-cheng/","stats":{"text":"3 min read","time":173000,"words":695,"minutes":3},"dateFormat":"2023-02-17"},{"content":"Have you always been using @Service, @Component, @Contoller or @ Bean to add your Spring beans? Have you seen some of the others’ code that looks fancy in injecting the beans but don’t know how it achieves the same purpose? Let’s take a look at some common ways to inject Spring beans into the container. 1. @Configuration + @Bean This might be the most common one. @ Configuration declares a configuration class and we can use @ Bean to add the bean into the container. @Configuration public class MyConfiguration { @Bean public Person person() { Person person = new Person(); person.setName(&quot;spring&quot;); return person; } } 2. @Component + @ComponentScan For the same example, we can declare the Person object as a Component. We will then use @ComponentScan “basePackages” to declare the paths for the components. @Component public class Person { private String name; public String getName() { return name; } public void setName(String name) { this.name = name; } @Override public String toString() { return &quot;Person{&quot; + &quot;name='&quot; + name + '\\'' + '}'; } } @ComponentScan(basePackages = &quot;com.springboot.initbean.*&quot;) public class Demo1 { public static void main(String[] args) { AnnotationConfigApplicationContext applicationContext = new AnnotationConfigApplicationContext(Demo1.class); Person bean = applicationContext.getBean(Person.class); System.out.println(bean); } } 3. @Import There are various flexible ways to use this annotation. 3.1 Direct import bean Direct import bean. Once we are done constructing the Person class, we can then import the class into the Spring container. public class Person { private String name; public String getName() { return name; } public void setName(String name) { this.name = name; } @Override public String toString() { return &quot;Person{&quot; + &quot;name='&quot; + name + '\\'' + '}'; } } @Import(Person.class) public class Demo1 { public static void main(String[] args) { AnnotationConfigApplicationContext applicationContext = new AnnotationConfigApplicationContext(Demo1.class); Person bean = applicationContext.getBean(Person.class); System.out.println(bean); } } 3.2 @Import + ImportSelector We can also define a MyImportSelector to realize ImportSelector interface and override selectImports method, in which we will specify beans we need. Now we only need to import the MyImportSelector class. @Import(MyImportSelector.class) public class Demo1 { public static void main(String[] args) { AnnotationConfigApplicationContext applicationContext = new AnnotationConfigApplicationContext(Demo1.class); Person bean = applicationContext.getBean(Person.class); System.out.println(bean); } } class MyImportSelector implements ImportSelector { @Override public String[] selectImports(AnnotationMetadata importingClassMetadata) { return new String[]{&quot;com.springboot.pojo.Person&quot;}; } } 3.3 @Import + ImportBeanDefinitionRegistrar For this method, we also need to realize an interface ImportBeanDefinitionRegistrar. @Import(MyImportBeanDefinitionRegistrar.class) public class Demo1 { public static void main(String[] args) { AnnotationConfigApplicationContext applicationContext = new AnnotationConfigApplicationContext(Demo1.class); Person bean = applicationContext.getBean(Person.class); System.out.println(bean); } } class MyImportBeanDefinitionRegistrar implements ImportBeanDefinitionRegistrar { @Override public void registerBeanDefinitions(AnnotationMetadata importingClassMetadata, BeanDefinitionRegistry registry) { // construct beanDefinition AbstractBeanDefinition beanDefinition = BeanDefinitionBuilder.rootBeanDefinition(Person.class).getBeanDefinition(); // register the bean into the container registry.registerBeanDefinition(&quot;person&quot;, beanDefinition); } } The difference between this method and the previous one is an additional declaration of “beanDefinition” — the metadata of the bean. 4 @FactoryBean FactoryBean is a bean itself, which, by the way, not to be confused with BeanFactory that manages beans. @Configuration public class Demo1 { @Bean public PersonFactoryBean personFactoryBean() { return new PersonFactoryBean(); } public static void main(String[] args) { AnnotationConfigApplicationContext applicationContext = new AnnotationConfigApplicationContext(Demo1.class); Person bean = applicationContext.getBean(Person.class); System.out.println(bean); } } class PersonFactoryBean implements FactoryBean&lt;Person&gt; { @Override public Person getObject() throws Exception { return new Person(); } @Override public Class&lt;?&gt; getObjectType() { return Person.class; } } Here we use method 1 — @Configuration + @Bean to add PersonFactoryBean into the Spring container. However, notice that I didn’t inject Person bean directly in @Bean. Instead, PersonFactoryBean is injected. Spring container then gets the Person bean from the factory bean. 5 BeanDefinitionRegistryPostProcessor When Spring container executes postProcessBeanDefinitionRegistry method in BeanDefinitionRegistryPostProcesso, it will process the beanDefinition separately, which we can use to adjust the beanDefinition. public class Demo1 { public static void main(String[] args) { AnnotationConfigApplicationContext applicationContext = new AnnotationConfigApplicationContext(); MyBeanDefinitionRegistryPostProcessor beanDefinitionRegistryPostProcessor = new MyBeanDefinitionRegistryPostProcessor(); applicationContext.addBeanFactoryPostProcessor(beanDefinitionRegistryPostProcessor); applicationContext.refresh(); Person bean = applicationContext.getBean(Person.class); System.out.println(bean); } } class MyBeanDefinitionRegistryPostProcessor implements BeanDefinitionRegistryPostProcessor { @Override public void postProcessBeanDefinitionRegistry(BeanDefinitionRegistry registry) throws BeansException { AbstractBeanDefinition beanDefinition = BeanDefinitionBuilder.rootBeanDefinition(Person.class).getBeanDefinition(); registry.registerBeanDefinition(&quot;person&quot;, beanDefinition); } @Override public void postProcessBeanFactory(ConfigurableListableBeanFactory beanFactory) throws BeansException { } } ","tags":[],"title":"the ways to inject beans into Spring container","link":"https://toomi.pages.dev/post/the-ways-to-inject-beans-into-spring-container/","stats":{"text":"5 min read","time":253000,"words":677,"minutes":5},"dateFormat":"2023-02-14"},{"content":"Spring 大量引入了 Java 的 Reflection 机制，通过动态 调用的方式避免硬编码方式的约束，并在此基础上建立了其核心组件 BeanFactory，以此作为其依赖注入 机制的实现基础。 org.springframework.beans 包中包括了这些核心组件的实现类，核心中的核心为 BeanWrapper 和 BeanFactory 类。这两个类从技术角度而言并不复杂，但对于 Spring 框架而言，却是关键所在，如果 有时间，建议对其源码进行研读，必有所获。 Bean Wrapper 所谓依赖注入，即在运行期由容器将依赖关系注入到组件之中。 讲的通俗点，就是在运行期，由Spring根据配置文件，将其他对象的引用通过组件的提供的setter方法进 行设定。 我们知道，如果动态设置一个对象属性，可以借助Java的Reflection机制完成： Class cls = Class.forName(&quot;net.xiaxin.beans.User&quot;); Method mtd = cls.getMethod(&quot;setName&quot;,new Class[]{String.class}); Object obj = (Object)cls.newInstance(); mtd.invoke(obj,new Object[]{&quot;Erica&quot;}); return obj; 上面我们通过动态加载了User类，并通过Reflection调用了User.setName方法设置其name属性。 对于这里的例子而言，出于简洁，我们将类名和方法名都以常量的方式硬编码。假设这些常量都是通过配 置文件读入，那我们就实现了一个最简单的BeanWrapper。这个BeanWrapper的功能很简单，提供一个 设置JavaBean属性的通用方法（Apache BeanUtils 类库中提供了大量针对Bean的辅助工具，如果有兴 趣可以下载一份源码加以研读）。 Spring BeanWrapper基于同样的原理，提供了一个更加完善的实现。 看看如何通过Spring BeanWrapper操作一个JavaBean： Object obj = Class.forName(&quot;net.xiaxin.beans.User&quot;).newInstance(); BeanWrapper bw = new BeanWrapperImpl(obj); bw.setPropertyValue(&quot;name&quot;, &quot;Erica&quot;); System.out.println(&quot;User name=&gt;&quot;+bw.getPropertyValue(&quot;name&quot;)); 对比之前的代码，相信大家已经知道BeanWrapper的实现原理。 诚然，通过这样的方式设定Java Bean属性实在繁琐，但它却提供了一个通用的属性设定机制，而这 样的机制，也正是Spring依赖注入机制所依赖的基础。 通过BeanWrapper，我们可以无需在编码时就指定JavaBean的实现类和属性值，通过在配置文件 加以设定，就可以在运行期动态创建对象并设定其属性（依赖关系）。 上面的代码中，我们仅仅指定了需要设置的属性名“name”，运行期，BeanWrapper将根据Java Bean规范，动态调用对象的“setName”方法进行属性设定。属性名可包含层次，如对于属性名“address.zipcode”，BeanWrapper会调用“getAddress().setZipcode”方法。 Bean Factory Bean Factory，顾名思义，负责创建并维护Bean实例。 Bean Factory负责根据配置文件创建Bean实例，可以配置的项目有： 1． Bean属性值及依赖关系（对其他Bean的引用） 2． Bean创建模式（是否Singleton模式，即是否只针对指定类维持全局唯一的实例） 3． Bean初始化和销毁方法 4． Bean的依赖关系 联合上面关于BeanWrapper的内容，我们可以看到，BeanWrapper实现了针对单个Bean的属性设 定操作。而BeanFactory则是针对多个Bean的管理容器，根据给定的配置文件，BeanFactory从中读取 类名、属性名/值，然后通过Reflection机制进行Bean加载和属性设定。 ApplicationContext BeanFactory提供了针对Java Bean的管理功能，而ApplicationContext提供了一个更为框架化的 实现（从上面的示例中可以看出，BeanFactory的使用方式更加类似一个API，而非Framework style）。 ApplicationContext覆盖了BeanFactory的所有功能，并提供了更多的特性。此外， ApplicationContext为与现有应用框架相整合，提供了更为开放式的实现（如对于Web应用，我们可以在 web.xml中对ApplicationContext进行配置）。 相对BeanFactory而言，ApplicationContext提供了以下扩展功能： 国际化支持 我们可以在Beans.xml文件中，对程序中的语言信息（如提示信息）进行定义，将程序中的提示 信息抽取到配置文件中加以定义，为我们进行应用的各语言版本转换提供了极大的灵活性。 资源访问 支持对文件和URL的访问。 事件传播 事件传播特性为系统中状态改变时的检测提供了良好支持。 多实例加载 可以在同一个应用中加载多个Context实例。 小结 BeanFactory 是 Spring 框架的基础设施，面向 Spring 本身；ApplicationContext 面向使用Spring 框架的开发者，几乎所有的应用场合我们都直接使用 ApplicationContext 而非底层的 BeanFactory。 参考 http://www.kaiyuanba.cn/content/develop/SpringGuide.pdf ","tags":[],"title":"Bean Wrapper 、  BeanFactory 、ApplicationContext等核心概念","link":"https://toomi.pages.dev/post/bean-wrapper-beanfactory-applicationcontext/","stats":{"text":"5 min read","time":273000,"words":1217,"minutes":5},"dateFormat":"2023-02-13"},{"content":"简介 在软件工程中，依赖注入（dependency injection）的意思为，给予调用方它所需要的事物。“依赖”是指可被方法调用的事物。依赖注入形式下，调用方不再直接指使用“依赖”，取而代之是“注入” 。“注入”是指将“依赖”传递给调用方的过程。在“注入”之后，调用方才会调用该“依赖”。传递依赖给调用方，而不是让让调用方直接获得依赖，这个是该设计的根本需求。在编程语言角度下，“调用方”为对象和类，“依赖”为变量。在提供服务的角度下，“调用方”为客户端，“依赖”为服务。 在理解它在编程中的含义之前，首先让我们了解一下它的总体含义，这可以帮助我们更好地理解这个概念。 依赖是指依靠某种东西来获得支持。比如我会说我们对手机的依赖程度过高。 在讨论依赖注入之前，我们先理解编程中的依赖是什么意思。 当 class A 使用 class B 的某些功能时，则表示 class A 具有 class B 依赖。 在 Java 中，在使用其他 class 的方法之前，我们首先需要创建那个 class 的对象（即 class A 需要创建一个 class B 实例）。 因此，将创建对象的任务转移给其他 class，并直接使用依赖项的过程，被称为“依赖项注入”。 为什么需要使用依赖注入 假设我们有一个 car class，其中包含各种对象，例如车轮、引擎等。 这里的 car class 负责创建所有依赖对象。现在，如果我们决定将来放弃 MRFWheels，而希望使用 Yokohama 车轮，该怎么办？ 我们将需要使用新的 Yokohama 依赖关系来重新创建 car 对象。但是，当使用依赖注入（DI）时，我们可以在运行时更改车轮 wheels（因为可以在运行时而不是在编译时注入依赖项）。 你可以将依赖注入视为代码中的中间人，它负责创建想要的 wheels 对象，并将其提供给car class。 它使 car class 不需要创建车轮 wheels、电池 battery 对象等。 三种类型的依赖注入 构造函数注入：依赖关系是通过 class 构造器提供的。 setter 注入：注入程序用客户端的 setter 方法注入依赖项。 接口注入：依赖项提供了一个注入方法，该方法将把依赖项注入到传递给它的任何客户端中。客户端必须实现一个接口，该接口的 setter 方法接收依赖。 如果对象有任何更改，则依赖注入会对其进行调查，并且不应影响到使用这些对象的类。这样，如果将来对象发生变化，则依赖注入负责为类提供正确的对象。 控制反转——依赖注入背后的概念 这是指一个类不应静态配置其依赖项，而应由其他一些类从外部进行配置。 这是 S.O.L.I.D 的第五项原则——一类应该依赖于抽象，而不是依赖于具体的东西（简单地说，就是硬编码）。 根据这些原则，一个类应该专注于履行其职责，而不是创建履行这些职责所需的对象。 这就是依赖注入发挥作用的地方：它为类提供了必需的对象。 ","tags":[],"title":"依赖注入","link":"https://toomi.pages.dev/post/yi-lai-zhu-ru/","stats":{"text":"4 min read","time":185000,"words":890,"minutes":4},"dateFormat":"2023-02-09"},{"content":"SELECT (total_elapsed_time / execution_count)/1000 N'平均时间ms' ,total_elapsed_time/1000 N'总花费时间ms' ,total_worker_time/1000 N'所用的CPU总时间ms' ,total_physical_reads N'物理读取总次数' ,total_logical_reads/execution_count N'每次逻辑读次数' ,total_logical_reads N'逻辑读取总次数' ,total_logical_writes N'逻辑写入总次数' ,execution_count N'执行次数' ,SUBSTRING(st.text, (qs.statement_start_offset/2) + 1, ((CASE statement_end_offset WHEN -1 THEN DATALENGTH(st.text) ELSE qs.statement_end_offset END - qs.statement_start_offset)/2) + 1) N'执行语句' ,creation_time N'语句编译时间' ,last_execution_time N'上次执行时间' FROM sys.dm_exec_query_stats AS qs CROSS APPLY sys.dm_exec_sql_text(qs.sql_handle) as st WHERE SUBSTRING(st.text, (qs.statement_start_offset/2) + 1, ((CASE statement_end_offset WHEN -1 THEN DATALENGTH(st.text) ELSE qs.statement_end_offset END - qs.statement_start_offset)/2) + 1) not like '%fetch%' and creation_time between '2012-05-07 01:35:00.000' and '2020-05-07 01:35:00.000' ORDER BY -- last_execution_time DESC total_elapsed_time / execution_count DESC; 查看当前执行的语句 SELECT [Spid] = session_Id, ecid, [Database] = DB_NAME(sp.dbid), [User] = nt_username, [Status] = er.status, [Wait] = wait_type, [Individual Query] = SUBSTRING(qt.text, er.statement_start_offset / 2, (CASE WHEN er.statement_end_offset = - 1 THEN LEN(CONVERT(NVARCHAR(MAX), qt.text)) * 2 ELSE er.statement_end_offset END - er.statement_start_offset) / 2), [Parent Query] = qt.text, Program = program_name, Hostname, nt_domain, start_time FROM sys.dm_exec_requests er INNER JOIN sys.sysprocesses sp ON er.session_id = sp.spid CROSS APPLY sys.dm_exec_sql_text(er.sql_handle) AS qt WHERE session_Id &gt; 50 /* Ignore system spids.*/ AND session_Id NOT IN (@@SPID) ","tags":[],"title":"SQL SERVER 慢查询日志","link":"https://toomi.pages.dev/post/sql-server-man-cha-xun-ri-zhi/","stats":{"text":"2 min read","time":90000,"words":276,"minutes":2},"dateFormat":"2023-02-07"},{"content":"You may have faced this question in your interview that what is the difference between lock and a monitor? Well, to answer this question you must have good amount of understanding of how java multi-threading works under the hood. Short answer, locks provide necessary support for implementing monitors. Long answer read below. Locks A lock is kind of data which is logically part of an object’s header on the heap memory. Each object in a JVM has this lock (or mutex) that any program can use to coordinate multi-threaded access to the object. If any thread want to access instance variables of that object; then thread must “own” the object’s lock (set some flag in lock memory area). All other threads that attempt to access the object’s variables have to wait until the owning thread releases the object’s lock (unset the flag). Once a thread owns a lock, it can request the same lock again multiple times, but then has to release the lock the same number of times before it is made available to other threads. If a thread requests a lock three times, for example, that thread will continue to own the lock until it has “released” it three times. Please note that lock is acquired by a thread, when it explicitly ask for it. In Java, this is done with the synchronized keyword, or with wait and notify. Monitors Monitor is a synchronization construct that allows threads to have both mutual exclusion (using locks) and cooperation i.e. the ability to make threads wait for certain condition to be true (using wait-set). In other words, along with data that implements a lock, every Java object is logically associated with data that implements a wait-set. Whereas locks help threads to work independently on shared data without interfering with one another, wait-sets help threads to cooperate with one another to work together towards a common goal e.g. all waiting threads will be moved to this wait-set and all will be notified once lock is released. This wait-set helps in building monitors with additional help of lock (mutex). Mutual exclusion Putting in very simple words, a monitor is like a building that contains one special room (object instance) that can be occupied by only one thread at a time. The room usually contains some data which needs to be protected from concurrent access. From the time a thread enters this room to the time it leaves, it has exclusive access to any data in the room. Entering the monitor building is called “entering the monitor.” Entering the special room inside the building is called “acquiring the monitor.” Occupying the room is called “owning the monitor,” and leaving the room is called “releasing the monitor.” Leaving the entire building is called “exiting the monitor.” When a thread arrives to access protected data (enter the special room), it is first put in queue in building reception (entry-set). If no other thread is waiting (own the monitor), the thread acquires the lock and continues executing the protected code. When the thread finishes execution, it release the lock and exits the building (exiting the monitor). If when a thread arrives and another thread is already owning the monitor, it must wait in reception queue (entry-set). When the current owner exits the monitor, the newly arrived thread must compete with any other threads also waiting in the entry-set. Only one thread will win the competition and own the lock. There is no role of wait-set feature. Cooperation In general, mutual exclusion is important only when multiple threads are sharing data or some other resource. If two threads are not working with any common data or resource, they usually can’t interfere with each other and needn’t execute in a mutually exclusive way. Whereas mutual exclusion helps keep threads from interfering with one another while sharing data, cooperation helps threads to work together towards some common goal. Cooperation is important when one thread needs some data to be in a particular state and another thread is responsible for getting the data into that state e.g. producer/consumer problem where read thread needs the buffer to be in a “not empty” state before it can read any data out of the buffer. If the read thread discovers that the buffer is empty, it must wait. The write thread is responsible for filling the buffer with data. Once the write thread has done some more writing, the read thread can do some more reading. It is also sometimes called a “Wait and Notify” OR “Signal and Continue” monitor because it retains ownership of the monitor and continues executing the monitor region (the continue) if needed. At some later time, the notifying thread releases the monitor and a waiting thread is resurrected to own the lock. This cooperation requires both i.e. entry-set and wait-set. Below given diagram will help you in understand this cooperation. Above figure shows the monitor as three rectangles. In the center, a large rectangle contains a single thread, the monitor’s owner. On the left, a small rectangle contains the entry set. On the right, another small rectangle contains the wait set. I hope that above discussion will help you in getting more insight. Free free to ask any question. Happy Learning !! 原文 https://howtodoinjava.com/java/multi-threading/multithreading-difference-between-lock-and-monitor/ ","tags":[],"title":"Difference between lock and monitor","link":"https://toomi.pages.dev/post/difference-between-lock-and-monitor/","stats":{"text":"6 min read","time":341000,"words":912,"minutes":6},"dateFormat":"2023-02-04"},{"content":"Most efficient wait is LockSupport.park/unpark, which doesn't require nasty (direct) usage of Unsafe, and doesn't pay to resynchronize your thread's local cache of memory. This point is important; the less work you do, the more efficient. By not synchronizing on anything, you don't pay to have your thread check with main memory for updates from other threads. In most cases, this is NOT what you want. In most cases, you want your thread to see all updates that happened &quot;before now&quot;, which is why you should use Object.wait() and .notify(), as you must synchronize memory state to use them. LockSupport allows you to safely park a thread for a given time, and so long as no other thread tries to unpark you, it will wait for that long (barring spurious wake ups). If you need to wait for a specific amount of time, you need to recheck the deadline and loop back into park() until that time has actually elapsed. You can use it to &quot;sleep&quot; efficiently, without another thread to have to wake you up via LockSupport.parkNanos or .parkUntil (for millis; both methods just call Unsafe for you). If you do want other threads to wake you up, chances are high that you need memory synchronization, and should not use park (unless carefully orchestrating volatile fields without race conditions is your thing). Good luck, and happy coding! You're not supposed to use either of these methods if you're an application programmer. They are both too low level, easy to screw up and not meant to be used outside libraries. Why not try to use a higher level construct like java.util.concurrent.locks ? To answer your question. park(...) works directly on the thread. It takes the thread as a parameter and puts it to sleep until unpark is called on the thread, unless unpark has already been called. It's supposed to be faster than Object.wait(), which operates on the monitor abstraction if you know which thread you need to block/unblock. Btw unpark is not really that Unsafe if used from inside Java: public native void unpark(Object thread) Unblock the given thread blocked on park, or, if it is not blocked, cause the subsequent call to park not to block. Note: this operation is &quot;unsafe&quot; solely because the caller must somehow ensure that the thread has not been destroyed. Nothing special is usually required to ensure this when called from Java (in which there will ordinarily be a live reference to the thread) but this is not nearly-automatically so when calling from native code. LockSupport.park/unpark has better performance, but it's too low level API. Besides, they have some different operations maybe you should notice Object lockObject = new Object(); Runnable task1 = () -&gt; { synchronized (lockObject) { System.out.println(&quot;thread 1 blocked&quot;); try { lockObject.wait(); System.out.println(&quot;thread 1 resumed&quot;); } catch (InterruptedException e) { // TODO Auto-generated catch block e.printStackTrace(); } } }; Thread thread1 = new Thread(task1); thread1.start(); Runnable task2 = () -&gt; { System.out.println(&quot;thread 2 running &quot;); synchronized (lockObject) { System.out.println(&quot;thread 2 get lock&quot;); lockObject.notify(); } }; Thread thread2 = new Thread(task2); thread2.start(); In this case, thread2 can get lock and notify the thread1 to resumed, because lockObject.wait(); will release the lock. Object lockObject = new Object(); Runnable task1 = () -&gt; { synchronized (lockObject) { System.out.println(&quot;thread 1 blocked&quot;); LockSupport.park(); System.out.println(&quot;thread 1 resumed&quot;); } }; Thread thread1 = new Thread(task1); thread1.start(); Runnable task2 = () -&gt; { System.out.println(&quot;thread 2 running &quot;); synchronized (lockObject) { System.out.println(&quot;thread 2 get lock&quot;); LockSupport.unpark(thread1); } }; Thread thread2 = new Thread(task2); thread2.start(); However, if you use LockSupport.park/unpark like this, it will cause dead lock. because thread1 won't release the lock by using LockSupport.park. therefore, thread1 can't resumed. So be careful, they have different behaviors besides blocking the thread. And in fact, there are some Class we can use it conveniently to coordinate in multi-thread environment, such as CountDownLatch, Semaphore, ReentrantLock 原文 https://newbedev.com/unsafe-park-vs-object-wait ","tags":[],"title":"Unsafe.park vs Object.wait","link":"https://toomi.pages.dev/post/unsafepark-vs-objectwait/","stats":{"text":"5 min read","time":263000,"words":704,"minutes":5},"dateFormat":"2023-02-02"},{"content":"首先看java中线程状态轮转图： 可以看到： 阻塞（synchronized） 等待（Object,wait()，Object.join()，LockSupport.park()） 超时等待（Thread.sleep(long)，Object.wait(long)，Thread.join(long)，LockSupport.parkNano()，LockSupport.parkUntil()） 名称解释 阻塞（BLOCKED） A thread that is blocked waiting for a monitor lock is in this state. 一个线程因为等待临界区的锁被阻塞产生的状态，synchronize 关键字产生的状态。 等待（WAITING） A thread that is waiting indefinitely for another thread to perform a particular action is in this state. 当wait，join，park方法调用时，进入waiting状态。前提是这个线程已经拥有锁了。 进入 waiting 状态是线程主动的，而进入 blocked 状态是被动的。 进入 blocked 状态是在同步（synchronized）代码之外，而进入 waiting 状态是在同步代码之内（然后马上退出同步）。 Thread Thread类里有两个ParkEvent和一个Parker， 其实ParkEvent和Parker实现和功能都类似，只是源码没有重构而已。 一个是给实现synchronized关键字用的； 一个是给Thread.sleep/wait用的； parker是用来实现J.U.C(Unsafe)的park/unpark(阻塞/唤醒)； 上面提到，WAITING状态的线程，主要指调用Object,wait()，Object.join()，LockSupport.park()方法的线程操作，其中AQS的挂起，也就是调用LockSupport.park()，恢复则调用LockSupport.unPark()。AQS.Condition.await()，内部也是调用LockSupport.park()； 原文 https://giteedev.gitee.io/yyz-coder/2020/08/20/java%E4%B8%AD%E9%98%BB%E5%A1%9E%E5%92%8C%E7%AD%89%E5%BE%85%E7%8A%B6%E6%80%81%E7%9A%84%E5%8C%BA%E5%88%AB%E6%98%AF%E5%95%A5%E7%8E%A9%E6%84%8F%E5%84%BF%EF%BC%9F/ ","tags":[],"title":"java中阻塞和等待状态的区别","link":"https://toomi.pages.dev/post/java-zhong-zu-sai-he-deng-dai-zhuang-tai-de-qu-bie/","stats":{"text":"2 min read","time":105000,"words":391,"minutes":2},"dateFormat":"2023-02-02"},{"content":"Java’s concurrency support has changed significantly over the last 20 years to mostly reflect the changes in hardware, software systems, and programming concepts. Going through the evolution of Java concurrency support can help in understanding the reason for the new additions and their roles. Java’s Initial Concurrency Support Initially, Java had threads and built-in monitor objects, which are supported by locks (via synchronized classes and methods), Runnable and Thread interfaces. The built-in monitor objects allow mutual exclusion and coordination among threads. The focus was on basic multi-threading and synchronisation primitives. They are efficient but low-level and very limited in capabilities. They are also difficult and error-prone to use, which can cause accidental complexity As per JDK 1.x release, there were only few classes present in this initial release. To be very specific, there classes/interfaces were: java.lang.Thread java.lang.ThreadGroup java.lang.Runnable java.lang.Process java.lang.ThreadDeath and some exception classes java.lang.IllegalMonitorStateException java.lang.IllegalStateException java.lang.IllegalThreadStateException. It also had few synchronized collections e.g. java.util.Hashtable. JDK 1.2 and JDK 1.3 had no noticeable changes related to multi-threading. (Correct me if I have missed anything). JDK 1.4, there were few JVM level changes to suspend/resume multiple threads with single call. But no major API changes were present. Java 5 Concurrency Changes n 2004, Java 5 introduced new concurrency support: Java Executor framework, synchronizers, blocking queues, atomics, and concurrent collections. JDK 1.5 was first big release after JDK 1.x; and it had included multiple concurrency utilities. Executor, semaphore, mutex, barrier, latches, concurrent collections and blocking queues; all were included in this release itself. The biggest change in java multi-threading applications cloud happened in this release. Read full set of changes in this link: http://docs.oracle.com/javase/1.5.0/docs/guide/concurrency/overview.html It was a big step forward with better support compared to the earlier one. The idea of thread pools as a higher-level idea capturing the power of threads allowed developers to decouple task submission from task execution. It also enabled coarse-grained task parallelism (there was not a lot of computing power at that time), which is Submit tasks to execute so that they can run in a pool of worker threads The results are computed and put into a queue The queue can be accessed by other threads to get the results. Also by Future and Callable interfaces, it provided higher-level and result-returning variants of Runnable and Thread. It is feature-rich and optimized but also difficult and error-prone as still uses low-levels of abstraction. Java 7 Concurrency Changes Java 1.7 added foundational parallelism support. It provided data parallelism that runs the same tasks on different data elements. It added RecursiveTask to support fork-join approach of divide-and-conquer algorithms. Fork-join approach is powerful and scalable but tricky to program correctly Java 8 Concurrency Changes Java 1.8 provided advanced level parallelism, which is support for streams and their parallel processing by building on the newly added support for lambdas. So, the focus was on functional programming for data parallelism and asynchronous programming. It added support for composing Futures via the CompletableFutures( implementation of Future) It provided an effective balance between productivity and performance Java 9 Concurrency Changes Changes Java 9, provided explicit support for distributed asynchronous programming. It is basically the Flow API based on the publish-subscribe protocol, including back-pressure, and forms the basis for reactive programming. It is a set of interfaces for publish-subscribe model to standardize the concepts to maximize the interoperability of different implementations. Which one to use? As the new changes came in, the level of abstraction has been increased. It is good to learn the constructs on all levels. Learning low-level constructs will help in understanding the high-level constructs better and how they work underneath. As a rule of thumb, it is always good to use the simplest and most efficient option. So, it is better to start with using the most high-level option, which is Java 8’s concurrency/parallelism frameworks (i.e parallel streams and completable futures) or Java 9’s reactive programming) if it fits your needs. Consider using lower-level constructs only if needed, which should be rare at the application level. 来源 https://www.turgaykivrak.com/posts/2021-12-14-evolution-java-concurrency/ ","tags":[],"title":"Evolution of Java Concurrency","link":"https://toomi.pages.dev/post/evolution-of-java-concurrency/","stats":{"text":"5 min read","time":280000,"words":750,"minutes":5},"dateFormat":"2023-02-02"},{"content":"1. 背景 1.1. SMP(Symmetric Multi-Processor) 对称多处理器结构，它是相对非对称多处理技术而言的、应用十分广泛的并行技术。 操作系统将任务队列对称地分布于多个CPU之上，从而极大地提高了整个系统的数据处理能力。 但是随着CPU数量的增加，每个CPU都要访问相同的内存资源，共享资源可能会成为系统瓶颈，导致CPU资源浪费。 1.2. NUMA(Non-Uniform Memory Access) 非一致存储访问，将CPU分为CPU模块，每个CPU模块由多个CPU组成，并且具有独立的本地内存、I/O槽口等，模块之间可以通过互联模块相互访问。 访问本地内存（本CPU模块的内存）的速度将远远高于访问远程内存(其他CPU模块的内存)的速度，这也是非一致存储访问的由来。 NUMA较好地解决SMP的扩展问题，当CPU数量增加时，因为访问远地内存的延时远远超过本地内存，系统性能无法线性增加。 2. 实现原理 2.1 自旋锁（Spin Lock） 自旋锁是指当一个线程尝试获取某个锁时，如果该锁已被其他线程占用，就一直循环检测锁是否被释放，而不是进入线程挂起或睡眠状态。 自旋锁适用于锁保护的临界区很小的情况，临界区很小的话，锁占用的时间就很短。 简单自旋锁实现示例： public class SimpleSpinLock implements Lock { //利用CAS操作，解决多线程并发操作导致数据不一致的问题 private final AtomicReference&lt;Thread&gt; owner = new AtomicReference&lt;&gt;(); @Override public void lock() { Thread currentThread = Thread.currentThread(); // 如果锁未被占用，则设置当前线程为锁的拥有者 while (!owner.compareAndSet(null, currentThread)) { } } @Override public void unlock() { Thread currentThread = Thread.currentThread(); // 只有锁的拥有者才能释放锁 owner.compareAndSet(currentThread, null); } ... } 缺点如下： 独占，不支持重入 CAS操作需要系统底层硬件的配合 保证各个CPU的缓存（L1、L2、L3、跨CPU Socket、主存）的数据一致性，通讯开销很大，在多处理器系统上更严重 没法保证公平性，不保证等待线程按照FIFO顺序获得锁 2.2 自旋可重入锁 在自旋锁基础上加入计数器，支持可重入 实现示例： public class ReentrantSpinLock implements Lock { //利用CAS操作，解决多线程并发操作导致数据不一致的问题 private AtomicReference&lt;Thread&gt; owner = new AtomicReference&lt;&gt;(); //加入计数器，支持可重入 private int count = 0; @Override public void lock() { Thread currentThread = Thread.currentThread(); //如果锁被当前线程占用，则计数器+1 if (currentThread == owner.get()) { count++; return; } // 如果锁未被占用，则设置当前线程为锁的拥有者 while (!owner.compareAndSet(null, currentThread)) { } } @Override public void unlock() { Thread currentThread = Thread.currentThread(); if (currentThread == owner.get()) { //count大于0，说明锁被占用多次，执行-1 if (count &gt; 0) { count--; } else { //执行锁释放 owner.compareAndSet(currentThread, null); } } } ... } 2.3 排队自旋锁 为了解决上面的公平性问题，类似于现实中银行柜台的排队叫号：锁拥有一个服务号，表示正在服务的线程，还有一个排队号 每个线程尝试获取锁之前先拿一个排队号，然后不断轮询锁的当前服务号是否是自己的排队号，如果是，则表示自己拥有了锁，不是则继续轮询 实现示例： public class TicketLock { private AtomicLong serviceNum = new AtomicLong();//服务号 private AtomicLong ticketNum = new AtomicLong();//排队号 public long lock() { //首先原子性地获得一个排队号 long myTicketNum = ticketNum.getAndIncrement(); // 只要当前服务号不是自己的就不断轮询 while (serviceNum.get() != myTicketNum) { } return myTicketNum; } public void unlock(long myTicketNum) { // 只有当前拥有者才能释放锁 serviceNum.compareAndSet(myTicketNum, myTicketNum + 1); } } 缺点: 独占，不可重入 虽然解决了公平性的问题，但是多处理器系统上，每个进程/线程占用的处理器都在读写同一个变量serviceNum ，每次读写操作都必须在多个处理器缓存之间进行缓存同步，这会导致繁重的系统总线和内存的流量，大大降低系统整体的性能 2.4 CLH锁 自旋公平锁（保证FIFO），独占锁，不可重入 CLH的发明人是：Craig，Landin and Hagersten，基于名称简写而来 CLH锁是一种基于单向链表的高性能、公平的自旋锁 申请加锁的线程通过前驱节点的变量进行自旋，在前置节点解锁后，当前节点会结束自旋，并进行加锁。 在SMP架构下，CLH更具有优势，在NUMA架构下，如果当前节点与前驱节点不在同一CPU模块下，跨CPU模块会带来额外的系统开销，而MCS锁更适用于NUMA架构。 实现示例如下： public class ClhSpinLock implements Lock { private final ThreadLocal&lt;Node&gt; prev; private final ThreadLocal&lt;Node&gt; node; private final AtomicReference&lt;Node&gt; tail; public ClhSpinLock() { this.node = ThreadLocal.withInitial(Node::new); this.prev = ThreadLocal.withInitial(() -&gt; null); this.tail = new AtomicReference&lt;Node&gt;(new Node()); } /** * 1.初始状态 tail指向一个node(head)节点 * +------+ * | head | &lt;---- tail * +------+ * * 2.lock-thread加入等待队列: tail指向新的Node，同时Prev指向tail之前指向的节点 * +----------+ * | Thread-A | * | := Node | &lt;---- tail * | := Prev | -----&gt; +------+ * +----------+ | head | * +------+ * * +----------+ +----------+ * | Thread-B | | Thread-A | * tail ----&gt; | := Node | --&gt; | := Node | * | := Prev | ----| | := Prev | -----&gt; +------+ * +----------+ +----------+ | head | * +------+ * 3.寻找当前node的prev-node然后开始自旋 * */ @Override public void lock() { final Node node = this.node.get(); node.locked = true; // 将tail设置为当前线程的节点，并获取到上一个节点，此操作为原子性操作 Node pred = this.tail.getAndSet(node); this.prev.set(pred); // 在前驱节点的locked字段上自旋等待 while (pred.locked) { } } @Override public void unlock() { final Node node = this.node.get(); // 将当前线程节点的locked属性设置为false，使下一个节点成功获取锁 node.locked = false; this.node.set(this.prev.get()); } ... private static class Node { private volatile boolean locked; } } 2.5 MCS锁 自旋公平锁（保证FIFO），独占锁，不可重入 MCS 来自于其发明人名字的首字母： John Mellor-Crummey和Michael Scott MSC与CLH最大的不同并不是链表是显示还是隐式，而是线程自旋的规则不同，CLH是在前趋结点的locked域上自旋等待，而MCS是在自己的结点的locked域上自旋等待 它解决了CLH在NUMA系统架构中获取locked域状态内存过远的问题 实现示例如下： public class McsSpinLock implements Lock { private final ThreadLocal&lt;Node&gt; current; private final AtomicReference&lt;Node&gt; tail; public McsSpinLock() { this.current = ThreadLocal.withInitial(Node::new); this.tail = new AtomicReference&lt;&gt;(); } @Override public void lock() { Node node = current.get(); Node pred = tail.getAndSet(node); // pred的初始值为null，所以第一个加锁线程，直接跳过判断，加锁成功 if (pred != null) { pred.next = node; // 在当前节点的locked字段上自旋等待 while (node.locked) { } } } @Override public void unlock() { Node node = current.get(); if (node.next == null) { // 如果设置成功，说明在此之前没有线程进行lock操作，直接return即可； // 如果失败，则说明在此之前有线程进行lock操作，需要自旋等待那个线程将自身节点设置为本线程节点的next， // 然后进行后面的操作。 if (tail.compareAndSet(node, null)) { return; } while (node.next == null) { } } // 通知下一个线程，使下一个线程加锁成功 node.next.locked = false; // 解锁后需要将节点之间的关联断开，否则会产生内存泄露ø node.next = null; } ... static class Node { volatile boolean locked = true; volatile Node next; } } 3、 CLH锁与MCS锁比较 对比说明 从代码实现来看，CLH比MCS要简单得多 从自旋的条件来看，CLH是在前驱节点的属性上自旋，而MCS是在本地属性变量上自旋 从链表队列来看，CLH的队列是隐式的，CLHNode并不实际持有下一个节点；MCS的队列是物理存在的 CLH锁释放时只需要改变自己的属性，MCS锁释放则需要改变后继节点的属性 原文 https://note.xcloudapi.com/2022/08/26/Java%E8%87%AA%E6%97%8B%E9%94%81%E3%80%81CLH%E9%94%81%E5%8F%8AMCS%E9%94%81%E5%8E%9F%E7%90%86%E5%8F%8A%E5%AE%9E%E7%8E%B0/ ","tags":[],"title":"Java自旋锁、CLH锁及MCS锁原理及实现","link":"https://toomi.pages.dev/post/java-zi-xuan-suo-clh-suo-ji-mcs-suo-yuan-li-ji-shi-xian/","stats":{"text":"9 min read","time":495000,"words":2031,"minutes":9},"dateFormat":"2023-01-11"},{"content":"synchronized Synchronized是Java中解决并发问题的一种最常用的方法，也是最简单的一种方法。Synchronized的作用主要有三个： 确保线程互斥的访问同步代码 保证共享变量的修改能够及时可见 有效解决重排序问题。 从语法上讲，Synchronized总共有三种用法： 修饰普通方法 修饰静态方法 修饰代码块 synchronized基于操作系统的管程(monitor)实现，对线程的阻塞和唤醒使用的是 Object的wait()和notify() AbstractQueuedSynchronizer (AQS) AQS是一种提供了原子式管理同步状态、阻塞和唤醒线程功能以及CLH队列模型的简单框架。Java中的大部分同步类（Lock、Semaphore、ReentrantLock等）都是基于AbstractQueuedSynchronizer（简称为AQS）实现的，AQS同时提供了互斥模式（exclusive）和共享模式（shared）两种不同的同步逻辑。基于AQS同步器实现参考： ReentrantLock ReentrantReadWriteLock Semaphore CountDownLatch ThreadPoolExecutor.Worker AQS是纯基于 JDK 实现，对线程的阻塞和唤醒使用的是Condition的await()、signal()方法 区别和联系 以AQS的经典实现ReentrantLock为例，分析其和synchronized 的区别，相比于synchronized，ReentrantLock需要显式的获取锁和释放锁，相对现在基本都是用JDK7和JDK8的版本，从Synchronized引入了偏向锁，轻量级锁（自旋锁）后，ReentrantLock的效率和synchronized区别基本可以持平了，在两种方法都可用的情况下，官方甚至建议使用synchronized， 其实synchronized的优化我感觉就借鉴了ReenTrantLock中的CAS技术。 ReentrantLock等待可中断，当持有锁的线程长时间不释放锁的时候，等待中的线程可以选择放弃等待，转而处理其他的任务。 公平锁：synchronized和ReentrantLock默认都是非公平锁，但是ReentrantLock可以通过构造函数传参改变。只不过使用公平锁的话会导致性能急剧下降。 绑定多个条件：ReentrantLock可以同时绑定多个Condition条件对象。 ReentrantLock基于AQS(AbstractQueuedSynchronizer 抽象队列同步器)实现。synchronized 是 JVM 实现的，而 ReentrantLock 是 JDK 实现的。JVM实现依赖于操作系统在用户态和内核态进行切换，成本比较高。而AQS在用户态就把加锁问题解决，从而避免了在用户态和内核态进行切换。 synchronized比较简单，ReentrantLock需要lock()和unlock()来实现，较为复杂 锁的细粒度和灵活度：很明显ReenTrantLock优于synchronized 阻塞与自旋锁、挂起 阻塞是指线程跳出当前执行任务，由于某种原因暂时无法继续执行任务逻辑。而自旋和挂起可以使线程处于阻塞状态，所以自旋锁与挂起是阻塞的子集关系。 挂起和唤醒一个线程将会带来上下文切换的开销，而如果在一个低并发的环境下，线程频繁的挂起和唤醒将消耗大量的系统资源，与CUP密集型程序发生大量的上下文切换，从而增加调度开销降低吞吐量。因此在低并发的环境下（线程等待时间较短）通过自旋等待将可以完全避免上下文切换带来的系统开销。 自旋等待会使阻塞线程与其他线程竞争CUP的时间片，占有cup资源，尽管这种开销是很小的，但是当大量的线程长时间的去竞争CUP的时间片将给操作系统带来毁灭性的灾难。有些事情如果我们不把它推向极端，我们是不会知道有多荒谬。同样的我们只有在一个极端条件下，才能发现多线程长时间自旋等待的危害。 自旋等待与挂起两种方式的效率高低取决于上下文切换的开销以及成功获取锁之前需要等待的时间，如果等待时间短则适合使用自旋锁，如果等待时间长则适合使用挂起操作。---《Java并发编程实践》 Doug Lea在设计AQS的线程阻塞策略使用了自旋等待和挂起两种方式，通过挂起线程前的低频自旋保证了AQS阻塞线程上下文切换开销及CUP时间片占用的最优化选择。保证在等待时间短通过自旋去占有锁而不需要挂起，而在等待时间长时将线程挂起。实现锁性能的最大化。 参考 https://note.xcloudapi.com/2022/08/03/AQS%E5%8E%9F%E7%90%86%E5%8F%8A%E5%AE%9E%E6%88%98/ https://fhfirehuo.github.io/Attacking-Java-Rookie/Chapter07/ReentrantLock.html https://blog.csdn.net/qq_28275283/article/details/76697120 ","tags":[],"title":"synchronized 与 AbstractQueuedSynchronizer (AQS)","link":"https://toomi.pages.dev/post/synchronized-yu-abstractqueuedsynchronizer-aqs/","stats":{"text":"5 min read","time":254000,"words":1178,"minutes":5},"dateFormat":"2023-01-11"},{"content":"多线程这块知识的学习，真正的难点不在于多线程程序的逻辑有多复杂，而在于理清J.U.C包中各个多线程工具类之间的关系、特点及其使用场景（从整体到局部、高屋建瓴） J.U.C包简介 J.U.C并发包，即java.util.concurrent包，是JDK的核心工具包，是JDK1.5之后，由 Doug Lea实现并引入。 整个java.util.concurrent包，按照功能可以大致划分如下： juc-locks 锁框架 juc-atomic 原子类框架 juc-sync 同步器框架 juc-collections 集合框架 juc-executors 执行器框架 juc-locks 锁框架 早期的JDK版本中，仅仅提供了synchronizd、wait、notify等等比较底层的多线程同步工具，开发人员如果需要开发复杂的多线程应用，通常需要基于JDK提供的这些基础工具进行封装，开发自己的工具类。JDK1.5+后，Doug Lea根据一系列常见的多线程设计模式，设计了JUC并发包，其中java.util.concurrent.locks包下提供了一系列基础的锁工具，用以对synchronizd、wait、notify等进行补充、增强。 java.util.concurrent.locks包的结构如下： juc-atomic 原子类框架 早期的JDK版本中，如果要并发的对Integer、Long、Double之类的Java原始类型或引用类型进行操作，一般都需要通过锁来控制并发，以防数据不一致 从JDK1.5开始，引入了java.util.concurrent.atomic工具包，该包提供了许多Java原始/引用类型的映射类，如AtomicInteger、AtomicLong、AtomicBoolean，这些类可以通过一种“无锁算法”，线程安全的操作Integer、Long、Boolean等原始类型。 所谓“无锁算法”，我们在讲juc-locks锁框架系列中，已经接触过太多次了，其实底层就是通过Unsafe类实现的一种比较并交换的算法，大致的结构如下（具体入参，根据上下文有所不同）： boolean compareAndSet(expectedValue, updateValue); 当希望修改的值与expectedValue相同时，则尝试将值更新为updateValue，更新成功返回true，否则返回false。 java.util.concurrent.atomic包结构如下: juc-sync 同步器框架 这里的juc-sync同步器框架，是指java.util.concurrent包下一些辅助同步器类，每个类都有自己适合的使用场景： 同步器名称 作用 CountDownLatch 倒数计数器，构造时设定计数值，当计数值归零后，所有阻塞线程恢复执行；其内部实现了AQS框架 CyclicBarrier 循环栅栏，构造时设定等待线程数，当所有线程都到达栅栏后，栅栏放行；其内部通过ReentrantLock和Condition实现同步 Semaphore 信号量，类似于“令牌”，用于控制共享资源的访问数量；其内部实现了AQS框架 Exchanger 交换器，类似于双向栅栏，用于线程之间的配对和数据交换；其内部根据并发情况有“单槽交换”和“多槽交换”之分 Phaser 多阶段栅栏，相当于CyclicBarrier的升级版，可用于分阶段任务的并发控制执行；其内部比较复杂，支持树形结构，以减少并发带来的竞争 juc-collections 集合框架 这里的juc-collections集合框架，是指java.util.concurrent包下的一些同步集合类，按类型划分可以分为：符号表、队列、Set集合、列表四大类，每个类都有自己适合的使用场景，整个juc-collections集合框架的结构如下图： 其中阻塞队列的分类及特性如下表： 队列特性 有界队列 近似无界队列 无界队列 特殊队列 有锁算法 ArrayBlockingQueue LinkedBlockingQueue、LinkedBlockingDeque / PriorityBlockingQueue、DelayQueue 无锁算法 / / LinkedTransferQueue SynchronousQueue juc-executors 执行器框架 executors框架是整个J.U.C包中类/接口关系最复杂的框架，executors其实可以划分为3大块，每一块的核心都是基于Executor这个接口： 线程池 Future模式 Fork/Join框架 原文 https://www.yijiyong.com/java/juc/02-juccatogray.html https://segmentfault.com/u/ressmix ","tags":[],"title":"JUC包介绍","link":"https://toomi.pages.dev/post/juc-bao-jie-shao/","stats":{"text":"4 min read","time":229000,"words":1024,"minutes":4},"dateFormat":"2023-01-10"},{"content":"工厂设计模式是Java中最常用的设计模式之一。它是一种创建型设计模式，能够用于创建一个或多个类所需要的对象。有了这个工厂，我们就能集中的创建对象。 集中创建方式给我们带来了一些好处，例如： 能够很容易的改变类创建的对象或者创建对象的方式； 能够很容易限制对象的创建，例如：我们只能为a类创建N个对象； 能够很容易的生成有关对象创建的统计数据。 在Java中，我们通常使用两种方式来创建线程：继承Thread类和实现Runnable接口。Java还提供了一个接口，既ThreadFactory接口，用于创建你自己的线程对象工厂。 很多类中，例如：ThreadPoolExecutor，使用构造函数来接收ThreadFactory来作为参数。这个工厂参数将会在程序执行时创建新的线程。使用ThreadFactory，你能够自定义执行程序如何创建线程，例如为线程定义适当的名称、优先级，或者你甚至可以将它设定为守护线程。 ThreadFactory例子 在这个例子中，我们将学习如何通过实现一个ThreadFactory接口来创建一个有个性化名称的线程对象，同时，我们保存了线程对象的创建信息。 Task.java class Task implements Runnable { @Override public void run() { try { TimeUnit.SECONDS.sleep(2); } catch (InterruptedException e) { e.printStackTrace(); } } } CustomThreadFactory.java public class CustomThreadFactory implements ThreadFactory { private intcounter; private String name; private List&lt;String&gt; stats; public CustomThreadFactory(String name) { counter = 1; this.name = name; stats = new ArrayList&lt;String&gt;(); } @Override public Thread newThread(Runnable runnable) { Thread t = new Thread(runnable, name + &quot;-Thread_&quot; + counter); counter++; stats.add(String.format(&quot;Created thread %d with name %s on %s \\n&quot;, t.getId(), t.getName(), new Date())); return t; } public String getStats() { StringBuffer buffer = new StringBuffer(); Iterator&lt;String&gt; it = stats.iterator(); while (it.hasNext()) { buffer.append(it.next()); } return buffer.toString(); } } 为了使用上面的线程工厂，请看下面的执行程序： public static void main(String[] args) { CustomThreadFactory factory = new CustomThreadFactory(&quot;CustomThreadFactory&quot;); Task task = new Task(); Thread thread; System.out.printf(&quot;Starting the Threads\\n\\n&quot;); for (int i = 1; i &lt;= 10; i++) { thread = factory.newThread(task); thread.start(); } System.out.printf(&quot;All Threads are created now\\n\\n&quot;); System.out.printf(&quot;Give me CustomThreadFactory stats:\\n\\n&quot; + factory.getStats()); } 上面的代码中，ThreadFactory接口只有一个叫做newThread()的方法，它接收一个Runnable对象作为参数，同时返回一个Thread对象。当你实现ThreadFactory接口时，你必须重写这个方法。 原文 http://howtodoinjava.com/2015/01/06/creating-threads-using-java-util-concurrent-threadfactory/ ","tags":[],"title":"使用java.util.concurrent.ThreadFactory类创建线程","link":"https://toomi.pages.dev/post/shi-yong-javautilconcurrentthreadfactory-lei-chuang-jian-xian-cheng/","stats":{"text":"3 min read","time":163000,"words":630,"minutes":3},"dateFormat":"2023-01-06"},{"content":" Safety Mutual exclusion: At most one thread can be in CS at a time Liveness Progress: If no thread is currently in CS and threads are trying to access, one should eventually be able to enter the CS. Bounded waiting: Once a thread T starts trying to enter the CS, there is a bound on the number of times other threads get in. About liveness requirements Liveness requirements are mandatory for a solution to be useful Progress vs. Bounded waiting Progress: If no thread can enter CS, we don’t have progress. Bounded waiting: If thread A is waiting to enter CS while B repeatedly leaves and re-enters C.S. ad infinitum, we don’t have bounded waiting ","tags":[],"title":"Critical section: Definition of the problem","link":"https://toomi.pages.dev/post/critical-section-definition-of-the-problem/","stats":{"text":"1 min read","time":44000,"words":119,"minutes":1},"dateFormat":"2023-01-06"},{"content":"详情 锁 ","tags":[],"title":"操作系统-锁","link":"https://toomi.pages.dev/post/cao-zuo-xi-tong-suo/","stats":{"text":"1 min read","time":0,"words":3,"minutes":1},"dateFormat":"2022-12-30"},{"content":"演变 在缺乏專門硬體指令的狀況下，存在多種同步機制演算法: (但其實各自的假設) Dekker’s algorithm: 1960 年 Lamport’s bakery algorithm: 1973 年 Peterson’s algorithm: 1981 年 現代的電腦硬體都有簡單且明確的指令可達到相同的效果，不用再顧慮上述演算法的 willingness vs. turn，換言之，這些軟體的同步演算法已無實用價值。 Edsger Dijkstra 提出 semaphore 概念時，被廣泛用於 CS 跟 signaling 這兩類問題。 signaling 是 process/threads 之間要通知彼此 「某些條件已經改變」 的情境：例如 producer-consumer 類型的問題可用兩個 semaphore 來實作，一個代表 「有資料可用」，另一個代表 「有空間可放東西」。 隨著電腦硬體逐漸提供 atomic 指令後，mutex 或稱為 lock 的機制被列入作業系統的實作考量: 需要進入 CS 時， 用 mutex/lock —— 上鎖/解鎖永遠是同一個 thread/process; 需要處理 signalling 時，用 semaphore —— 等待/通知通常是不同的 threads/processes; 簡言之，要搶資源時用 mutex，要互相通知時用 semaphore。 既生瑜，何生亮？ 先來看 semaphore 與 mutex 的差異，參照 Michael Barr 的文章 Mutexes and Semaphores Demystified: mutex 與 Semaphore 都用於確保 critical section (以下簡稱 CS) 的正確，讓多個執行單元運作並存取資源時，執行結果不會因為執行單元的時間先後的影響而導致錯誤。 mutex 與 semaphore 的差別在於： process 使用 mutex 時，process 的運作是持有 mutex，執行 CS 來存取資源，然後釋放 mutex Mutex 就像是資源的一把鎖：解鈴還須繫鈴人 process 使用 semaphore 時，process 總是發出信號 (signal)，或者總是接收信號 (wait)，同一個 process 不會先後進行 signal 與 wait 換言之，process 要不擔任 producer，要不充當 consumer 的角色，不能兩者都是。semaphore 是為了保護 process 的執行同步正確; mutex 與 semaphore 兩者設計來解決不同的問題。區分 mutex 與 binary semaphore： mutex 確保數個 process 在一個時間點上，只能有一個 process 存取單項資源; semaphore 讓數個 producer 與數個 consumer 在計數器的基礎上進行合作; 补充 Mutex 與 semaphore 是經常被混用的觀念(尤其是 binary semaphore = mutex 這種理解)，做為系統的開發者，勢必需要釐清其中的差異。 對於 mutex 來說，解鎖只能由上鎖的 thread 來完成，經常使用的情境是對於資源的保護 / 互斥。就像是對於一個房間，只存在一把鑰匙，拿到的人就可以進入房間，而其他的人必須排隊等待，直到那個人出來。 semaphore 可以由原本的 thread 或是另外一個 thread 解開，因此可以讓數個 producer 與數個 consumer 在計數器的基礎上進行合作。經常的使用情境是在兩個 thread 間的同步上，當 thread 進行至某個點時，去通知 thread B 可以繼續往下執行。 另一個 mutex 與 binary semaphore 的差異在於，mutex 的使用可能產生副作用: priority inversion。假設有優先權從高至低的三個 thread T1、T2、T3，其中 T1 和 T3 需要一個由 mutex 保護的資源: 最開始沒有其他 thread 在運行，因此 T3 先執行並拿到 mutex T2 一段時間後產生，因為 T2 priority 較高，因此 interrupt 時排程器讓 T2 先執行 T1 一段時間後產生，因為 T1 priority 最高，理論上該由 T1 先執行，但是因為資源被 T3 佔用了，因此 CPU 只能先讓給 T2 執行，這就是 priority inversion 為此， mutex 實作中需要採用一些機制來防止 priority inversion。但是，因為 semaphore 是為了不同 thread 間同步而存在，實作上就不必為此煩惱。 Mutex 與 Semaphore 最大的差異 最大的差異在於 Mutex 只能由上鎖的 thread 解鎖，而 Semaphore 沒有這個限制，可以由原本的 thread 或是另外一個 thread 解開。另外，Mutex 只能讓一個 thread 進入 critical section，Semaphore 的話則可以設定要讓幾個 thread 進入。這讓實際上使用 Mutex 跟 Semaphore 場景有很大的差別。 Mutex 的兩個特性：一個是只能有持鎖人解鎖、一個是在釋放鎖之前不能退出的特性，讓 Mutex 叫常使用在 critical section 只能有一個 thread 進入，而且要避免 priority inversion 的時候；Semaphore 也能透過 binary semaphore 做到類似的事情，卻沒有辦法避免 priority inversion 出現。 而 Semaphore 更常是用在同步兩個 thread 或功能上面，因為 Semaphore 實際上使用的是 signal 的 up 與 down，讓 Semaphore 可以變成是一種 notification 的作用，例如 A thread 執行到某個地方時 B thread 才能繼續下去，就可以使用 Semaphore 來達成這樣的作用。 在 Linux kernel 中，一開始是只有 semaphore 這個 structure，直到 2.6.16 版當中才把 mutex 從 semaphore 中分離出來 (這點可以從 LDD3e* 看出來)。雖然 Mutex 與 Semaphore 兩者都是休眠鎖，但是 Linux kernel 在實作 Mutex 的時候，有用到一些加速的技巧，將上鎖分為3個步驟： Fast path: 嘗試使用 atomic operation 直接減少 counter 數值來獲得鎖。 Mid path: 第一步失敗的話，嘗試使用特化的 MCS spinlock 等待然後取鎖。 當持鎖的 thread 還在運行，而且沒有存在更高 priority 的 task 時，我們可以大膽假設，持鎖 thread 很快就會把 thread 釋放出來 (看看 code 就知道了)，因此會使用一個特化的 MCS spinlock 等待鎖被釋放。特化的 MCS spinlock 可以在被 reschedule 的時候退出 MCS spinlock queue。當走到這步時，就會到 Slow path。 Slow path: 鎖沒有辦法取得，只好把自己休眠了。走到這一步，mutex 才會將自己加入 wait-queue 然後休眠，等待有人 unlock 後才會被喚醒。 参考链接 https://hackmd.io/@sysprog/linux-sync?type=view ","tags":[],"title":"Mutex 與 Semaphore 区别","link":"https://toomi.pages.dev/post/mutex-yu-semaphore-qu-bie/","stats":{"text":"6 min read","time":340000,"words":1499,"minutes":6},"dateFormat":"2022-12-29"},{"content":"自旋锁（spin lock） 自旋锁是Linux中使用非常频繁的锁，原理简单。 内核当发生访问资源冲突的时候，可以有两种锁的解决方案选择： 一个是原地等待 一个是挂起当前进程，调度其他进程执行（睡眠） Spinlock 是内核中提供的一种比较常见的锁机制，自旋锁是“原地等待”的方式解决资源冲突的，即，一个线程获取了一个自旋锁后，另外一个线程期望获取该自旋锁，获取不到，只能够原地“打转”（忙等待）。由于自旋锁的这个忙等待的特性，注定了它使用场景上的限制 —— 自旋锁不应该被长时间的持有（消耗 CPU 资源），一般应用在中断上下文。 原理 下面以去银行办业务为例来讲解。 银行的办事大厅一般会有几个窗口同步进行。今天很不巧，只有一个窗口提供服务。现在的银行服务都是采用取号排队，叫号服务的方式。 当你去银行办理业务的时候，首先会去取号机器领取小票，上面写着你排多少号。然后你就可以排队等待了。一般还会有个显示屏，上面会显示一个数字（例如：&quot;请xxx号到1号窗口办理&quot;），代表当前可以被服务顾客的排队号码。每办理完一个顾客的业务，显示屏上面的数字都会增加1。等待的顾客都会对比自己手上写的编号和显示屏上面是否一致，如果一致的话，就可以去前台办理业务了。 现在早上刚开业，顾客A是今天的第一个顾客，去取号机器领取0号（next计数）小票，然后看到显示屏上显示0（owner计数），顾客A就知道现在轮到自己办理业务了。 顾客A到前台办理业务（持有锁）中，顾客B来了。同样，顾客B去取号机器拿到1号（next计数）小票。然后乖乖的坐在旁边等候。顾客A依然在办理业务中，此时顾客C也来了。顾客C去取号机器拿到2号（next计数）小票。顾客C也乖乖的找个座位继续等待。 终于，顾客A的业务办完了（释放锁）。然后，显示屏上面显示1（owner计数）。顾客B和C都对比显示屏上面的数字和自己手中小票的数字是否相等。顾客B终于可以办理业务了（持有锁）。顾客C依然等待中。 顾客B的业务办完了（释放锁）。然后，显示屏上面显示2（owner计数）。顾客C终于开始办理业务（持有锁）。顾客C的业务办完了（释放锁）。 3个顾客都办完了业务离开了。只留下一个银行柜台服务员。 最终，显示屏上面显示3（owner计数）。取号机器的下一个排队号也是3号（next计数）。无人办理业务（锁是释放状态）。 Linux中针对每一个spin lock会有两个计数。分别是next和owner（初始值为0）。进程A申请锁时，会判断next和owner的值是否相等。如果相等就代表锁可以申请成功，否则原地自旋。直到owner和next的值相等才会退出自旋。假设进程A申请锁成功，然后会next加1。此时owner值为0，next值为1。进程B也申请锁，保存next得值到局部变量temp（temp = 1）中。由于next和owner值不相等，因此原地自旋读取owner的值，判断owner和temp是否相等，直到相等退出自旋状态。当然next的值还是加1，变成2。进程A释放锁，此时会将owner的值加1，那么此时B进程的owner和temp的值都是1，因此B进程获得锁。当B进程释放锁后，同样会将owner的值加1。最后owner和next都等于2，代表没有进程持有锁。next就是一个记录申请锁的次数，而owner是持有锁进程的计数值。 信号量（semaphore） 信号量又称为信号灯，它是用来协调不同进程间的数据对象的，而最主要的应用是共享内存方式的进程间通信。本质上，信号量是一个计数器，它用来记录对某个资源（如共享内存）的存取状况。它负责协调各个进程，以保证他们能够正确、合理的使用公共资源。它和spin lock最大的不同之处就是：无法获取信号量的进程可以睡眠，因此会导致系统调度。一般说来，为了获得共享资源，进程需要执行下列操作： 测试控制该资源的信号量。 若此信号量的值为正，则允许进行使用该资源。进程将信号量减1。 若此信号量为0，则该资源目前不可用，进程进入睡眠状态，直至信号量值大于0，进程被唤醒，转入步骤1 当进程不再使用一个信号量控制的资源时，信号量值加1。如果此时有进程正在睡眠等待此信号量，则唤醒此进程。 信号量（semaphore）是进程间通信处理同步互斥的机制。是在多线程环境下使用的一种措施，它负责协调各个进程，以保证他们能够正确、合理的使用公共资源。 它和spin lock最大的不同之处就是：无法获取信号量的进程可以睡眠，因此会导致系统调度。 原理 信号量一般可以用来标记可用资源的个数。举2个生活中的例子： 我们要坐火车从南京到新疆，这个'任务'特别的耗时，只能在车上等着车到站，但是我们没有必要一直睁着眼睛等着车到站，最好的情况就是我们上车就直接睡觉，醒来就到站，这样从人（用户）的角度来说，体验是最好的，对比于进程，程序在等待一个耗时的任务的时候，没有必须要占用CPU，可以暂停当前任务使其进入休眠状态，当等待的事件发生之后再由其他任务唤醒，这种场景采用信号量比较合适。 我们在等待电梯、等待洗手间，这种场景需要等待的事件并不是很多，如果我们还要找个地方睡一觉，然后等电梯到了或者洗手间可以用了再醒来，那很显然这也没有必要，我们只需要排好队，刷一刷抖音就可以了，对比于计算机程序，比如驱动在进入中断例程，在等待某个寄存器被置位，这种场景需要等待的时间很短暂，系统开销远小于进入休眠的开销，所以这种场景采用自旋锁比较合适。 读写锁（read-write lock） 不管是自旋锁还是信号量在同一时间只能有一个进程进入临界区。对于有些情况，我们是可以区分读写操作的。因此，我们希望对于读操作的进程可以并发进行。对于写操作只限于一个进程进入临界区。而这种同步机制就是读写锁。读写锁一般具有以下几种性质。 同一时间有且仅有一个写进程进入临界区。 在没有写进程进入临界区的时候，同时可以有多个读进程进入临界区。 读进程和写进程不可以同时进入临界区。 读写锁有两种，一种是信号量类型，另一种是spin lock类型。下面以spin lock类型讲解。 原理 下面我们用厕所模型来理解读写锁。 我发现公司一般都会有保洁阿姨打扫厕所。如果以男厕所为例的话，我觉得男士进入厕所就相当于读者进入临界区。因为可以有多个男士进厕所。而保洁阿姨进入男士厕所就相当于写者进入临界区。 假设A男士发现保洁阿姨不在打扫厕所，就进入厕所。随后B和C同时也进入厕所。 然后保洁阿姨准备打扫厕所，发现有男士在厕所里面，因此只能在门口等待。 ABC都离开了厕所。保洁阿姨迅速进入厕所打扫。然后D男士去上厕所，发现保洁阿姨在里面。灰溜溜的出来了在门口等着。现在体会到了写者（保洁阿姨）具有排他性，读者（男士）可以并发进入临界区了吧。 既然我们允许多个读者进入临界区，因此我们需要一个计数统计读者的个数。同时，由于写者永远只存在一个进入临界区，因此只需要一个bit标记是否有写进程进入临界区。所以，我们可以将两个计数合二为一。只需要1个unsigned int类型即可。最高位（bit31）代表是否有写者进入临界区，低31位（0~30bit）统计读者个数。 互斥体（mutex） 互斥体实现了“互相排斥”（mutual exclusion）同步的简单形式（所以名为互斥体(mutex)）。互斥体禁止多个线程同时进入受保护的代码“临界区”（critical section）。因此，在任意时刻，只有一个线程被允许进入这样的代码保护区。 任何线程在进入临界区之前，必须获取（acquire）与此区域相关联的互斥体的所有权。如果已有另一线程拥有了临界区的互斥体，其他线程就不能再进入其中。这些线程必须等待，直到当前的属主线程释放（release）该互斥体。 什么时候需要使用互斥体呢？ 互斥体用于保护共享的易变代码，也就是，全局或静态数据。这样的数据必须通过互斥体进行保护，以防止它们在多个线程同时访问时损坏。 前文提到的semaphore在初始化count计数的时候，可以分为计数信号量和互斥信号量（二值信号量）。mutex和初始化计数为1的二值信号量有很大的相似之处。他们都可以用做资源互斥。但是mutex却有一个特殊的地方：只有持锁者才能解锁。但是，二值信号量却可以在一个进程中获取信号量，在另一个进程中释放信号量。如果是应用在嵌入式应用的RTOS，针对mutex的实现还会考虑优先级反转问题。 原理 既然mutex是一种二值信号量，因此就不需要像semaphore那样需要一个count计数。由于mutex具有“持锁者才能解锁”的特点，所以我们需要一个变量owner记录持锁进程。释放锁的时候必须是同一个进程才能释放。当然也需要一个链表头，主要用来便利睡眠等待的进程。原理和semaphore及其相似，因此在代码上也有体现。 原文链接 https://zhuanlan.zhihu.com/p/197553129 ","tags":[],"title":"自旋锁、信号量、互斥体概述","link":"https://toomi.pages.dev/post/zi-xuan-suo-xin-hao-liang-hu-chi-ti-gai-shu/","stats":{"text":"11 min read","time":600000,"words":2896,"minutes":11},"dateFormat":"2022-12-29"},{"content":"之前從軟體的角度寫過 memory barrier 的介紹。《Memory Barriers: a Hardware View for Software Hackers》則是從硬體的角度了解硬體設計者的需求，以及 read/write memory barrier 如何運作。我只有讀完前五章，後面用我理解的方式摘要這篇文章。本文的圖示都是從該篇文章取出來的。 因为对内存访问顺序的重排可以获取更好的性能，如果某些场合下，程序的逻辑正确性需要内存访问顺序和program order一致，例如：同步原语，那么软件工程师可以使用memory barrier这样的工具阻止CPU对内存访问的优化。 Memory barriers are used to provide control over the order of memory accesses. This is necessary sometimes because optimizations performed by the compiler and hardware can cause memory to be accessed in a different order than intended by the developer. A memory barrier affects instructions that access memory in two ways: provides control over the order that memory access instructions are performed, and provides control over when memory access instructions will complete. Memory access instructions, such as loads and stores, typically take longer to execute than other instructions. Therefore, compilers use registers to hold frequently used values and processors use high speed caches to hold the most frequently used memory locations. Another common optimization is for compilers and processors to rearrange the order that instructions are executed so that the processor does not have to wait for memory accesses to complete. This can result in memory being accessed in a different order than specified in the source code. While this typically will not cause a problem in a single thread of execution, it can cause a problem if the location can also be accessed from another processor or device. Types of Memory Barriers As mentioned above, both compilers and processors can optimize the execution of instructions in a way that necessitates the use of a memory barrier. A memory barrier that affects both the compiler and the processor is a hardware memory barrier, and a memory barrier that only affects the compiler is a software memory barrier. In addition to hardware and software memory barriers, a memory barrier can be restricted to memory reads, memory writes, or both. A memory barrier that affects both reads and writes is a full memory barrier. There is also a class of memory barrier that is specific to multi-processor environments. The name of these memory barriers are prefixed with &quot;smp&quot;. On a multi-processor system, these barriers are hardware memory barriers and on uni-processor systems, they are software memory barriers. The barrier() macro is the only software memory barrier, and it is a full memory barrier. All other memory barriers in the Linux kernel are hardware barriers. A hardware memory barrier is an implied software barrier. Using Memory Barriers The two most common needs for memory barriers are to manage memory shared by more than one processor and IO control registers that are mapped to memory locations. In the case of shared memory, when there is only one CPU, hardware memory barriers are not needed. Because of this, memory barriers that are only needed to control shared memory between processors can be optimized for better performance on systems with only one processor. As mentioned above, the name of these memory barriers are prefixed with &quot;smp&quot;. 参考 https://medium.com/fcamels-notes/%E5%BE%9E%E7%A1%AC%E9%AB%94%E8%A7%80%E9%BB%9E%E4%BA%86%E8%A7%A3-memry-barrier-%E7%9A%84%E5%AF%A6%E4%BD%9C%E5%92%8C%E6%95%88%E6%9E%9C-416ff0a64fc1 https://bruceblinn.com/linuxinfo/MemoryBarriers.html ","tags":[],"title":"從硬體觀點了解 memory barrier 的實作和效果","link":"https://toomi.pages.dev/post/cong-ying-ti-guan-dian-liao-jie-memory-barrier-de-shi-zuo-he-xiao-guo/","stats":{"text":"4 min read","time":229000,"words":685,"minutes":4},"dateFormat":"2022-12-26"},{"content":"启动方式 用java命令启动Java程序，有两种方式： 通过指定入口类： java SomeClass 。 这里的SomeClass是main()方法所在的类。其一般格式为java [ options ] SomeClass[ arguments ]。 指定入口类所在jar包：java -jar SomeJar.jar。这里的SomeJar.jar是入口类所在的jar包。其一般格式为 java [ options ] -jar SomeJar.jar [ arguments ]。 在一般格式中： [ options ]：配置 Java 的虚拟机选项（或称 系统参数） [ arguments ]：配置 Java 程序参数（或称 运行参数） 例如： java -Dfile.encoding=UTF-8 -Dusername=Joe Test hi a b c d 启动参数的类型 在启动Java程序时，可以指定两类参数，分别叫做虚拟机选项（VM options）和程序参数（program arguments）。 虚拟机选项是指由JVM支持、用于设置虚拟机启动过程的参数。 程序参数是指用户自定义的参数，在代码中可以通过main()方法的String[] args获取。 虚拟机选项（VM options） 虚拟机选项分为3类，分别是： 标准参数（-）：所有的JVM实现都必须实现这些参数的功能，而且向后兼容。 非标准参数（-X）：默认JVM实现这些参数的功能，但是并不保证所有JVM实现都满足，且不保证向后兼容。 非Stable参数（-XX）：此类参数各个JVM实现会有所不同，将来可能会随时取消，需要慎重使用。 标准参数 标准参数是指以-开头的参数。常见的标准参数如： -client / -server: 指定虚拟机以client模式（Windows下的默认模式）还是server模式（Linux下的默认模式）启动。 -jar：指定以jar包的形式执行一个应用程序。要这样执行一个应用程序，必须让jar包的manifest文件中声明初始加载的Main-class，该类中必须有public static void main(String[] args)方法。 -verbose 或 -verbose:class：输出JVM载入类的相关信息，当JVM报告说找不到类或者类冲突时可此进行诊断。 -verbose:gc：输出每次GC的相关情况。 -verbose:jni：输出native方法调用的相关情况，一般用于诊断jni调用错误信息。 -X：输出非标准的参数列表及其描述。 -? 或 -help：输出java标准参数列表及其描述。 -version：输出java的版本信息，比如jdk版本、vendor、model。 -javaagent:jarpath[=options]：指定jvm启动时装入java语言设备代理。Jarpath文件中的mainfest文件必须有Agent-Class属性，代理类必须实现方法public static void premain(String agentArgs, Instrumentation inst)（和main方法类似）。当JVM初始化时，将按代理类的说明顺序调用premain方法。具体参见java.lang.instrument软件包的描述。 -enableassertions[:&quot;...&quot; | : ] 或 -ea[:&quot;...&quot; | : ]：用来设置JVM是否启动断言机制（从JDK 1.4开始支持），缺省时JVM关闭断言机制。 -disableassertions[:&quot;...&quot; | :&lt;class ; ] 或 -da[:&quot;...&quot; | : ]：用来设置JVM关闭断言处理，一般用于相同package内某些class不需要断言的场景。 -enablesystemassertions 或 -esa：激活系统类的断言。 -disablesystemassertions 或 -dsa：关闭系统类的断言。 类加载路径 JVM的类加载路径通过-classpath 或-cp 指定，多个路径之间用分号分隔，如： -classpath &quot;D:\\Program Files\\Java\\jdk1.8.0_231\\jre\\lib\\charsets.jar;D:\\Program Files\\Java\\jdk1.8.0_231\\jre\\lib\\deploy.jar&quot; 使用-classpath后，JVM 将不再使用CLASSPATH中的类加载路径。（如果-classpath和CLASSPATH都没有设置，则JVM使用当前路径(.)作为类加载路径。） JVM 搜索类的方式和顺序为： Bootstrap：Bootstrap中的路径是JVM自带的jar或zip文件，JVM首先搜索这些包文件，用System.getProperty(&quot;sun.boot.class.path&quot;)可得到其值。 Extension：Extension是位于JRE_HOME/lib/ext目录下的jar文件，JVM在搜索完Bootstrap后就搜索该目录下的jar文件，用System.getProperty(&quot;java.ext.dirs&quot;)可得到其值。 User：User搜索顺序为当前路径、CLASSPATH、-classpath。用System.getProperty(&quot;java.class.path&quot;)可得到其值。 系统属性参数 系统属性参数以-D=value形式定义，运行在此虚拟机之上的应用程序可用System.getProperty(&quot;propertyName&quot;)得到value的值。 多个系统属性参数之间用空格隔开。如果value中有空格，则需要用双引号将该值括起来，如-Dname=”space string”。 参数可以是 Java 默认的，此类参数由 JVM 虚拟机自动识别并生效，例如-Dfile.encoding=UTF-8用于指定文件编码格式； 也可以是用户自定义的，例如-Dusername=Joe，程序中可以读取该参数值，执行相关逻辑。 如-Dspring.profiles.active=dev可以在SpringBoot启动中指定系统变量，用于多环境(开发、测试、预发、线上)的区分。 关于标准参数更详细的说明可以参看这篇文章。 非标准参数 非标准参数又称为扩展参数，是指以-X开头的参数，可以通过java -X打印出所有的非标准参数。 常见的非标准参数如： -Xms：指定JVM堆的初始大小（memory of start?），如-Xms3550m。 -Xmx：指定JVM堆的最大值（memory of maximum?），如-Xmx3550m。 -Xmn：指定JVM堆中新生代的大小（memory of nursery），如-Xmn2g。 -Xss：设置单个线程栈的大小（stack size），一般默认为512k。等同于-XX:NewSize。 -Xprof：跟踪正运行的程序，并将跟踪数据在标准输出输出；适合于开发环境调试。 -Xloggc:：与-verbose:gc功能类似，只是将每次GC事件的相关情况记录到一个文件中。若与verbose命令同时出现在命令行中，则以-Xloggc为准。 -Xint：设置JVM以解释模式运行，所有的字节码将被直接执行，而不会编译成本地码。 更详细的说明可以参看这篇文章。 非Stable参数 非Stable参数是指以-XX开头的参数。 这类参数在JVM中可能是不健壮的，SUN也不推荐使用，后续可能会在没有通知的情况下就直接取消了。 但这些参数中有些的确很有用，比如我们经常会见到的-XX:PermSize、-XX:MaxPermSize。 这些参数可以大致分成三类： 行为参数（Behavioral Options）：用于改变jvm的一些基础行为 性能调优（Performance Tuning）：用于jvm的性能调优 调试参数（Debugging Options）：一般用于打开跟踪、打印、输出等jvm参数，用于显示jvm更加详细的信息 更详细的说明可以参看这篇文章。 程序参数（program arguments） 用户自定义的参数。在代码中通过main()方法的String[] args获取后，用户在代码中按约定的格式对其进行解析，得到所需的参数值。 最终，我们来看一个包含各类参数的java启动命令： java -jar -server -Xmx3550m -XX:+HeapDumpOnOutOfMemoryError -XX:+PrintGCDetails -XX:+PrintGCDateStamps -Dserver.connection-timeout=60000 -Dspring.profiles.active=online SomeJar.jar argument1 argument2 原文 https://www.cnblogs.com/haycheng/p/12781261.html ","tags":[],"title":"Java 启动参数","link":"https://toomi.pages.dev/post/java-qi-dong-can-shu/","stats":{"text":"7 min read","time":401000,"words":1727,"minutes":7},"dateFormat":"2022-12-16"},{"content":"javaagent 是什么 Javaagent是java命令的一个参数。参数 javaagent 可以用于指定一个 jar 包，并且对该 java 包有2个要求： 这个 jar 包的 MANIFEST.MF 文件必须指定 Premain-Class 项。 Premain-Class 指定的那个类必须实现 premain() 方法。 premain 方法，从字面上理解，就是运行在 main 函数之前的的类。当Java 虚拟机启动时，在执行 main 函数之前，JVM 会先运行-javaagent所指定 jar 包内 Premain-Class 这个类的 premain 方法 。 javaagent使用 JVM参数中添加 -javaagent:/path/to/MyPerf4J-ASM.jar ，并且确保这个参数在-jar参数之前。例如 java -javaagent:/path/to/MyPerf4J-ASM.jar -DMyPerf4JPropFile=/path/to/MyPerf4J.properties -jar yourApp.jar 参考 https://www.cnblogs.com/rickiyang/p/11368932.html ","tags":[],"title":"javaagent使用","link":"https://toomi.pages.dev/post/javaagent-shi-yong/","stats":{"text":"1 min read","time":46000,"words":183,"minutes":1},"dateFormat":"2022-12-16"},{"content":"Preface AbstractQueuedSynchronizer is the basic framework for implementing concurrency tools in the JDK, and a deeper understanding of it will help us to better use its features and related tools. We hope you will read this article carefully and gain something from it. In Java, access to shared resources by multiple threads is controlled by lock. We know that the lock function can be implemented by the synchronized keyword, which can implicitly acquire locks, that is, we do not need to care about the process of acquiring and releasing locks by using this keyword, but while it provides convenience, it also means that its flexibility is reduced. For example, there is a scenario where lock A is acquired first, then lock B is acquired, when lock B is acquired, lock A is released and lock C is acquired, when lock C is acquired, then lock B is released and lock D is acquired, and so on. After Java SE 5, a new Lock interface and a series of implementation classes were added to provide the same functionality as the synchronized keyword, which requires us to display the lock acquisition and release, in addition to providing synchronization features such as interruptible lock acquisition operations and timeout lock acquisition. Most of the Lock interface implementation classes provided in the JDK aggregate a subclass of the synchronizer AQS to achieve multi-threaded access control, so let’s take a look at the basic framework for building locks and other synchronization components - AQS ( AbstractQueuedSynchronizer) AQS basic data structure Synchronized Queues When a thread fails to obtain the synchronization status, the synchronizer encapsulates the current thread and the current waiting status into an internally defined node Node and then adds it to the queue. When the synchronization state is released, the first node in the synchronization queue is woken up and allowed to try to get the synchronization state again. The basic structure of the synchronization queue is as follows. Queue Node The synchronous queue uses the static internal class Node in the synchronizer to hold references to threads that get synchronized state, the wait state of the thread, the predecessor node and the successor node. The names and specific meanings of the attributes of the Node nodes in the synchronous queue are shown in the following table. attribute type and name description volatile int waitStatus The wait status of the current node in the queue volatile Node prev The predecessor node that is assigned when the node is added to the synchronization queue (using the tail-add method) volatile Node next the successor node volatile Thread thread The thread that gets the synchronization status Node nextWaiter waits for the successor node in the queue, or if the current node is shared, this field is a SHARED constant Each node thread has two lock modes, SHARED means that the thread waits for the lock in shared mode and EXCLUSIVE means that the thread waits for the lock in exclusive mode. Also the wait status waitStatus of each node can only take enumerated values from the following table. source https://www.javai.net/post/202204/java-aqs-principle-1/ ","tags":[],"title":"AbstractQueuedSynchronizer implementation  (principle - 1)","link":"https://toomi.pages.dev/post/abstractqueuedsynchronizer-implementation-principle-1/","stats":{"text":"4 min read","time":196000,"words":523,"minutes":4},"dateFormat":"2022-12-15"},{"content":"对分析工作熟练的人，想必都用过 Excel 中的 VLOOKUP 函数，参考我的另一篇博客：VLOOKUP 函数跨工作表跨文件使用方式 ，但是目前已经被官方取消【2019 年末宣布】，转而使用 XLOOKUP。此外，可能很少有人知道，在 Elasticsearch 中也存在这样一个类似的查询接口：terms lookup，可以跨索引查询数据，看起来很是方便，本文简单介绍一下。开发环境基于 Elasticsearch v5.6.8。 接口介绍 在 Elasticsearch 中，是存在父子文档这种设置的，即在父索引中嵌套一个子索引，例如在主帖中嵌套用户的信息。这样查询主帖时，不仅可以同时指定用户的筛选条件，而且返回数据时可以连同用户信息一起返回，在使用层面很方便。 但是，这种存储方式显然冗余了大量的用户信息，如果数据量级很大，浪费了大量的存储空间，不可取。随着业务的数据增长，这种设计方式肯定要被淘汰掉，转而选择把父子文档拆分，此时如果还想使用类似父子文档的查询特性，可以选择跨索引查询，即 terms lookup。 例如查询某个用户关注列表中所有用户发表的主帖，如果是拆开查询，需要两步：先查出关注列表，再查询发表的主帖，而使用 terms lookup 查询只需要一步即可。但是这种方式有诸多的限制，下面会举例说明，我想这也是为了性能考虑。 其实，terms lookup 是一个查询过滤器【filter，从 v0.90 开始引进】，只不过不需要用户指定 filter、filtered 等关键字，所以用户使用起来也无感知。 参考官方文档：query-dsl-terms-query 。 读者在继续往下阅读之前可以先去了解一下与 缓存 相关的知识点，例如：如何开启、如何关闭、删除缓存、自定义命名、什么场景应该使用、哪些查询不会被缓存。 演示示例 我在这里使用两个典型的场景进行演示，一个可以支持，另一个不能支持。假如有 2 个索引：用户 my-index-user、主帖 my-index-post，它们之间是通过用户的 id 来进行关联的【在用户索引中字段为 item_id，在主帖索引中字段为 user_item_id】，即用户可以任意发表主帖。 场景一 查询某个用户的关注列表中所有用户发表的主帖。【可以支持】 查询思路分两个步骤，一是利用 item_id 查询用户索引，返回关注列表 friends 中的所有 item_id；二是利用一步骤中返回的 itemt_id 列表，去匹配主帖的 user_item_id 字段，从而查询所有的主帖。 转换为 terms lookup 查询为： POST my-index-post/_search { &quot;query&quot;: { &quot;bool&quot;: { &quot;must&quot;: [ { &quot;terms&quot;: { &quot;user_item_id&quot;: { &quot;index&quot;: &quot;my-index-user&quot;, &quot;type&quot;: &quot;user&quot;, &quot;id&quot;: &quot;0f42d65be1f5287e1c9c26e3728814aa&quot;, &quot;path&quot;: &quot;friends&quot; } } } ] } } } 可以看到查询出来 2743 条主帖，这样的查询方式是不是很方便呢。 下面可以拆分步骤简单验证一下，先查询用户索引，把关注列表查出来： POST my-index-user/user/_search { &quot;query&quot;: { &quot;bool&quot;: { &quot;must&quot;: [ { &quot;terms&quot;: { &quot;id&quot;: [ &quot;0f42d65be1f5287e1c9c26e3728814aa&quot; ] } } ] } }, &quot;_source&quot;: [ &quot;item_id&quot;, &quot;friends&quot;, &quot;birth_year&quot; ] } 场景二 查询出生日期是 1994 年的所有用户发表的主帖。【不可以支持】 查询思路分两个步骤，一是利用 birth_year 查询用户索引，返回满足条件的所有 item_id；二是利用一步骤中返回的 itemt_id 列表，去匹配主帖的 user_item_id 字段，从而查询所有的主帖。 转换为 terms lookup 查询为： POST my-index-post/_search { &quot;query&quot;: { &quot;bool&quot;: { &quot;must&quot;: [ { &quot;terms&quot;: { &quot;user_item_id&quot;: { &quot;index&quot;: &quot;my-index-user&quot;, &quot;type&quot;: &quot;user&quot;, &quot;birth_year&quot;: 1994, &quot;path&quot;: &quot;item_id&quot; } } } ] } } } 可以看到，报错了：[terms] query does not support [birth_year] within lookup element，想象很美好，现实很残酷，其实 Elasticsearch 并不支持对用户索引使用非 id 字段条件，也就是指定内层的索引条件，只能是和 id 有关的，这也是一种限制。 引申说明 terms 个数限制，对于场景一来说，如果关注列表中的 item_id 过多，也会导致查询主帖的 terms 匹配失败，因为 terms 查询是有个数限制的。可以通过配置更改，设置 terms 最大个数：index.max_terms_count，默认最大个数为 65535，可以根据集群情况降低，例如设置为 10000，为了集群稳定，一般不需要设置那么大。 内层返回字段需要存储，对于场景一来说，如果关注列表 friends 字段没有存储【stored_fields 属性】，只是做了索引，也是无法支持的，会报错：[terms] query does not support [friends] within lookup element。 对于场景二来说，表现的就是 terms lookup 无法支持复杂的查询条件，只能是和 id 字段有关的，这样就降低了 Elasticsearch 的计算量。 原链接 https://www.playpi.org/2019060601.html ","tags":[],"title":"Elasticsearch 的 terms lookup 查询","link":"https://toomi.pages.dev/post/elasticsearch-de-terms-lookup-cha-xun/","stats":{"text":"6 min read","time":301000,"words":1313,"minutes":6},"dateFormat":"2022-12-14"},{"content":"索引的生成分为两个部分： 创建阶段： 添加文档阶段，通过IndexWriter调用addDocument方法生成正向索引文件； 文档添加后，通过flush或merge操作生成倒排索引文件。 搜索阶段： 用户通过查询语句向Lucene发送查询请求； 通过IndexSearch下的IndexReader读取索引库内容，获取文档索引； 得到搜索结果后，基于搜索算法对结果进行排序后返回。 索引创建及搜索流程如下图所示： Lucene索引构成 正向索引 ucene的基础层次结构由索引、段、文档、域、词五个部分组成。正向索引的生成即为基于Lucene的基础层次结构一级一级处理文档并分解域存储词的过程。 索引文件层级关系如图1所示： 索引（index）：Lucene索引库包含了搜索文本的所有内容，可以通过文件或文件流的方式存储在不同的数据库或文件目录下。 段（segment）：一个索引中包含多个段，段与段之间相互独立。由于Lucene进行关键词检索时需要加载索引段进行下一步搜索，如果索引段较多会增加较大的I/O开销，减慢检索速度，因此写入时会通过段合并策略对不同的段进行合并。 文档(document)：Lucene会将文档写入段中，一个段中包含多个文档。 域(field)：一篇文档会包含多种不同的字段，不同的字段保存在不同的域中。 词(tern)：Lucene会通过分词器将域中的字符串通过词法分析和语言处理后拆分成词，Lucene通过这些关键词进行全文检索。 倒排索引 Lucene全文索引的核心是基于倒排索引实现的快速索引机制。 倒排索引原理如图2所示，倒排索引简单来说就是基于分析器将文本内容进行分词后，记录每个词出现在哪篇文章中，从而通过用户输入的搜索词查询出包含该词的文章。 问题：上述倒排索引使用时每次都需要将索引词加载到内存中，当文章数量较多，篇幅较长时，索引词可能会占用大量的存储空间，加载到内存后内存损耗较大。 解决方案：从Lucene4开始，Lucene采用了FST来减少索引词带来的空间消耗。 FST(Finite StateTransducers)，中文名有限状态机转换器。其主要特点在于以下四点： 查找词的时间复杂度为O(len(str))； 通过将前缀和后缀分开存储的方式，减少了存放词所需的空间； 加载时仅将前缀放入内存索引，后缀词在磁盘中进行存放，减少了内存索引使用空间的损耗； FST结构在对PrefixQuery、FuzzyQuery、RegexpQuery等查询条件查询时，查询效率高。 具体存储方式如图3所示： 倒排索引相关文件包含.tip、.tim和.doc这三个文件，其中： tip：用于保存倒排索引Term的前缀，来快速定位.tim文件中属于这个Field的Term的位置，即上图中的aab、abd、bdc。 tim：保存了不同前缀对应的相应的Term及相应的倒排表信息，倒排表通过跳表实现快速查找，通过跳表能够跳过一些元素的方式对多条件查询交集、并集、差集之类的集合运算也提高了性能。 doc：包含了文档号及词频信息，根据倒排表中的内容返回该文件中保存的文本信息。 原文 https://segmentfault.com/a/1190000040371949 ","tags":[],"title":"Lucene基础工作流程","link":"https://toomi.pages.dev/post/lucene-ji-chu-gong-zuo-liu-cheng/","stats":{"text":"4 min read","time":197000,"words":945,"minutes":4},"dateFormat":"2022-12-14"},{"content":"什么是lock-free，简单的说就是不直接使用锁，减少锁在系统中占用的开销。相比于基于锁的算法而言，Lock-free 算法具有明显的特征：某个线程在执行数据访问时挂起不会阻碍其他的线程继续执行（某个线程持有锁之后挂起，导致其他的线程没有办法申请锁）。这意味着在任意时刻，多个 lock-free 线程可以同时访问同一数据而不产生数据竞争。 上面的定义保证了 lock-free 程序中的一组线程中至少有一个可以顺利的执行而不产生阻塞，从而确保整个程序的顺利执行。lock-free中常见的就是使用cas来代替加锁。 1. 非阻塞数据结构（Non-Blocking Data Structures） 在算法上实现非阻塞难度大，可以利用特定的数据结构实现，非阻塞数据结构可以氛围以下三类 1.1. Obstruction-Free 当其他所有线程都挂起（suspended）的时候，可以保证有一个线程可以完成对共享资源的操作 Obstruction-freedom is the weakest form of a non-blocking data structure. Here, we only require that a thread is guaranteed to proceed if all other threads are suspended. More precisely, a thread won't continue to starve if all other threads are suspended. This is different from using locks in that sense, that if the thread was waiting for a lock and a thread that holds the lock is suspended, the waiting thread would wait forever. 1.2. Lock-Free 在任意时间，至少有一个线程可以完成对共享资源的操作。 A data structure provides lock-freedom if, at any time, at least one thread can proceed. All other threads may be starving. The difference to obstruction-freedom is that there is at least one non-starving thread even if no threads are suspended. 1.3 Wait-Free 所有线程都能保证在有限的步骤或时间内，完成对共享资源的操作 A data structure is wait-free if it's lock-free and every thread is guaranteed to proceed after a finite number of steps, that is, threads will not starve for an “unreasonably large” number of steps. 1.4 Summary Let's summarize these definitions in graphical representation: The first part of the image shows obstruction-freedom as Thread 1 (top thread) can proceed (green arrow) as soon we suspend the other threads (at the bottom in yellow). The middle part shows lock-freedom. At least Thread 1 can progress while others may be starving (red arrow). The last part shows wait-freedom. Here, we guarantee that Thread 1 can continue (green arrow) after a certain period of starvation (red arrows). 参考链接 https://www.jianshu.com/p/d6eb7b58731f https://www.baeldung.com/lock-free-programming https://www.cnblogs.com/gaochundong/p/lock_free_programming.html ","tags":[],"title":"无锁编程lock-free","link":"https://toomi.pages.dev/post/wu-suo-bian-cheng-lock-free/","stats":{"text":"3 min read","time":174000,"words":618,"minutes":3},"dateFormat":"2022-12-13"},{"content":"插入了数据，有时还需要获取刚才生成的序列值另作他用，返回给前端也好，或者插入其他将来需要关联的表。 记得曾经有个面试题：假设当前表IDENTITY列最大值为N，在存储过程中，对这个表插入1行数据，获取到的IDENTITY列值有时小于或者大于N+1，可能是什么原因？ 获取IDENTITY列值有三种方式： IDENT_CURRENT( 'table_name' ) 返回为任何会话和任何作用域中的特定表最后生成的标识值。 @@IDENTITY 返回为当前会话的所有作用域中的任何表最后生成的标识值。 SCOPE_IDENTITY() 返回为当前会话和当前作用域中的任何表最后生成的标识值。 IDENT_CURRENT( 'table_name' ) 针对特定表，是全局的。@@IDENTITY和SCOPE_IDENTITY()针对所有表，区别在于作用域，也就是上下文： 如果当前INSERT语句上有函数，触发器等(不同作用域的)对象返回的IDENTITY值，那么@@IDENTITY会取所有表上的最后1个，而不是当前表上的； SCOPE_IDENTITY()会取当前作用域所有表上最后1个IDENTITY值，被调用的函数，触发器已经超出了作用域/上下文。所以在使用INSERT后，接着使用SCOPE_IDENTITY()获取IDENTITY列值，就不会有问题了： insert test values('z'); select SCOPE_IDENTITY() as curr_value 一个GO语句/批处理，也是一个上下文的分界点，但是SQL语句是顺序执行的，所以一个会话里，只要在INSERT之后用SCOPE_IDENTITY()来获取IDENTITY值是没问题的。 原链接 https://www.cnblogs.com/seusoftware/p/3804333.html ","tags":[],"title":"sql server 获取IDENTITY列值","link":"https://toomi.pages.dev/post/sql-server-huo-qu-identity-lie-zhi/","stats":{"text":"2 min read","time":89000,"words":410,"minutes":2},"dateFormat":"2022-12-09"},{"content":"加载器 对于虚拟机，只有两种不同的类加载器： 启动类加载器(Bootstrap ClassLoader)，这个类加载器使用C++语言实现，是虚拟机自身的一部分； 其它所有的类加载器，这些类加载器都由Java语言实现，独立于虚拟机外部，并且全部继承自java.lang.ClassLoader。 具体划分，如下： 启动类加载器(Bootstrap ClassLoader)：这个类加载器负责将放置在&lt;JAVA_HOME&gt;\\lib目录中的，或者被-Xbootclasspath参数所指定路径中的，并且是虚拟机能识别的(仅按照文件名识别，如rt.jar，名字不符合的类库即使放置在lib目录中也不会被加载)类库加载到虚拟机内存中。启动类加载器无法被Java程序直接使用，因为是C++语言实现的。 扩展类加载器(Extension ClassLoader)：这个类加载器由sun.misc.Launcher$ExtClassLoader实现，它负责加载&lt;JAVA_HOME&gt;\\lib\\ext目录中的，或者被java.ext.dirs系统变量所指定的路径中的所有类库，开发者可以直接使用扩展类加载器。 应用程序类加载器(Application ClassLoader)：这个类加载器由sum.misc.Launcher.$AppClassLoader来实现。由于这个类加载器是ClassLoader中的getSystemClassLoader()方法的返回值，所以一般也被称为系统类加载器。它负责加载用户类路径上所指定的类库（classPath），开发者可以直接使用这个类加载器，如果应用程序中没有自定义过自己的类加载器，一般情况下这个就是程序中默认的类加载器。 应用程序由这三种类加载器互相配合进行加载的，如果有必须，还可以加入自己定义的类加载器。这些类加载器之间的关系一般如下图 上图中展示的类加载器之间的层次关系，就称为类加载器的双亲委派模型(Parents Delegation Model)。双亲委派模型要求除了顶层的启动类加载器之外，其余的类加载器都应当有自己的父类加载器。这里的类加载器之间的父子关系一般不会以继承的关系来实现，而是使用组合关系来复用父加载器的代码。 双亲委派模型 如果一个类加载器收到了类加载的请求，它首先不会自己去尝试加载这个类，而是把这个请求委派给父类加载器去完成，每一个层次的类加载器都是如此，因此所有的加载请求最终都应该传送到顶层的启动类加载器中，只有当父加载器反馈自己无法完全这个加载请求时（查看该类是否是在自己的加载范围），子加载器才会尝试自己去加载。 双亲委派的好处： 安全性：使用双亲委派模型来组织类加载器之间的关系，有一个很明显的好处，就是Java类随着它的类加载器（说白了，就是它所在的目录）一起具备了一种带有优先级的层次关系，这对于保证Java程序的稳定运作很重要。例如，类java.lang.Object类存放在JDK\\jre\\lib下的rt.jar之中，因此无论是哪个类加载器要加载此类，最终都会委派给启动类加载器进行加载，这边保证了Object类在程序中的各种类加载器中都是同一个类，可以防止java核心类被篡改。 避免重复加载：当使用了双亲委派机制后，当一个类已经被上级类加载器加载后，下级类加载器就不会出现重复加载的情况。 使用场景 Class.getClassLoader().getResource(String path)中，path不能以'/'开头，path是指类加载器的加载范围，在资源加载的过程中，使用的逐级向上委托的形式加载的，'/'表示Boot ClassLoader，类加载器中的加载范围，因为这个类加载器是C++实现的，所以加载范围为null。如下所示：参考 public class Test { public static void main(String[] args) { System.out.println(Test.class.getClassLoader().getResource(&quot;&quot;)); System.out.println(Test.class.getClassLoader().getResource(&quot;/&quot;)); } } Class.getResource和ClassLoader.getResource Class.getResource(String path) path不以’/'开头时，默认是从此类所在的包下取资源； path 以’/'开头时，则是从ClassPath根下获取； Class.getResource和Class.getResourceAsStream在使用时，路径选择上是一样的。 Class.getClassLoader().getResource(String path) ClassLoader 代表的是用来加载其他类的类，任何一个class都包含对加载这个class的类加载器(ClassLoader)的引用，所以其加载资源一定是绝对路径，且不能以‘/’开头 path不能以‘/’开头； path是从ClassPath根下获取 Class.getClassLoader（）.getResource和Class.getClassLoader（）.getResourceAsStream在使用时，路径选择上也是一样的。 参考 https://juejin.cn/post/6844904159993397262 ","tags":[],"title":"Java类加载器及加载范围","link":"https://toomi.pages.dev/post/java-lei-jia-zai-qi-ji-jia-zai-fan-wei/","stats":{"text":"5 min read","time":271000,"words":1250,"minutes":5},"dateFormat":"2022-12-08"},{"content":" 2023-08-18 重新修改，对 ClassLoader 重新理解 文件系统路径 与 类路径(classpath) 此处参考 java编译器编译.java文件和java虚拟机执行.class文件时的路径和写法不一样。在没有设置任何classpath环境变量的情况下，javac可以编译全路径的.java文件。例如： javac d:\\myjava\\HelloWorld.java 编译后，在.java同路径目录下生成class文件。 默认java虚拟机要从classpath环境变量的路径中搜索class文件去执行，对于java虚拟机来说，这不是类文件，而是类。它只有类路径，而没有文件系统路径。而classpath环境变量正是为java虚拟机提供搜索类路径的环境。注意，虚拟机不会递归搜索classpath定义的路径。 也就是说，上面的java文件可以正确编译，但却不能执行。但如果将classpath设置为&quot;.;d:\\myjava\\&quot;，则java虚拟机将先从当前路径搜索，再从d:\\myjava下搜索class文件。 于是上面的HelloWorld.java编译后，可以直接执行: java HelloWorld 小结 对于 ClassLoader来说，类路径，而没有文件系统路径。类路径都是基于 classpath ，没有相对路径和绝对路径之分，所以不需要在资源路径前加上斜杠 / ，如使用的是绝对路径（例如 /myresource.txt），那么系统会尝试从文件系统的绝对路径中查找资源，而不是从类路径中查找，这可能导致找不到资源文件。例如 InputStream inputStream = getClass().getClassLoader().getResourceAsStream(&quot;myresource.txt&quot;); 但是，如果使用 Class 类 的 getResourceAsStream() ,是有相对路径和绝对路径，查看源代码，会发现其也是使用 ClassLoader 进行资源加载，但是会对 传入的资源名字进行处理，如果资源参数没有以 / 开头，就在前面拼接对应class的package 名字，然后再交给 ClassLoader 进行资源加载，给人产生一种相对路径的感觉 private String resolveName(String name) { if (!name.startsWith(&quot;/&quot;)) { Class&lt;?&gt; c = this; while (c.isArray()) { c = c.getComponentType(); } String baseName = c.getPackageName(); // 获取package名字 if (baseName != null &amp;&amp; !baseName.isEmpty()) { name = baseName.replace('.', '/') + &quot;/&quot; + name; // 拼接package名字，在返回 } } else { name = name.substring(1); } return name; } 例子 今天在Java程序中读取resources资源下的文件，由于对Java结构了解不透彻，遇到很多坑。正常在Java工程中读取某路径下的文件时，可以采用绝对路径和相对路径，绝对路径没什么好说的，相对路径，即相对于当前类的路径。在本地工程和服务器中读取文件的方式有所不同，以下图配置文件为例： 1、本地读取资源文件 Java类中需要读取properties中的配置文件，可以采用文件（File）方式进行读取： File file = new File(&quot;src/main/resources/properties/test.properties&quot;); InputStream in = new FileInputStream(file); 注意：当在IDEA中运行（不部署在服务器上），可以读取到该文件； 原因：JavaWeb项目部署服务器中，会将项目打包成Jar包或者war包，此时就不会存在 src/main/resources 目录，JVM会在编译项目时，主动将 java文件编译成 class文件 和 resources 下的静态文件放在 target/classes目录下； 理解：Java文件只有编译成 class文件才会被JVM执行，本地执行时会，当前项目即为Java进程的工作空间，虽然class文件在target/classes目录下，但是target/classes不是class文件运行的目录，只是存放的目录，运行目录还是在IDEA的模块下，所以运行时会找到 src/main/resources 资源文件！ 2、服务器（Tomcat）读取资源文件 当工程部署到Tomcat中时，按照上边方式，则会抛出异常：FileNotFoundException。原因：Java工程打包部署到Tomcat中时，properties的路径变到顶层（classes下），这是由Maven工程结构决定的。由Maven构建的web工程，主代码放在src/main/java路径下，资源放在src/main/resources路径下，当构建jar包 或 war包时，JVM虚拟机会自动编译java文件为class文件存放在 target/classes目录下，resource资源下的文件会原封不动的拷贝一份到 target/classes 目录下： 此时读取资源文件时，采用流（Stream）的方式读取，并通过JDK中Properties类加载，可以方便的获取到配置文件中的信息： InputStream in = this.getClass().getResourceAsStream(&quot;/properties/test.properties&quot;); Properties properties = new Properties(); properties.load(in); properties.getProperty(&quot;name&quot;); 3、读取方式比较 在Java中获取资源的时候，经常用到getResource和getResourceAsStream 3.1、Class.getResource(String path) path不以'/'开头时，默认是从此类所在的包下取资源； path以'/'开头时，则是从项目的ClassPath根下获取资源。在这里'/'表示ClassPath的根目录。 JDK设置这样的规则，是很好理解的，path不以'/'开头时，我们就能获取与当前类所在的路径相同的资源文件，而以'/'开头时可以获取ClassPath根下任意路径的资源。 如下所示的例子： public class Test { public static void main(String[] args) { System.out.println(Test.class.getResource(&quot;&quot;)); System.out.println(Test.class.getResource(&quot;/&quot;)); } } 运行结果为： file:/D:/work_space/java/bin/net/swiftlet/ file:/D:/work_space/java/bin/ 3.2、Class.getClassLoader().getResource(String path) path不能以'/'开头，path是指类加载器的加载范围，在资源加载的过程中，使用的逐级向上委托的形式加载的，'/'表示Boot ClassLoader，类加载器中的加载范围，因为这个类加载器是C++实现的，所以加载范围为null。如下所示： public class Test { public static void main(String[] args) { System.out.println(Test.class.getClassLoader().getResource(&quot;&quot;)); System.out.println(Test.class.getClassLoader().getResource(&quot;/&quot;)); } } 运行结果为： file:/D:/work_space/java/bin/ null 从上面可以看出： class.getResource(&quot;/&quot;) == class.getClassLoader().getResource(&quot;&quot;) 其实，Class.getResource和ClassLoader.getResource本质上是一样的，都是使用ClassLoader.getResource加载资源的。下面请看一下jdk的Class源码: public java.net.URL getResource(String name) { name = resolveName(name); ClassLoader cl = getClassLoader0(); if (cl==null) { // A system class. return ClassLoader.getSystemResource(name); } return cl.getResource(name); } 从上面就可以看才出来：Class.getResource和ClassLoader.getResource本质上是一样的。至于为什么Class.getResource(String path)中path可以'/'开头，是因为在name = resolveName(name);进行了处理： private String resolveName(String name) { if (name == null) { return name; } if (!name.startsWith(&quot;/&quot;)) { Class c = this; while (c.isArray()) { c = c.getComponentType(); } String baseName = c.getName(); int index = baseName.lastIndexOf('.'); if (index != -1) { name = baseName.substring(0, index).replace('.', '/') +&quot;/&quot;+name; } } else {//如果是以&quot;/&quot;开头，则去掉 name = name.substring(1); } return name; } ","tags":[],"title":"Java项目读取resources资源文件路径","link":"https://toomi.pages.dev/post/java-xiang-mu-du-qu-resources-zi-yuan-wen-jian-lu-jing/","stats":{"text":"7 min read","time":395000,"words":1630,"minutes":7},"dateFormat":"2022-12-08"},{"content":"Java 获取指定范围内的经纬度, 用的是正方形范围，而不是圆形 import javax.validation.constraints.NotNull; /** * 经纬度信息 */ public class GeoService { /** * 当前定位的纬度 */ Double latitude; // 当前定位经度 Double longitude; /** * 每米的经度 */ private final double longitudeMi = 0.00001141; /** * 每米的纬度 */ private final double latitudeMi = 0.00000899; private static final double EARTH_RADIUS = 6378137;//赤道半径 private static double rad(double d) { return d * Math.PI / 180.0; } /** * 获取当前定位与另一个坐标的距离 * * @param latitude * @param longitude * @return */ public double getDistance(double latitude, double longitude) { if (this.latitude &lt;=0 ){ return 0; } double radLat1 = rad(this.latitude); double radLat2 = rad(latitude); double a = radLat1 - radLat2; double b = rad(this.longitude) - rad(longitude); double s = 2 * Math.asin(Math.sqrt(Math.pow(Math.sin(a / 2), 2) + Math.cos(radLat1) * Math.cos(radLat2) * Math.pow(Math.sin(b / 2), 2))); s = s * EARTH_RADIUS; return s;//单位 米 } public GeoService(@NotNull double latitude, @NotNull double longitude) { this.latitude = latitude; this.longitude = longitude; } /** * 获取指定范围的经度左偏移量 * * @param distance * @return */ public double getLongitudeLeft(int distance) { return longitude - this.longitudeMi * distance; } /** * 获取指定范围的经度右偏移量 * * @param distance * @return */ public double getLongitudeRight(int distance) { return longitude + this.longitudeMi * distance; } /** * 获取指定范围的纬度左偏移量 * * @param distance * @return */ public double getLatitudeLeft(int distance) { return latitude - this.latitudeMi * distance; } public double getLatitudeRight(int distance) { return latitude + this.latitudeMi * distance; } } javascript 腾讯地图转百度地图坐标 /** * 坐标转换，百度地图坐标转换成腾讯地图坐标 * lng 腾讯经度（pointy） * lat 腾讯纬度（pointx） * 经度&gt;纬度 */ function bMapToQQMap(lng, lat) { if (lng == null || lng == '' || lat == null || lat == '') return [lng, lat]; var x_pi = 3.14159265358979324; var x = parseFloat(lng) - 0.0065; var y = parseFloat(lat) - 0.006; var z = Math.sqrt(x * x + y * y) - 0.00002 * Math.sin(y * x_pi); var theta = Math.atan2(y, x) - 0.000003 * Math.cos(x * x_pi); var lng = (z * Math.cos(theta)).toFixed(7); var lat = (z * Math.sin(theta)).toFixed(7); return [lng, lat]; } /** * 坐标转换，腾讯地图转换成百度地图坐标 * lng 腾讯经度（pointy） * lat 腾讯纬度（pointx） * 经度&gt;纬度 */ function qqMapToBMap(lng, lat) { if (lng == null || lng == '' || lat == null || lat == '') return [lng, lat]; var x_pi = 3.14159265358979324; var x = parseFloat(lng); var y = parseFloat(lat); var z = Math.sqrt(x * x + y * y) + 0.00002 * Math.sin(y * x_pi); var theta = Math.atan2(y, x) + 0.000003 * Math.cos(x * x_pi); var lng = (z * Math.cos(theta) + 0.0065).toFixed(5); var lat = (z * Math.sin(theta) + 0.006).toFixed(5); return [lng, lat]; } 参考 https://www.cnblogs.com/zyulike/p/11812534.html ","tags":[],"title":"地理位置计算","link":"https://toomi.pages.dev/post/di-li-wei-zhi-ji-suan/","stats":{"text":"3 min read","time":166000,"words":532,"minutes":3},"dateFormat":"2022-12-07"},{"content":"字符串在 Java 的 String 类内部由一个包含该字符串中所有字符的 char[] 来表示，其中的每个字符 char 又是由 2 个字节组成，因为 Java 内部使用 UTF-16。举例来说，如果一个字符串含有英文字符，那么这些英文字符的前 8 比特都将为 0，因为一个ASCII字符都能被单个字节来表示。 当然有许多字符需要 16 比特，但从统计角度来说只需 8 比特的情况占大多数，例如：LATIN-1 ，因此这能成为一种改善内存占用及性能的一个机会。更重要的是：由于 JVM 存储字符串的方式导致 JVM 堆空间通常很大一部分都被字符串所占据。 大多数情况下，字符串实例常占用比它实际需要的内存多一倍的空间。 在此篇文章中，我们将讨论 JDK 6 引入的可选的 Compressed String 功能及 JDK 9 最新引入的 Compact String ，它们两者的设计目的都是优化字符串在 JVM 中的内存占用。 Compressed String - Java 6 JDK 6 update 21 版本中引入了一个新的虚拟机参数选项：-XX:+UseCompressedStrings 当此选项启用时，字符串将以 byte[] 的形式存储，代替原来的 char[]，因此可以节省一些内存。然而，此功能最终在 JDK 7 中被移除，主要原因在于它将带来一些无法预料的性能问题。 Compact String - Java 9 Java 9 重新采纳字符串压缩这一概念。这意味着无论何时我们创建一个所有字符都能用一个字节的 LATIN-1 编码来描述的字符串，都将在内部使用字节数组的形式存储，且每个字符都只占用一个字节。另一方面，如果字符串中任一字符需要多于 8 比特位来表示时，该字符串的所有字符都统统使用两个字节的 UTF-16 编码来描述。因此基本上能如果可能，都将使用单字节来表示一个字符。 现在的问题是：所有的字符串操作如何执行？ 怎样才能区分字符串是由 LATIN-1 还是 UTF-16 来编码？为了处理这些问题，字符串的内部实现进行了一些调整。引入了一个 final 修饰的成员变量 coder, 由它来保存当前字符串的编码信息。 Java 9 中字符串的实现 之前，字符串都是以字符数组来存储： private final char[] value; 现在，它将使用字节数组来存储了： private final byte[] value; private final byte coder; coder 可以被赋值为如下的常量： static final byte LATIN1 = 0; static final byte UTF16 = 1; 现在，大多数的字符串操作都将检查 coder 变量，从而采取特定的实现： public int indexOf(int ch, int fromIndex) { return isLatin1() ? StringLatin1.indexOf(value, ch, fromIndex) : StringUTF16.indexOf(value, ch, fromIndex); } private boolean isLatin1() { return COMPACT_STRINGS &amp;&amp; coder == LATIN1; } 改变原因 主要是为了节约String占用的内存。 众所周知，在大多数Java程序的堆里，String占用的空间最大，并且绝大多数String只有Latin-1字符，这些Latin-1字符只需要1个字节就够了。JDK9之前，JVM因为String使用char数组存储，每个char占2个字节，所以即使字符串只需要1字节/字符，它也要按照2字节/字符进行分配，浪费了一半的内存空间。 JDK9是怎么解决这个问题的呢？一个字符串出来的时候判断，它是不是只有Latin-1字符，如果是，就按照1字节/字符的规格进行分配内存，如果不是，就按照2字节/字符的规格进行分配（UTF-16编码），提高了内存使用率。 举个类似的例子说就是，工厂生产的有多种型号的零件，一批零件必须装入同一种型号的包装发货，零件有大号和小号两种，而且绝大多数都是小号，之前包装的时候不管一批零件是大是小全部放到大号的包装里，造成了空间浪费，现在如果一批里面有大号就全装大号，如果没有大号就全装小号，提升了空间利用率。 这种做法带来的好处是显而易见的： 原本一个仓库装不下的零件，现在可以装下了（用更少的内存跑更大的应用） 原本仓库一天往外运一次，现在可以一天半甚至两天运一次（减少GC次数） 为什么用UTF-16而不用UTF-8 这就要从这两个字符集的设计说起了。 UTF-8实际上是对空间利用效率最高的编码集，它是不定长的，可以最大限度利用内存和网络。它是小包装装得下就用小包装，小包装装就看看能不能用大包装装，大包装都装不下就用超大包装。但是这种编码集只适用于传输和存储，并不适合拿来做String的底层实现。这是为什么呢？ 因为String有随机访问的方法，所谓随机访问，就是charAt、subString这种方法，随便指定一个数字，String要能给出结果。如果字符串中的每个字符占用的内存是不定长的，那么进行随机访问的时候，就需要从头开始数每个字符的长度，才能找到你想要的字符。试想，如果大小包装混装，你想拿到第N个零件，你必须一个一个数包装盒，数到N，才能找到你要的零件。而如果包装是一样的大小，你就可以通过简单的计算知道你要找的零件距离你有多少远，直接过去拿就行了。 但是又有人会问了，UTF-16也是变长的啊，一个字符可能在UTF-16里面占用4个字节咧。是的，是的，UTF-16是变长的，但这是在现实世界里是这样。在java的世界里，一个字符（char）就是2个字节，从\\u0000到\\uFFFF，占4个字节的字符，在java里是用两个char来存储的，而String的各种操作，都是以java的字符（char）为单位的，charAt是取得第几个char，subString取的也是第几个到第几个char组成的子串，甚至length返回的都是char的个数，从来没有哪个方法可以让你“通过下标取出字符串中第几个'现实意义'中的字符”，所以UTF-16在java的世界里，就可以视为一个定长的编码。还是工厂的例子：如果哪天出现了一个超大的零件咋办？简单，我这就只有大号包装，一个大号的包装都装不下的时候怎么办呢，把超大号的零件切成两份就行了，用两个大号包装去装，出厂也视为两个零件。关于这种超大零件，可以试着跑以下下面的代码体会下： public class StringTest { public static void main(String[] args) { System.out.println(&quot;🀎&quot;.length()); System.out.println(&quot;🀎&quot;.charAt(0)); System.out.println(&quot;🀎&quot;.charAt(1)); } } 原链接 https://www.zhihu.com/question/447224628 https://reionchan.github.io/2017/09/25/java-9-compact-string/#compact-string---java-9 ","tags":[],"title":"Java 9 新特性 - Compact Strings","link":"https://toomi.pages.dev/post/java-9-xin-te-xing-compact-strings/","stats":{"text":"7 min read","time":410000,"words":1863,"minutes":7},"dateFormat":"2022-12-02"},{"content":"overview 原文 https://www.javamadesoeasy.com/2015/02/linkedhashmap-custom-implementation.html ","tags":[],"title":"LinkedHashMap custom implementation in java","link":"https://toomi.pages.dev/post/linkedhashmap-custom-implementation-in-java/","stats":{"text":"1 min read","time":4000,"words":13,"minutes":1},"dateFormat":"2022-11-28"},{"content":"I will be explaining how we will put and get key-value pair in HashMap by overriding equals method：helps in checking equality of entry objects. hashCode method： helps in finding bucket’s index on which data will be stored. We will maintain bucket (ArrayList) which will store Entry (LinkedList). Entry&lt;K,V&gt; We store key-value pair by using Entry&lt;K,V&gt;，Entry contains K key V value Entry&lt;K,V&gt; next (i.e. next entry on that location of bucket). static class Entry&lt;K, V&gt; { K key; V value; Entry&lt;K,V&gt; next; public Entry(K key, V value, Entry&lt;K,V&gt; next){ this.key = key; this.value = value; this.next = next; } } Putting 5 key-value pairs I will explain you the whole concept of HashMap by putting 5 key-value pairs in HashMap. Initially, we have bucket of capacity=4. (all indexes of bucket i.e. 0,1,2,3 are pointing to null) 第一步 Let’s put first key-value pair in HashMap，Key=21, value=12，newEntry Object will be formed like this We will calculate hash by using our hash(K key) method - in this case it returns key%capacity = 21%4= 1. So, 1 will be the index of bucket on which newEntry object will be stored. We will go to 1st index as it is pointing to null we will put our newEntry object there. At completion of this step, our HashMap will look like this: 第二步 Let’s put second key-value pair in HashMap： Key=25, value=121 ， newEntry Object will be formed like this： We will calculate hash by using our hash(K key) method - in this case it returns key%capacity= 25%4= 1 So, 1 will be the index of bucket on which newEntry object will be stored. We will go to 1st index, it contains entry with key=21, we will compare two keys(i.e. compare 21 with 25 by using equals method), as two keys are different we check whether entry with key=21’s next is null or not, if next is null we will put our newEntry object on next. At completion of this step our HashMap will look like this 第三步 Let’s put third key-value pair in HashMap：Key=30, value=151，newEntry Object will be formed like this We will calculate hash by using our hash(K key) method - in this case it returns key/capacity= 30%4= 2. So, 2 will be the index of bucket on which newEntry object will be stored. We will go to 2nd index as it is pointing to null we will put our newEntry object there. At completion of this step, our HashMap will look like this： 第四步 Let’s put fourth key-value pair in HashMap： Key=33, value=15，Entry Object will be formed like this We will calculate hash by using our hash(K key) method - in this case it returns key/capacity= 33%4= 1, So, 1 will be the index of bucket on which newEntry object will be stored. We will go to 1st index t contains entry with key=21, we will compare two keys (i.e. compare 21 with 33 by using equals method, as two keys are different, proceed to next of entry with key=21 (proceed only if next is not null). now, next contains entry with key=25, we will compare two keys (i.e. compare 25 with 33 by using equals method, as two keys are different, now next of entry with key=25 is pointing to null so we won’t proceed further, we will put our newEntry object on next. At completion of this step our HashMap will look like this 完整代码 package com.ankit; /** * @author AnkitMittal, JavaMadeSoEasy.com * Copyright (c), AnkitMittal . All Contents are copyrighted and must not be * reproduced in any form. * This class provides custom implementation of HashMap(without using java api's)- * which allows us to store data in key-value pair form. * insertion order of key-value pairs is not maintained. * @param &lt;K&gt; * @param &lt;V&gt; */ class HashMapCustom&lt;K, V&gt; { private Entry&lt;K,V&gt;[] table; //Array of Entry. private int capacity= 4; //Initial capacity of HashMap static class Entry&lt;K, V&gt; { K key; V value; Entry&lt;K,V&gt; next; public Entry(K key, V value, Entry&lt;K,V&gt; next){ this.key = key; this.value = value; this.next = next; } } @SuppressWarnings(&quot;unchecked&quot;) public HashMapCustom(){ table = new Entry[capacity]; } /** * Method allows you put key-value pair in HashMapCustom. * If the map already contains a mapping for the key, the old value is replaced. * Note: method does not allows you to put null key though it allows null values. * Implementation allows you to put custom objects as a key as well. * Key Features: implementation provides you with following features:- * &gt;provide complete functionality how to override equals method. * &gt;provide complete functionality how to override hashCode method. * @param newKey * @param data */ public void put(K newKey, V data){ if(newKey==null) return; //does not allow to store null. //calculate hash of key. int hash=hash(newKey); //create new entry. Entry&lt;K,V&gt; newEntry = new Entry&lt;K,V&gt;(newKey, data, null); //if table location does not contain any entry, store entry there. if(table[hash] == null){ table[hash] = newEntry; }else{ Entry&lt;K,V&gt; previous = null; Entry&lt;K,V&gt; current = table[hash]; while(current != null){ //we have reached last entry of bucket. if(current.key.equals(newKey)){ if(previous==null){ //node has to be insert on first of bucket. newEntry.next=current.next; table[hash]=newEntry; return; } else{ newEntry.next=current.next; previous.next=newEntry; return; } } previous=current; current = current.next; } previous.next = newEntry; } } /** * Method returns value corresponding to key. * @param key */ public V get(K key){ int hash = hash(key); if(table[hash] == null){ return null; }else{ Entry&lt;K,V&gt; temp = table[hash]; while(temp!= null){ if(temp.key.equals(key)) return temp.value; temp = temp.next; //return value corresponding to key. } return null; //returns null if key is not found. } } /** * Method removes key-value pair from HashMapCustom. * @param key */ public boolean remove(K deleteKey){ int hash=hash(deleteKey); if(table[hash] == null){ return false; }else{ Entry&lt;K,V&gt; previous = null; Entry&lt;K,V&gt; current = table[hash]; while(current != null){ //we have reached last entry node of bucket. if(current.key.equals(deleteKey)){ if(previous==null){ //delete first entry node. table[hash]=table[hash].next; return true; } else{ previous.next=current.next; return true; } } previous=current; current = current.next; } return false; } } /** * Method displays all key-value pairs present in HashMapCustom., * insertion order is not guaranteed, for maintaining insertion order * refer LinkedHashMapCustom. * @param key */ public void display(){ for(int i=0;i&lt;capacity;i++){ if(table[i]!=null){ Entry&lt;K, V&gt; entry=table[i]; while(entry!=null){ System.out.print(&quot;{&quot;+entry.key+&quot;=&quot;+entry.value+&quot;}&quot; +&quot; &quot;); entry=entry.next; } } } } /** * Method implements hashing functionality, which helps in finding the appropriate * bucket location to store our data. * This is very important method, as performance of HashMapCustom is very much * dependent on this method's implementation. * @param key */ private int hash(K key){ return Math.abs(key.hashCode()) % capacity; } } /** * Main class- to test HashMap functionality. */ public class HashMapCustomApp { public static void main(String[] args) { HashMapCustom&lt;Integer, Integer&gt; hashMapCustom = new HashMapCustom&lt;Integer, Integer&gt;(); hashMapCustom.put(21, 12); hashMapCustom.put(25, 121); hashMapCustom.put(30, 151); hashMapCustom.put(33, 15); hashMapCustom.put(35, 89); System.out.println(&quot;value corresponding to key 21=&quot; + hashMapCustom.get(21)); System.out.println(&quot;value corresponding to key 51=&quot; + hashMapCustom.get(51)); System.out.print(&quot;Displaying : &quot;); hashMapCustom.display(); System.out.println(&quot;\\n\\nvalue corresponding to key 21 removed: &quot; + hashMapCustom.remove(21)); System.out.println(&quot;value corresponding to key 51 removed: &quot; + hashMapCustom.remove(51)); System.out.print(&quot;Displaying : &quot;); hashMapCustom.display(); } } /*Output value corresponding to key 21=12 value corresponding to key 51=null Displaying : {21=12} {25=121} {33=15} {30=151} {35=89} value corresponding to key 21 removed: true value corresponding to key 51 removed: false Displaying : {25=121} {33=15} {30=151} {35=89} */ 原文链接 https://www.javamadesoeasy.com/2015/02/hashmap-custom-implementation.html ","tags":[],"title":"HashMap custom implementation in java ","link":"https://toomi.pages.dev/post/hashmap-custom-implementation-in-java/","stats":{"text":"9 min read","time":531000,"words":1427,"minutes":9},"dateFormat":"2022-11-28"},{"content":"概要 有好长一段时间，兴趣爱好都落在了程序性能观测这个领域。之所以有兴趣，一来是解决一下工作中的问题，在这平淡的生活中寻找一丝快乐；二来也是想以史为鉴，以后在自己的代码里尽可能不要犯同样的错。 当局者迷，会体现在生活中的方方面面，正如写程序的人不知道程序的哪个函数会成为热点一样，我以前上来就是一张火焰图，也没有交代这个怎么得来的，现在准备把这一块补一下。 造场景 火焰图可用来分析程序中的热点函数，找出那些执行耗时比较长的函数(大多数情况下这些函数是性能问题的根源所在)，为后面的代码优化指明方向。 要想讲清楚这个(火焰图)，目前没有想到合适的方法；如若把我走过的弯路和正道都讲一遍，就认识而言也算得上比较全面了。 扯远了，要讲的毕竟不是什么屠龙之术。trace 是要真刀真枪干的，还是先把环境搞起来。看下面这段 cpp 我就用它做原型了。 #include &lt;iostream&gt; void foo() { int j = 0; for(int i=0; i&lt; 1000; i++) { j = i * 10 + 1; } } void bar() { int j = 0; for(int i=0; i&lt; 2000; i++) { j = i * 10 + 1; } } int main(int, char**) { while(true) { foo(); bar(); } } 程序的逻辑非常直接， main 函数会不断地调用 foo 和 bar 这两个函数。其实仔细看 foo 和 bar 的逻辑几乎一样，foo 会给 j 赋值 1k 次，而 bar 会给 j 赋值 2k 次。如果说 foo 的开销是 1 那么 bar 的开销就应该是 2 ，整个 main 的开销差不多就是 3 。 把程序编译安装到 /usr/bin/fires 后面这个程序就是我的实验环境了。注意这个安装路径我们会在后面的 trace 里面用到它。 性能测试 前面的分析得知，如果说 main 函数运行了 3s ，那么应该是有 1s 花在了 foo 函数上，有 2s 花在了 bar 函数上。至于是不是这个比例还要用数字说话，这个数据确认起来也相对简单，我们只要把程序运行起来把各个函数的耗时统计一下，就行了。 1. 在后台运行程序 nohup /usr/bin/fires &amp; top Tasks: 128 total, 2 running, 126 sleeping, 0 stopped, 0 zombie %Cpu(s): 50.2 us, 0.0 sy, 0.0 ni, 49.8 id, 0.0 wa, 0.0 hi, 0.0 si, 0.0 st MiB Mem : 7685.1 total, 712.8 free, 3708.5 used, 3263.9 buff/cache MiB Swap: 0.0 total, 0.0 free, 0.0 used. 3573.2 avail Mem PID USER PR NI VIRT RES SHR S %CPU %MEM TIME+ COMMAND 2509283 root 20 0 5828 2000 1832 R 100.0 0.0 0:10.55 fires 2369444 root 20 0 51936 11068 4480 S 0.4 0.1 2:16.32 redis-s+ 1 root 20 0 169420 11492 7248 S 0.0 0.1 2:51.53 systemd 2 root 20 0 0 0 0 S 0.0 0.0 0:01.45 kthreadd 可以看到当程序运行起来之后它自己独占了一个核，并且把这个核心跑到了 100% 。 2. 统计函数的执行耗时 统计 foo | bar 函数的执行耗时，这里我就直接用 ebpf 来追踪，foo 函数的耗时统计代码如果。 #!/usr/bin/bpftrace BEGIN { printf(&quot;start trace fires user-function foo calls .... Hit Ctrl-C to end.\\n&quot;); @btrace_begin[$1] = nsecs; @counts[$1] = 0; @duration[$1] = 0; } uprobe:/usr/bin/fires:foo { @start[$1] = nsecs; @counts[$1] = @counts[$1] + 1; } uretprobe:/usr/bin/fires:foo /@start[$1]/ { $temp = nsecs; if($temp &gt; @start[$1]) { @duration[$1] = @duration[$1] + (int64)($temp - @start[$1] ); } } END { @btrace_end[$1] = nsecs; printf(&quot;\\ntrace %d (s) \\n&quot;, (@btrace_end[$1] - @btrace_begin[$1])/1e9); printf(&quot;function foo called %d times, spend %d ms&quot;, @counts[$1], (int64)(@duration[$1] / 1e6)); delete(@btrace_begin[$1]); delete(@btrace_end[$1]); delete(@start[$1]); delete(@counts[$1]); delete(@duration[$1]); } foo 的耗时统计(追踪了 9s ，发现 foo 总共执行了 3.2s ) ./trace-foo.bt ~ Attaching 4 probes... start trace fires user-function foo calls .... Hit Ctrl-C to end. trace 9 (s) function foo called 982712 times, spend 3266 ms bar 的耗时统计(追踪了 6s ，发现 bar 总共执行了 4.1s) ./trace-bar.bt ~ Attaching 4 probes... start trace fires user-function bar calls .... Hit Ctrl-C to end. trace 6 (s) function bar called 708620 times, spend 4069 ms trace 的结果，基本上和分析源码得出来的结论差不多。foo 的开销大概是总时间的 1/3 ，bar 的开销大概是总时间的 2/3 。也就是说我们通过 trace 看到了程序执行中最为耗时的调用 “bar 函数”。 对于一个像 MySQL 这么大的项目我们不可能，每一个函数都去 trace 来发现问题，这么做的工作量就太大了。 火焰图的理论基础 数学啊！它有时能把复杂的事情搞简单，如果我们不想全量 trace 的话，我们可以像《统计学》一样抽样啊！ 函数的调用在计算机里面是通过压栈来完成的，一个函数执行完成之后就会从栈中弹出，对于我们这个程序的函数调用栈只有以下两种可能。要么它在执行 foo ，要么它在执行 bar 。 如果我们在程序运行的过程中，去抽样&amp;统计函数调用栈各个情况和它的占比，只要样本数据够多，执行 foo 的情况差不多会占 33.3%，执行 bar 的情况差不多会占 66.6%. 这能说明什么呢？你去采样了 100 次，有 66 次是 bar 占着 cpu 在干活，说明主要的耗时就在 bar 这个函数上呀。如果有办法把 bar 运行时间降下来，那节约的是时间，提升的是性能。在想到解决办法之前，更加重要的一步是发现问题的根源，火焰图可以帮我们发现这类问题。 如果在这里直接上火焰图的话，那就是忘了初心了。现在我打算用 excel 手工一步步把画火焰图画出来，假设我们采样了 3 次，那么我们的结果有可能是像下面这样的。 三个分裂的图，这样不够直观。现在处理一下把它合成一张图，在整理的过程中，还要尽可能把同一个函数多个样本点放在一起；在这里我把两个 bar 合并在一起。 一个手工用 excel 搞的火焰图就这样出来了。 总结 关于火焰图怎么看 第一步：从下往上看，找到最宽的那条，它就是主要矛盾。如果看不出什么问题，不要留恋，我们要再往上看。 第二步：重复第一步，直到看到了最上面。 原文链接 蒋乐兴-初代庄主 ","tags":[],"title":"完全掌握火焰图的作图原理&看图技巧","link":"https://toomi.pages.dev/post/wan-quan-zhang-wo-huo-yan-tu-de-zuo-tu-yuan-li-andkan-tu-ji-qiao/","stats":{"text":"7 min read","time":415000,"words":1691,"minutes":7},"dateFormat":"2022-11-26"},{"content":"起因 在java官方文档种，有提示 Staying Away from the Stack Class ， 原因是Stack 类继承 Vector ，但是Vector 类的设计和实现有缺陷，Vector是在java 1.0中开始出现，在java 1.2 版本后，新的collection框架中的List已经是官方推荐的Vector的替代品，而Vector仅仅是作为向后兼容使用。 不推荐使用Vector的的原因如下，原文： 性能问题，Vector的所有方法都使用了 synchronization ,也就是加锁了 Vector只是对方法进行加锁，对整个类没有加锁，也就是不同线程，可以同时调用同一实例不同的方法 官方替代品 推荐用ArrayList 替代 Vector， Deque 替代 Stack 手写\bStack 先定义Stack接口 /** * A small stack interface. You can (1) query the size of the stack, (2) ask * whether it is empty, (3) push an item, (4) pop an item, and (5) peek at the * top item. */ public interface Stack { /** * Adds the given item to the top of the stack. */ void push(Object item); /** * Removes the top item from the stack and returns it. * * @exception java.util.NoSuchElementException if the stack is empty. */ Object pop(); /** * Returns the top item from the stack without popping it. * * @exception java.util.NoSuchElementException if the stack is empty. */ Object peek(); /** * Returns the number of items currently in the stack. */ int size(); /** * Returns whether the stack is empty or not. */ default boolean isEmpty() { return size() == 0; } } A Linked Stack ,基于 link 实现 import java.util.NoSuchElementException; /** * An implementation of the stack interface using singly-linked nodes. */ public class LinkedStack implements Stack { private record Node(Object data, Node next) { } private Node top = null; private int size = 0; public void push(Object item) { top = new Node(item, top); size++; } public Object pop() { var item = peek(); top = top.next; size--; return item; } @Override public boolean isEmpty() { // Override the default implementation for efficiency return top == null; } public Object peek() { if (isEmpty()) { throw new NoSuchElementException(); } return top.data; } public int size() { return size; } } 利用stack解决括号匹配问题 public class ValidBracketChecker { public static boolean validBrackets(String s) { var stack = new LinkedStack(); try { for (var c : s.toCharArray()) { if (c == '(' || c == '[' || c == '{') { stack.push(String.valueOf(c)); } else if (c == '}' &amp;&amp; !stack.pop().equals(&quot;{&quot;)) { return false; } else if (c == ']' &amp;&amp; !stack.pop().equals(&quot;[&quot;)) { return false; } else if (c == ')' &amp;&amp; !stack.pop().equals(&quot;(&quot;)) { return false; } } // If there is anything left over on the stack after running // through the whole string, we have &quot;unclosed&quot; brackets return stack.isEmpty(); } catch (Exception e) { return false; } } public static void main(String[] args) { System.out.println(validBrackets(args[0])); } } 参考 https://cs.lmu.edu/~ray/notes/stacks/ ","tags":[],"title":"java手写栈","link":"https://toomi.pages.dev/post/java-shou-xie-zhan/","stats":{"text":"3 min read","time":166000,"words":525,"minutes":3},"dateFormat":"2022-11-25"},{"content":" 两个函数都是中间操作，都非常的‘懒’，没有对Stream的终止操作，两个函数都不会工作。 peek函数的存在仅仅是为了debug，而map是Stream的一个核心函数，两个函数的地位不同。 两个函数的参数（peek是Consumer，map是Function）起作用的时机不同。map的Function在生成新的Stream之前被执行，新Stream中的元素是上游Stream中元素经Function作用后的值。peek函数的Consumer工作在生成Stream之后。 一个有代表性的样例 以下面一段函数（来自peek函数的官方注释）为例解释peek和map两个函数工作机制的不同： List&lt;String&gt; list = Stream.of(&quot;one&quot;, &quot;two&quot;, &quot;three&quot;, &quot;four&quot;) .filter(e -&gt; e.length() &gt; 3) .peek(e -&gt; System.out.println(&quot;Filtered value: &quot; + e)) .map(String::toUpperCase) .peek(e -&gt; System.out.println(&quot;Mapped value: &quot; + e)) .collect(Collectors.toList()); System.out.println(list); 输出如下： Filtered value: three Mapped value: THREE Filtered value: four Mapped value: FOUR [THREE, FOUR] 工作示意图如下： 工作流程概述： 初始Stream包含四个字符串:one，two，three和four Stream遇到的第一个中间操作是filter，filter的谓词是只保留长度大于3的字符串，经过谓词过滤后filter返回是一个仅包含两个字符串three和four的Stream1，谓词工作在Stream1生成之前。 Stream遇到的第二个中间操作是peek，peek的Consumer是打印Stream中的字符串，peek直接生成一个和上游Stream1包含相同元素的Stream2，peek函数的Consumer工作在生成Stream2之后。 Stream遇到的第三个中间操作是map，map的Function将字符串转换为全大写，Function作用于上游Stream2的每一个元素，并生成新的Stream3。 Stream遇到的第四个中间操作是peek，peek的Consumer依然只是打印Stream4中的字符串，Consumer依然工作在Stream4生成之后。 map函数对Stream中元素执行的是映射操作，会以新的元素(map的结果)填充新的Stream，严格的讲map不是修改原来的元素。peek只能消费Stream中的元素，是否可以更该Stream中的元素，取决于Stream中的元素是否是不可变对象。如果是不可变对象，则不可修改Stream中的元素；如果是可变对象，则可以修改对象的值，但是无法修改对象的引用。 来源： 作者：大哥你先走 链接：https://www.jianshu.com/p/4fabc8a7abca ","tags":[],"title":"peek和map的异同之处","link":"https://toomi.pages.dev/post/peek-he-map-de-yi-tong-zhi-chu/","stats":{"text":"3 min read","time":139000,"words":599,"minutes":3},"dateFormat":"2022-11-23"},{"content":"Mutable Objects: When you have a reference to an instance of an object, the contents of that instance can be altered Immutable Objects: When you have a reference to an instance of an object, the contents of that instance cannot be altered Building an Immutable class Fields must be private Obviously all of the fields must be private. There is little point removing the mutators if they aren't even required to change the instance contents. Make sure methods can't be overridden. If your class gets extended, it could add extra fields that are not immutable, or the methods could be overridden to return a different value each time. There are two ways to protect against this. The preferred way is to make the class final. This is sometimes referred to as &quot;Strong Immutability&quot;. It prevents anyone from extending your class and accidentally or deliberately making it mutable. The second way, also called &quot;Weak Immutability&quot; is to make your methods final. It allows others to extend your class to add more behaviour, but protects the original contract specified by the class. If you want a more verbose description, imagine a class A is weakly immutable. If you have an instance of object A, it is immutable. If someone creates class B that extends A, it is only the behaviour defined by the A class that is immutable. Any added behaviour from class B may not be immutable. Protect mutable fields The last requirement which many people fall victim too, is to build your immutable class from primitive types or immutable fields, otherwise you have to protect mutable fields from manipulation. To highlight this problem, we'll use the example of a supposedly immutable class representing a person. Our class has a first and last name, as well as a date of birth. import java.util.Date; public final class BrokenPerson { private String firstName; private String lastName; private Date dob; public BrokenPerson( String firstName, String lastName, Date dob) { this.firstName = firstName; this.lastName = lastName; this.dob = dob; } public String getFirstName() { return this.firstName; } public String getLastName() { return this.lastName; } public Date getDOB() { return this.dob; } } This all looks fine, until someone uses it like this: Date myDate = new Date(); BrokenPerson myPerson = new BrokenPerson( &quot;David&quot;, &quot;O'Meara&quot;, myDate ); System.out.println( myPerson.getDOB() ); myDate.setMonth( myDate.getMonth() + 1 ); System.out.println( myPerson.getDOB() ); Our Template for Immutable Classes Now we have a template for creating immutable objects. Make all fields private Don't provide mutators Ensure that methods can't be overridden by either making the class final (Strong Immutability) or making your methods final (Weak Immutability) If a field isn't primitive or immutable, make a deep clone on the way in and the way out. Which classes are Immutable? To finish up, lets discuss the common Java classes that are immutable and those that aren't. Firstly, all of the java.lang package wrapper classes are immutable: Boolean, Byte, Character, Double, Float, Integer, Long, Short, String. As in the Person classes we discussed, java.util.Date objects are not immutable. The classes java.math.BigInteger and BigDecimal are not immutable either, although maybe they should have been. source https://javaranch.com/journal/2003/04/immutable.htm ","tags":[],"title":"Mutable and Immutable Objects","link":"https://toomi.pages.dev/post/mutable-and-immutable-objects/","stats":{"text":"4 min read","time":204000,"words":545,"minutes":4},"dateFormat":"2022-11-19"},{"content":"字符集就是字符的集合，如常见的 ASCII字符集，GB2312字符集，Unicode字符集等。这些不同字符集之间最大的区别是所包含的字符数量的不同。 字符编码则代表字符集的实际编码规则，是用于计算机解析字符的，如 GB2312，GBK，UTF-8 等。字符编码的本质就是如何使用二进制字节来表示字符的问题。 字符集和编码是一对多的关系，同一字符集可能有多种字符编码，如Unicode字符集就有 UTF-8，UTF-16 等。 在前端开发中，Javascript程序是使用Unicode字符集，Javascript源码文本通常是基于UTF-8编码。 但js代码中的字符串类型是UTF-16编码的，这也是为什么会碰到api接口返回字符串在前端出现乱码，因为多数服务都使用utf-8编码，前后编码方式不一致。 说起字符集的发展历程，可以总结为一句话：几乎都是对ASCII字符集的扩展。 ASCII 我们知道，计算机是使用二进制来处理信息的。 其中，每一个二进制位(bit)有 0和1 两种状态。一个字节(byte)则有8个二进制位，可以有256种状态。 而ASCII就是基于拉丁字母、主要用于显示英文的一种单字节字符集，它的编码和字符是一一对应的，因为它就是使用一个字节8个二进制位来表示，不会超过256个字符。 标准的ASCII字符总计有128个字符(2^7)，其中前面32个控制字符，后面96个是可打印字符，包括常用的大小写字母数字标点符号等。因为只占用了一个字节的后7位，那字节的最高位一般设置为0。 'a'.charCodeAt() // 97 'A'.charCodeAt() // 65 '9'.charCodeAt() // 57 '.'.charCodeAt() // 46 如上，每个字符会对应一个编码(使用数字标识)，总共会从0-128。完整的ASCII码表，网上很容易找到。 通过ASCII码表，我们发现，小写字母并没有和大写字母挨着排序？这是为了方便大小写之间的转换， A 排在 65(64 + 1) 位，而 a 排在 97(64 + 32 + 1) 位。 65 ^ 32 = 97 // A ^ 32 = a 字符集的发展历史 ASCII是几乎所有字符集的基础。 标准的ASCII码最多只能标识128个字符，欧美国家可以很好的使用，但其他国家的字符变多，自然就不够用了。 这个时候，最高位就开始被惦记上，通过扩展ASCII码的最高位，又能满足用于特殊符号的一些国家的需求，这种就是扩展ASCII码。 但是亚非拉更多非拉丁语系的国家，字符成千上万，只能使用新的方式。 如中文，就又进行了扩展，小于127的字符的意义与标准ASCII码相同，当需要标识汉字时，使用2个字节，每个字节都大于127。这种多字节字符集即GB2312，后续因为不断的扩展，如繁体字和各种符号，甚至少数民族的语言符号等等，又使用了包括GBK等不同字符集。 因此，很多国家都制定了自己的编码字符集，基本都是在ASCII的基础上进行的。 各字符集虽然都能够兼容标准ASCII码，但在使用交流上的不便是显而易见的，乱码也是随处可见。为了解决这种各自为战的问题，Unicode字符集就诞生了。 Unicode Unicode是国际组织制定的，用于收纳世界上所有文字和符号的字符集方案。 前128个字符同ASCII一样，进行扩充后，使用数字0-0x10FFFF来映射这些字符，最多可以有1114112个字符。目前仍然只使用了其中的一小部分。 Unicode 编码字符集旨在收集全球所有的字符，为每个字符分配唯一的字符编号即代码点（Code Point），用 U+紧跟着十六进制数表示。所有字符按照使用上的频繁度划分为 17 个平面（编号为 0-16），即基本的多语言平面和增补平面。基本的多语言平面（英文为 Basic Multilingual Plane，简称 BMP）又称平面 0，收集了使用最广泛的字符，代码点从 U+0000 到 U+FFFF，每个平面有 2^16=65536 个码点；增补平面从平面 1~16，分为增补多语言平面（平面 1）、增补象形平面（平面 2）、保留平面（平面 3~13）、增补专用平面等，每个增补平面也有 2^16=65536 个码点。所以 17 个平面总计有 17 × 65,536 = 1,114,112 个码点。下图 是 Unicode 平面分布图，以及 Unicode 各个平面码点空间 Unicode只是一个字符集，定义了字符与数字的映射关系，但对于计算机中如何存储，没有做任何规定。由于字符数量之大，码点的范围很宽，排在前面的码点，可能用1个字节就能表示，而码点较大的，可能需要2个字节，3个字节，4个字节才能表示。那么计算机如何确定是将1个字节解释为一个字符呢，还是将2个字节连在一起解释为一个字符呢？还是3个，4个字节连在一起为一个字符。于是出现了一些解决方案： 取最大的，将所有字符都存储为4个字节（假设4个字节已经足够囊括所有码点），这样计算机固定以4个字节为单位来解释字符编码。但这样会产生极大的空间浪费，对于一篇纯英文文档，会浪费3倍的空间。 存储表示字节数的信息，让计算机知道该字符是以几个字节来存储的，如UTF-8编码。 Unicode字符集可以有不同的编码方式，如UTF-8,UTF-16,UTF-32，这里UTF指的是Unicode Transformation Format，即Unicode转换格式，即将Unicode编码空间中每个字符对应的码点，与字节顺序进行一一映射。 码元 码元(Code Unit)可以理解为对码点进行编码时的最小基本单元，码元是一个整体。而字符编码的作用就是将Unicode码点转换成码元序列。 Unicode常用的编码方式有 UTF-8 、UTF-16 和 UTF-32，UTF是Unicode TransferFormat的缩写。 UTF-8是8位的单字节码元，UTF-16是16位的双字节码元，UTF-32是32位的四字节码元。 编码方式 码元 编码后字节数 UTF-8 8位 1-4字节 UTF-16 16位 2字节或者4字节 UTF-32 32位 4字节 另外，为什么总看到使用十六进制数据来表示如码点等各种数据呢？ 因为，两位的十六进制正好等于一个字节8位，0xff = 0b11111111。 UTF 字节序 最小编码单元是多字节才会有字节序的问题存在，UTF-8 最小编码单元是一字节，所以 它是没有字节序的问题，UTF-16 最小编码单元是 2 个字节，在解析一个 UTF-16 字符之前，需要知道每个编码单元的字节序 比如：前面提到过，&quot;中&quot; 字的 Unicode 码是 4E2D, &quot;?&quot; 字符的 Unicode 码是 2D4E， 当我们收到一个 UTF-16 字节流 4E2D 时，计算机如何识别它表示的是字符 &quot;中&quot; 还是 字符 &quot;?&quot; 呢 ? 所以，对于多字节的编码单元，需要有一个标记显式的告诉计算机，按照什么样的顺序解析字符，也就是字节序，字节序分为 大端字节序 和 小端字节序 小端字节序简写为 LE( Little-Endian ), 表示 低位字节在前，高位字节在后, 高位字节保存在内存的高地址端，而低位字节保存在内存的低地址端 大端字节序简写为 BE( Big-Endian ), 表示 高位字节在前，低位字节在后，高位字节保存在内存的低地址端，低位字节保存在在内存的高地址端 下面以 0x4E2D 为例来说明大端和小端，具体参见下图: 数据是从高位字节到低位字节显示的，这也更符合人们阅读数据的习惯，而内存地址是从低地址向高地址增加 所以，字符0x4E2D 数据的高位字节是 4E，低位字节是 2D 按照大端字节序的高位字节保存内存低地址端的规则，4E 保存到低内存地址 0x10001 上，2D 则保存到高内存地址 0x10002 上 对于小端字节序，则正好相反，数据的高位字节保存到内存的高地址端，低位字节保存到内存低地址端的，所以 4E 保存到高内存地址 0x10002 上，2D 则保存到低内存地址 0x10001 上 BOM BOM 是 byte-order mark 的缩写，是 &quot;字节序标记&quot; 的意思, 它常被用来当做标识文件是以 UTF-8、UTF-16 或 UTF-32 编码的标记 在 Unicode 编码中有一个叫做 &quot;零宽度非换行空格&quot; 的字符 ( ZERO WIDTH NO-BREAK SPACE ), 用字符 FEFF 来表示 。对于 UTF-16 ，如果接收到以 FEFF 开头的字节流， 就表明是大端字节序，如果接收到 FFFE， 就表明字节流 是小端字节序。 UTF-8 没有字节序问题，上述字符只是用来标识它是 UTF-8 文件，而不是用来说明字节顺序的。&quot;零宽度非换行空格&quot; 字符 的 UTF-8 编码是 EF BB BF, 所以如果接收到以 EF BB BF 开头的字节流，就知道这是UTF-8 文件 ， 下面的表格列出了不同 UTF 格式的固定文件头 UTF编码 固定文件头 UTF-8 EF BB BF UTF-16LE FF FE UTF-16BE FE FF UTF-32LE FF FE 00 00 UTF-32BE 00 00 FE FF 根据上面的 固定文件头，下面列出了 &quot;中&quot; 字在文件中的存储 ( 包含文件头 ) 编码 固定文件头 Unicode 编码 0X004E2D UTF-8 EF BB BF 4E 2D UTF-16BE FE FF 4E 2D UTF-16LE FF FE 2D 4E UTF-32BE 00 00 FE FF 00 00 4E 2D UTF-32LE FF FE 00 00 2D 4E 00 00 MySQL 中的 utf8 和 utf8mb4 MySQL 中的 &quot;utf8&quot; 实际上不是真正的 UTF-8， &quot;utf8&quot; 只支持每个字符最多 3 个字节, 对于超过 3 个字节的字符就会出错, 而真正的 UTF-8 至少要支持 4 个字节。MySQL 中的 &quot;utf8mb4&quot; 才是真正的 UTF-8，下面以 test 表为例来说明, 表结构如下: mysql&gt; show create table test\\G *************************** 1. row *************************** Table: test Create Table: CREATE TABLE `test` ( `name` char(32) NOT NULL ) ENGINE=InnoDB DEFAULT CHARSET=utf8 向 test 表分别插入 &quot;中&quot; 字 和 Unicode 码为 0x10A6F 的字符，这个字符需要从 https://unicode-table.com/cn/10A6F/ 直接复制到 MySQL 控制台上，手工输入会无效，具体的执行结果如下图: 原文链接 https://www.cnblogs.com/jimojianghu/p/16205678.html https://blog.51cto.com/u_15445690/4720328 https://www.51cto.com/article/661981.html ","tags":[],"title":"字符集和字符编码","link":"https://toomi.pages.dev/post/zi-fu-ji-he-zi-fu-bian-ma/","stats":{"text":"11 min read","time":642000,"words":2845,"minutes":11},"dateFormat":"2022-11-17"},{"content":"整体架构 从Mysql官网提供的体系结构图中可以看出来出来，整个mysql分为以下几部分 MyISAM不支持事务(不支持ACID),InnoDB支持事务(支持ACID) 使用DML语句时，MyISAM锁级别是表锁，同一时间只支持一个session修改表,而InnoDB是行锁,粒度更细，并发更高 MyISAM不支持外键,InnoDB支持 MyISAM写入速度慢,查询速度快，InnoDB写入快，查询慢(相对) InnoDB因为支持事务，所以更可靠，MyISAM无法保证数据完整性 MyISAM只存储索引，不缓存数据，支持全文索引,InnoDB缓存索引以及数据,不支持全文索引(5.6之前) 所以在选择哪个引擎的时候要根据实际业务选择，比如某个业务只会插入一次数据，后面的都只有查询,那么我就建议使用MyISAM，如果需要支持事务那就用InnoDB。 InnoDB引擎 由于我们平时使用多的还是InnoDB引擎，所以这里主要剖析下InnoDB的存储结构(MyISAM和InnoDB在存储结构上是类似的) 5.7和8.0有些许不一致，主要是系统表空间的一些内容会移动到单独的表空间中去 为了保证数据不丢失,我们的数据必须要落盘,但是如果每次增删改查都去操作磁盘，那效率太低了，所以mysql会在内存中抽象出一份数据结构和磁盘一一对应，这样我们每次的增删改查就会优先操作内存中的数据。 咱们插入的数据是存储在磁盘的,而平时我们通过Navicate或者shell查询出来的数据，展现形式为表格，并不代表我们的数据也是这样直接这样存储在磁盘上的，磁盘上的数据有自己的格式。 比如我们将数据存在在文件是这样存储的 user.id=1 user.name=think123 user.age=18 内存结构 将内存中数据刷新到磁盘并不是一条数据一条数据从内存刷新到磁盘的，为了效率，InnoDB将数据划分为若干个页，以页作为磁盘和内存之间交互的基本单位，InnoDB中页的⼤⼩⼀般为 16KB,和磁盘上默认的页大小一致。 也就是在⼀般情况下，⼀次少从磁盘中读取16KB的内容到内存中，⼀次少把内存中的16KB内容刷新到磁盘中。 而内存数据结构主要分为 Buffer Pool(缓冲池) 和 Log Buffer(日志缓冲)，它们管理的都是页，而页中存储的是数据。 原文 https://z.itpub.net/article/detail/3E097E5138AA4A892D4BA75704C34B07 ","tags":[],"title":"MySQL的体系结构","link":"https://toomi.pages.dev/post/mysql-de-ti-xi-jie-gou/","stats":{"text":"3 min read","time":147000,"words":688,"minutes":3},"dateFormat":"2022-11-17"},{"content":" Of the many subfields within computer science, algorithms and data structures may be the most fundamental—it seems to characterize computer science like perhaps no other. What is involved in the study of algorithms and data structures? Why Must They Go Together? data structures and algorithms were mentioned together. Why? Computing systems are concerned with the storage and retrieval of information. For systems to be economical, the data must be organized (into data structures) in such a way as to support efficient manipulation (by algorithms). Choosing the wrong algorithms and data structures makes a program slow at best and unmaintainable and insecure at worst. How Do They Go Together? It’s very helpful to think of the data type as the bridge between data structures and algorithms. Data Types So these data types are the key to everything right? What are they, exactly? First of all, note that programs manipulate information, and information in a program is represented with entities. Entities are things like numbers, text, lists, and so on. To help us understand entities, it helps to classify them into types. We have an intuition about what types are. As a first approximation, a type is just a set of values, for example: ℕ={0,1,2,3,4,...} ℤ={...,−2,−1,0,1,2,...} 𝔹={𝖿𝖺𝗅𝗌𝖾,𝗍𝗋𝗎𝖾} ℝ=the set of all real numbers ℂ=the set of complex numbers ℍ=the set of quaternions 𝖯𝗋𝗂𝗆𝖺𝗋𝗒𝖢𝗈𝗅𝗈𝗋={𝖱𝖤𝖣,𝖦𝖱𝖤𝖤𝖭,𝖡𝖫𝖴𝖤} 𝖲𝗎𝗂𝗍={𝖲𝖯𝖠𝖣𝖤𝖲,𝖧𝖤𝖠𝖱𝖳𝖲,𝖣𝖨𝖠𝖬𝖮𝖭𝖣𝖲,𝖢𝖫𝖴𝖡𝖲} 𝖳𝗐𝗈𝖣𝗂𝗆𝖾𝗇𝗌𝗂𝗈𝗇𝖺𝗅𝖯𝗈𝗂𝗇𝗍=ℝ×ℝ 𝖢𝖺𝗋𝖽={1..13}×𝖲𝗎𝗂𝗍 𝖣𝖾𝖼𝗄=the set of all permutations of Card 𝖲𝗆𝖺𝗅𝗅𝖱𝖾𝖺𝗅={𝑥∣𝑥∈ℝ∧0.0≤𝑥∧𝑥≤1.0} 𝖢𝗈𝗅𝗈𝗋=𝖯𝗋𝗂𝗆𝖺𝗋𝗒𝖢𝗈𝗅𝗈𝗋→𝖲𝗆𝖺𝗅𝗅𝖱𝖾𝖺𝗅 𝖡𝖳𝖲𝖬𝖾𝗆𝖻𝖾𝗋𝗌={Jin,Suga,J-Hope,RM,Jimin,V,Jungkook} 𝖫𝗂𝗌𝗍&lt;𝖨𝗇𝗍𝖾𝗀𝖾𝗋&gt;=ℤ∗ 𝖫𝗂𝗌𝗍&lt;𝛼&gt;=𝛼∗ (woah—that type is parametrized by a type variable) 𝖳𝗋𝖾𝖾&lt;𝛼&gt;=𝛼×𝖳𝗋𝖾𝖾&lt;𝛼&gt;∗ 🤯 That’s not really good enough, though! The reason why entities have certain types is that they behave in certain ways: you can multiply numbers, reverse lists, capitalize text, block users, revoke credentials, promote employees, and so on. Properly specifying a type requires defining how the entities of that type behave. These behaviors are captured (algebraically 😮😮😮) as operations with zero or more inputs and a single output. For example: The same thing can be said without pictures, for example: CLUBS: Suit TEN: Rank HEARTS: Suit JACK: Rank DIAMONDS: Suit QUEEN: Rank SPADES: Suit KING: Rank ACE: Rank makeCard : Suit × Rank → Card TWO: Rank getSuit : Card → Suit THREE: Rank getRank : Card → Rank FOUR: Rank newDeck : Deck FIVE: Rank positionOf : Deck × Card → int SIX: Rank cardAt : Deck × int → Card SEVEN: Rank topCard : Deck → Card EIGHT: Rank shuffle : Deck → Deck NINE: Rank split : Deck → Deck Ah, now we have behaviors. A data type is a set of values together with operations that define their behavior. Not every operation is applicable to every type. Types provide constraints on what we can and cannot say. Abstract Data Types As an aside: you will sometimes hear the term abstract data type, abbreviated “ADT.” People use this term to emphasize the fact that the “internal structure and internal workings of the operations” are somehow unknown to the outside world. Only the effects of the operations matter. To many people, though, saying “data type” is enough—all datatypes are “abstract” in that sense. A useful exercise is to identify things in your world and try to list as many behaviors as you can. Here is a starter set: Data Structures A data structure is an arrangement of data for the purpose of being able to store and retrieve information. Usually a data structure exists in order to provide a physical representation of a data type. For example, a list (a data type, which we saw above) can be represented by an array (a data structure) or by attaching a link from each element to its successor (another kind of data structure). Wait, what are arrays and links? Building Blocks Data structures are built with three primary structuring mechanisms: Let’s dive in. A object, also known as an record, struct, or named tuple, is a single entity that has many properties, where each property has a name and a value, which is another entity. Here is a simple object, can you guess what it represents? An array is an entity that has a number of constituent elements, packed tightly together in memory and indexable by small natural numbers. In most programming languages, the first index is 0, in others (Julia, Lua, MATLAB), the first index is 1. Here are some cities in Alaska to visit (in order, because arrays are ordered): Important: Note that were the properties of a object come together to make one conceptual thing; arrays are generally thought of as a collection of distinct things. ","tags":[],"title":"Data Structures and Algorithms","link":"https://toomi.pages.dev/post/data-structures-and-algorithms/","stats":{"text":"5 min read","time":283000,"words":757,"minutes":5},"dateFormat":"2022-11-15"},{"content":"收集平时用对工具 在线类 表格转换markdown 图书下载 tvbox iptv iptv6 网盘 idea active ","tags":[],"title":"工具","link":"https://toomi.pages.dev/post/gong-ju/","stats":{"text":"1 min read","time":6000,"words":27,"minutes":1},"dateFormat":"2022-11-08"},{"content":" In programming, concurrency is the composition of independently executing processes, while parallelism is the simultaneous execution of (possibly related) computations. Concurrency is about dealing with lots of things at once. Parallelism is about doing lots of things at once.” Source: blog.golang.org Concurrency is about structure, parallelism is about execution. Unit of Concurrency As far as I know, concurrency has three levels: Multiprocessing: Multiple Processors/CPUs executing concurrently. This unit here is a CPU. Multitasking: Multiple tasks/processes running concurrently on a single CPU. The OS executes these tasks by switching between them very frequently. This unit here is a Process. Multithreading: Multiple parts of the same program running concurrently. In this case, we go a step further and divide the same program into multiple parts/threads and run those threads concurrently. Thread in Java Creating and Managing Thread There are two options for creating a Thread in Java. Each thread is created in Java 8 will consume about 1MB as default on OS 64 bit. You can check via command line: java -XX:+PrintFlagsFinal -version | grep ThreadStackSize. Option 1: You can create a class that inherits the Thread.java class. Option 2: You can use a Runnable object. To create and manage Thread in Java you can use the Executors framework. Java Concurrency API defines three executor interfaces that cover everything that is needed for creating and managing threads: -** Executor: **launch a task specified by a Runnable object. ExecutorService: a sub-interface of Executor that adds functionality to manage the lifecycle of the tasks. ScheduledExecutor: a sub-interface of ExecutorService that adds functionality to schedule the execution of the tasks. Most of the executor implementations use thread pools to execute tasks. ","tags":[],"title":"Concurrency and parallelism in Java","link":"https://toomi.pages.dev/post/concurrency-and-parallelism-in-java/","stats":{"text":"2 min read","time":106000,"words":283,"minutes":2},"dateFormat":"2022-11-02"},{"content":"信号量及管程 并发问题: 存在竞争条件，除过互斥还需要同步等操作，或者临界区里有多个线程执行，锁无法满足 信号量 信号量(sem)用一个整形来表示，有两个原子操作 P操作: sem减一，如果sem&lt;0，等待，否则继续 V操作: sem加一，如果sem&lt;=0(当前有些进程在等待这个信号量)，唤醒一个等待的P 用一个铁路的例子来说明，铁路有一段是并行的，同时可以走两辆，那么当这段有两辆时，信号量就要求后面再来的火车等待，直到唤醒为止 P和V是荷兰语，荷兰科学家Dijkstra在20世纪60年代提出 V: Verhoog (荷兰语增加) P: Prolaag (荷兰语减少) 信号量机制在早期操作系统里是主要的同步原语，现在则少用了 信号量的使用 信号量是整数，是被保护的变量 初始化完成后，改变信号量的唯一方法是通过P和V P会阻塞，V不会 V后唤醒哪一个线程，实践中FIFO经常被使用 两种类型信号量 二进制信号量: 0/1两种取值 一般/计数信号量: 可取任何非负值 用二进制信号量实现互斥 在临界区前进行P操作，临界区后进行V操作，即可实现对临界区的互斥访问 muetx = new Semaphore(0); mutex-&gt;P(); // Critical Section mutex-&gt;V(); 用二进制信号量实现调度约束 初始化为0，线程A需要在线程B执行到某一步后才能继续执行，那么线程A这步前调用P操作，线程B这步后调用V操作，则线程A只有在B执行完后才会继续执行 更复杂的互斥操作 例如，目前有一个buffer，一个生产者线程会不停向里面写数据，一个消费者线程会从里面取数据，正确性要求 同一时间只能有一个线程操作缓冲区 缓冲区为空，消费者必须等待生产者 缓存区满，生产者必须等待消费者 如何用信号量解决：每个约束用一个单独信号量完成 mutex = new Semaphore(1); fullBuffers = new Semaphore(0); emptyBuffers = new Semaphore(n); 二进制信号量负责互斥，初始化为1 一般信号量fullBuffers，初始化为0 一般信号量emptyBuffers，初始化为n 生产者deposit函数 emptyBuffers-&gt;P(); // 一直小于0后才会阻塞 mutex-&gt;P(); Add c to the buffer mutex-&gt;V(); fullBuffers-&gt;V(); 消费者remove函数 fullBuffers-&gt;P(); mutex-&gt;P(); Remove c from buffer mutex-&gt;V(); emptyBuffers-&gt;V(); 两者的P和V都不能交换顺序 信号量的实现 信号量本身是一个整形 进程的等要通过等待队列实现 class Semaphore { int sem; WaitQueue q; }; Semaphore::P() { sem --; if (sem &lt; 0) { Add this thread t to q; block(p); } } Semaphore::V() { sem ++; if (sem &lt;= 0) { Remove a thread t from q; // FIFO wakeup(t); } } 信号量不能够处理死锁问题 管程 能不能把信号量更简化一些，管程就是抽象度更高的概念 目的: 分离互斥和条件同步，最开始出现在编程语言的设计里，针对语言并发机制完成 什么是管程（Monitor） 一个锁: 指定临界区 0或者多个条件变量: 等待/通知信号量用于管理并发访问共享数据，将需要等待的资源挂上去 如图，右上角是进入管程的队列，进入管程是互斥的 进入管程之后，线程要执行管程维护的一系列的函数，即圆柱体 执行函数过程中，会对管程维护的一系列共享变量进行操作，即图中x和y，得不到满足时需要等待在条件变量对应队列上，同时释放对管程的lock 条件变量有两个操作，wait和signal 具体各部分操作描述如下 Lock 即一个信号量，一个时刻只有一个线程可以进入管程中 Lock::Acquire(); Lock::Release(); Condition Variable Wait(); // 释放锁，睡眠 Signal(); // 唤醒等待者 Class Condition { int numWaiting = 0; WaitQueue = q; }; Condition::Wait(lock) { numWaiting ++; // 多了一个睡眠线程 Add this thread t to q; // 加入等待队列 release(lock); schedule(); // 选择下一个线程来执行，need mutex require(lock); } Condition::Signal() { if (numWaiting &gt; 0) { // 有线程等待条件变量 Remove a thread t from q; wakeup(t); // need mutex numWaiting --; } } 用管程来解决buffer问题 classBoundedBuffer { Lock lock; int count = 0; Condition notFull, notEmpty; }; // 一进入函数就互斥，保证进入管程线程唯一 BoundedBuffer::Deposit(c) { lock-&gt;Acquire(); while (count == n) notFull.Wait(&amp;lock); // 这里即前一步的wait，里面会release掉lock，让其它线程进入执行 Add c to the buffer; count ++; notEmpty.Signal(); lock-&gt;Release(); } BoundedBuffer::Remove(c) { lock-&gt;Acquire(); while (count == 0) notEmpty.Wait(&amp;lock); Remove c from buffer; count --; notFull.Signal(); lock-&gt;Release(); } 执行顺序 在一个线程signal令一个线程后，因为管程同时只能有一个线程在执行，让哪个线程继续执行，是一个问题 在一个线程signal令一个线程后，因为管程同时只能有一个线程在执行，让哪个线程继续执行，是一个问题 Hansen-style，发signal线程执行完后再执行等待线程 (Most real OSes, or Java) 这两种其实会有影响，如上面代码，如果用hoare机制，用if代替while也行，因为这时只唤醒一个等待线程，不用再次用while确认 用Hansen，会有多个线程来抢资源，要用while再次确认 10.5 经典同步问题¶ 10.5.1 读者-写者问题¶ 读者优先¶ 问题描述: 访问共享数据，读者优先，即读者可以跳过正在等待的写者去读数据 允许同一时间有多个读者，但任何时候都只有一个写者 没有写者时读者才能访问数据 没有读者和写者时写者才能访问数据 任何时候只有一个线程可以操作共享变量 对读者有一个计数RCount，初始化为0，因为可以有多个读者，同时用二元信号量CountMutex来保护，初始化为1 只会有一个写者，所以不用计数，只用一个二元信号量WriteMutex来保护，初始化为1 因为是对变量的保护，所以用二元信号量 Writer sem_wait(WriteMutex); // P，减 write; sem_post(WriteMutex); // V，加 Reader 因为CountMutex是对Rcount的信号量，所以每次操作Rcount前都要先减后增 sem_wait(CountMutex); if (Rcount == 0) sem_wait(WriteMutex); // 没有读者，确保没有写者即可 Rcount ++; sem_post(CountMutex); read; sem_wait(CountMutex); Rcount --; if (Rcount == 0) sem_post(WriteMutex); sem_post(CountMutex) 写者优先¶ 一旦写者就绪，后面的写者可以跳过读者来写 管程实现 Monitor's State Variables AR = 0; // active readers AW = 0; // active writers WR = 0; // waiting readers WW = 0; // waiting writers Condition okToRead; Condition okToWrite; Lock lock; Reader Database::Read() { // Wait until no writers StartRead(); read database; // checkout - wake up DoneRead(); } Database::StartRead() { lock.Acquire(); while (AW + WW &gt; 0) { WR ++; okToRead.wait(&amp;lock); // 被唤醒后继续执行 WR --; } AR ++; lock.Release(); } Database::DoneRead() { lock.Acquire(); AR --; if (AR == 0 &amp;&amp; WW &gt; 0) okToWrite.signal(); lock.Release(); } Writer Database::Write() { // Wait until no readers/writers 只包含正在读的读者 StartWrite(); write database; // check out - wake up waiting readers/writers DoneWrite(); } Database::StartWrite() { lock.Acquire(); while (AW + AR &gt; 0) { WW ++; okToWrite.wait(&amp;lock); WW --; } AW ++; lock.Release(); } Database::DoneWrite() { lock.Acquire(); AW --; if (WW &gt; 0) okToWrite.signal(); // 只唤醒一个线程 else if (WR &gt; 0) okToRead.broadcast(); // 唤醒所有线程，因为所有读线程都可以读，并且多个Reader可以同时读 lock.Release(); } 10.5.2 哲学家就餐问题¶ 问题描述: 1965年Dijkstra提出，5个哲学家围绕着一张圆桌而坐，桌上放着5把叉子，每两个哲学家之间放一支，哲学家动作包括思考和就餐，就餐时需要同时拿起左右两支叉子，思考则同时放下。如何保证哲学家们动作有序进行？(不会有人永远拿不到叉子) 死锁出现¶ #define N 5 void philosopher (int i) { while (true) { think(); take_fork(i); // 拿左边叉子 take_fork((i+1)%N); // 拿右边叉子 eat(); put_fork(i); put_fork((i+1)%N); } } 以上方法会出现死锁，所有人先拿左边叉子，再拿右边叉子，会每人拿到一把，死锁 改进¶ #define N 5 void philosopher (int i) { while (true) { think(); take_fork(i); // 拿左边叉子 if (fork((i+1)%N)) { take_fork((i+1)%N); eat(); break; } else { put_fork(i); wait_random_time(); } } } 如果拿不到右边叉子，放下左边叉子，随机等待一段时间，可行，但非万全之策 互斥访问 把拿叉子用锁保护，正确，每次只允许一个人进餐 理论上说，应该允许两个不相邻的哲学家同时进餐 正确算法¶ 因为相邻哲学家不能同时进餐，所以检查左右邻居就可以判断出自己能不能进餐 哲学家思考-&gt;饥饿-&gt;检查最优邻居-&gt;拿两把叉子吃东西-&gt;放下 拿一把叉子没有意义，这里通过状态来记录是否两把叉子都拿了起来 有数据结构来描述哲学家状态 有一个数据结构来描述每个哲学家当前状态，该状态是临接资源，读和写互斥 唤醒左邻右舍，存在着同步 #define N 5 // 哲学家个数 #define LEFT (i-1+N)%N // 左邻 #define RIGHT (i+1)%N // 右舍 #define THINKING 0 // 思考状态 #define HUNGRY 1 // 饥饿状态 #define EATING 2 // 进餐状态 int state[N] // 记录每个人状态 由于该数组是临界值，需要互斥访问该资源。这里把整个数组都保护起来，因为一个人尝试拿的时候，其他人状态不能改变 semaphore mutex; // 互斥信号量，初值1 一个哲学家吃饱后，可能要唤醒邻居，存在着同步关系 semaphore s[N]; // 同步信号量，初值0 算法主体 // 第i个哲学家执行的函数 void philosopher(int i) { while (true) { think(); take_forks(i); // 拿到两把叉子或者被阻塞 eat(); put_forks(i); } } void take_forks(int i) { P(mutex); state[i] = HUNGRY; test_take_left_right_forks(i); V(mutex); P(s[i]); // 没有叉子便阻塞 } void test_take_left_right_forks(int i) { if (state[i] == HUNGRY &amp;&amp; state[LEFT] != EATING &amp;&amp; state[RIGHT] != EATING) { state[i] = EATING; // 置为EATING即两把叉子已经拿起来 V(s[i]); // 通知自己吃饭，因为之后会有P操作 } } void put_forks(int i) { P(mutex); state[i] = THINKING; // 只要不是EATING，就是两把叉子都放下 test_take_left_right_forks(LEFT); test_take_left_right_forks(RIGHT); V(mutex); } put_forks里，左右邻居会被阻塞在take_forks里P(s[i])这句上，在test成功后，会V(s[i])，接着会执行eat 剩下think和eat相对简单，eat不需要做任何操作，think只需要在最开始将状态置为THINKING 以上五个进程并发执行即可解决哲学家就餐问题 原链接 https://dpatrickx.github.io/wiki/os/semaphore/ https://yuerer.com/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E4%B9%8B-%E4%BF%A1%E5%8F%B7%E9%87%8F/ ","tags":[],"title":"信号量及管程","link":"https://toomi.pages.dev/post/xin-hao-liang-ji-guan-cheng/","stats":{"text":"12 min read","time":664000,"words":2755,"minutes":12},"dateFormat":"2022-10-27"},{"content":"JAVA内存模型定义了一个规范，那就是每个线程都有一个工作内存，线程操作共享变量的时候需要从主内存读取到工作内存，然后再传递给工作线程使用。共享变量修改后先刷新到工作内存，然后再刷新回主内存。 JMM 关于同步规定： 线程解锁前，必须把共享变量的值刷新回主内存 线程加锁前，必须读取主内存的最新值到自己的工作内存 加锁解锁是同一把锁 JMM定义了8种指令，它工作的时候就是通过这8种指令来操作内存的。 包括怎么锁定变量、怎么将数据从主存传到工作内存、怎么传给正在被CPU调度的线程、修改之后CPU怎么传回工作内存、工作内存又怎么传递回主存： lock（锁定）：把主内存中的一个共享变量标记为一个线程独享的状态 unlock（解锁）：把主内存的变量从线程独享的lock状态中解除出来 read（读取）：把主内存的一个共享变量传输工作内存中 load（载入）：把从主内存传输到工作内存的共享变量，赋值给工作内存中的变量副本 use（使用）：把工作内存中的变量副本的值，传递给执行引擎CPU assign（赋值）：执行引擎（CPU）执行完之后，把修改过的变量值重新赋值给工作内存中的变量副本 store（存储）：把工作内存中修改过共享变量的值传递到主内存中 write（写入）：把传递到主内存中的变量值，重新写回给主内存的共享变量 比如线程A要执行一个x++的操作，可能要经历下面的过程： lock和unlock 工作线程A操作该共享内存变量的时候，执行lock指令，主内存中的这个共享变量；同时告诉线程B这个共享变量我准备修改了，让它失效掉。 B线程的变量副本失效之后，运行时候用到，需要到主内存重新读取（执行read、load操作放入工作内存）；发现该主内存的变量被锁定了，读取失败；此时相当于线程A拥有该变量的独享操作 线程A执行i++操作，经历上述说的（read、load、use、assign、store、write）指令之后；操作完成执行unlock释放锁定的这个内存变量 线程B这个时候再去主内存读取的时候，发现未被锁定，就可以重新读取了 变量不可见性内存语义 JMM：Java 内存模型，是 Java 虚拟机规范中所定义的一种内存模型，Java 内存模型是标准化的，屏蔽掉了底层不用计算机的区别。 Java 内存模型描述了 Java 程序中各种变量（线程共享变量）的访问规则，以及在 JVM 中将变量存储到内存和从内存中读取变量这样的底层细节。 JMM 具有以下规定： 所有的共享变量都存储于主内存。这里所说的变量指的是实例变量和类变量。不包含局部变量，因为局部变量是线程私有的，因此不存在竞争问题。 每一个线程还存在自己的工作内存，线程的工作内存，保留了被线程使用的变量的工作副本。 线程对变量的所有的操作（读、取）都必须在工作内存中完成，而不能直接读写主内存中的变量。 不同线程之间也不能直接访问对方工作内存中的变量，线程间变量的值的传递需要通过主内存中转来完成。 变量不可见性解决方案 如何实现在多线程下访问共享变量的可见性：也就是实现一个线程修改变量后，对其他线程可见？ 使用 synchronized 关键字加锁 使用 volatile 关键字 其中，volatile 保证不同线程对共享变量操作的可见性，也就是说一个线程修改了 volatile 修饰的变量，当修改写回主内存时，另外一个线程立即看到最新的值。所以，volatile 修饰的变量可以在多线程并发修改下，实现线程间变量的可见性。 ","tags":[],"title":"JAVA内存模型","link":"https://toomi.pages.dev/post/java-nei-cun-mo-xing/","stats":{"text":"4 min read","time":236000,"words":1138,"minutes":4},"dateFormat":"2022-10-27"},{"content":"由于java的CAS同时具有 volatile 读和volatile写的内存语义，因此Java线程之间的通信现在有了下面四种方式： A线程写volatile变量，随后B线程读这个volatile变量。 A线程写volatile变量，随后B线程用CAS更新这个volatile变量。 A线程用CAS更新一个volatile变量，随后B线程用CAS更新这个volatile变量。 A线程用CAS更新一个volatile变量，随后B线程读这个volatile变量。 Java的CAS会使用现代处理器上提供的高效机器级别原子指令，这些原子指令以原子方式对内存执行读-改-写操作，这是在多处理器中实现同步的关键（从 本质上来说，能够支持原子性读-改-写指令的计算机器，是顺序计算图灵机的异步等价机器，因此任何现代的多处理器都会去支持某种能对内存执行原子性读-改 -写操作的原子指令）。同时，volatile变量的读/写和CAS可以实现线程之间的通信。把这些特性整合在一起，就形成了整个concurrent包 得以实现的基石。如果我们仔细分析concurrent包的源代码实现，会发现一个通用化的实现模式： 首先，声明共享变量为volatile； 然后，使用CAS的原子条件更新来实现线程之间的同步； 同时，配合以volatile的读/写和CAS所具有的volatile读和写的内存语义来实现线程之间的通信。 AQS，非阻塞数据结构和原子变量类（java.util.concurrent.atomic包中的类），这些concurrent包中的基础类都是使 用这种模式来实现的，而concurrent包中的高层类又是依赖于这些基础类来实现的。从整体来看，concurrent包的实现示意图如下： synchronized ​ synchronized是一种悲观锁,多线程情况下,当一个线程拿到锁时, 会导致其它所有需要锁的线程挂起 ,阻塞其他线程,只有等持有锁的线程执行完毕或抛出异常释放锁. 线程阻塞和唤醒切换以及用户态到内核态间的切换操作额外浪费消耗cpu资源 .如果一个优先级高的线程等待一个优先级低的线程释放锁会导致优先级倒置，引起性能风险。 CAS CAS:Compare and Swap, 翻译成比较并交换。 java.util.concurrent包中借助CAS实现了区别于synchronized同步锁的一种乐观锁。 CAS是一种乐观锁机制,它不会阻塞线程, 先从内存位置读取到值，然后和预期值比较。如果相等，则将此内存位置的值改为新值 ，返回 true。如果不相等，说明和其他线程冲突了，则不做任何改变，返回 false. 这种机制在不阻塞其他线程的情况下避免了并发冲突，比独占锁的性能高很多 . CAS 本身并未实现失败后的处理机制，它只负责返回成功或失败的布尔值，后续由调用者自行处理。只不过我们最常用的处理方式是重试而已。 本文先从CAS的应用说起，再深入原理解析。 CAS应用 CAS有3个操作数，内存值V，旧的预期值A，要修改的新值B。当且仅当预期值A和内存值V相同时，将内存值V修改为B，否则什么都不做。 非阻塞算法 （nonblocking algorithms） 一个线程的失败或者挂起不应该影响其他线程的失败或挂起的算法。 现代的CPU提供了特殊的指令，可以自动更新共享数据，而且能够检测到其他线程的干扰，而 compareAndSet() 就用这些代替了锁定。 拿出AtomicInteger来研究在没有锁的情况下是如何做到数据正确性的。 AQS AbstractQueuedSynchronizer：叫做抽象队列同步器，它是一个实现了同步器功能的基础框架。 如下图所示，就是一个普通的并发工具同步器应该具有的功能。 （1）首先线程1去同步器获取资源，即获取锁，获取锁成功直接执行业务方法 （2）线程2也获取锁，但是获取锁失败，就会进入等待队列，阻塞等待 （3）当线程1释放锁的时候，需要去唤醒还在等待锁的线程2，然后线程2苏醒之后继续去尝试获取锁。 上面的红色部分，包括资源、获取失败进行等待队列、释放资源唤醒等待队列线程、线程苏醒重新竞争锁，AQS就是封装了这些通用的流程和功能。而获取锁、释放锁是非通用的，留给具体的同步器去实现。比如：ReentrantLock、CountDownLatch、Semaphore等使用了AQS框架的基本机制，然后自己实现了获取锁、释放锁的具体逻辑，这样就形成了不同的同步器了。 AQS内部 从整体上给你说说，AQS内部封装了哪些数据结构和功能，后面再介绍一下这些东西： （1）首先对资源进行了定义，使用一个volatile int state表示资源 （2）规定了获取独占资源的入口为acquire()方法、释放独占资源的入口release() （3）规定了获取共享资源的入口acquireShared()、释放独占资源入口 releaseShared()方法 （4）声明了实际获取资源、释放资源的具体实现方法，AQS这里只是声明，由子类去重写，实现获取和释放的逻辑。实际获取独占资源、实际释放独占资源、实际获取共享资源、实际释放共享资源的入口方法，没有具体的实现，让子类实现这些方法从而形成不同的同步工具，这些抽象方法包括： （4-1）实际获取独占资源的方法，具体实现逻辑封装在子类的tryAcquire()方法内部 （4-2）实际释放独占资源的方法，具体实现逻辑封装在子类的tryRelease()方法内部 （4-3）实际获取共享资源的方法，具体实现逻辑封装在子类的tryAcquireShared()方法内部 （4-4）实际释放共享资源的方法，具体实现逻辑封装在子类的tryReleaseShared()方法内部 （5）封装了一个Node的数据结构，用来存储线程信息。通过多个Node串成一个双向链表的等待队列；链表存储了获取锁失败而进入等待队列的线程 （6）封装了一套非常核心的，线程获取资源失败而如何进入等待队列；以及释放资源之后怎么唤醒等待队列中的线程再次竞争资源的这么一套机制。 这套机制非常核心，基于AQS之上的同步工具类底层都是使用这套机制来实现的。 参考 https://www.cnblogs.com/duanxz/archive/2012/08/08/2628517.html ","tags":[],"title":"concurrent包的实现","link":"https://toomi.pages.dev/post/concurrent-bao-de-shi-xian/","stats":{"text":"7 min read","time":376000,"words":1786,"minutes":7},"dateFormat":"2022-10-27"},{"content":"管理往大了说是搭班子、定战略、带队伍，往小了说就是带领小伙伴完成被分配的项目，所以管理也可以粗暴的被解读为：带领小伙伴完成任务，具体的管理动作也多发生在如何确保任务完成的过程中。 与上述一致，我们先尝试站在全局，看看管理动作隶属于哪一环： 可以看到，管理动作主要解决的是组织能力，所谓组织能力就是我们常说的组织执行力，其三要素是： 员工思维：愿不愿意； 员工能力：会不会； 员工治理：土壤问题； 所以管人，也就是处理这三个领域的问题。 员工思维 员工思维主要关注的是员工愿不愿意做这个事情。 人们进步的驱动力只有两个热爱与上进：热爱是因为喜欢这个事情本身，给不给钱都要做，为爱发电；上进是因为有所求，本质是穷，想赚钱或者想变强以便赚钱。 通常而言【赚钱】和【变强赚更多的钱】是我们做某事的根本驱动力，所以，想要员工有意愿只需要解决任一问题： 事情做了有钱赚； 事情做了能变强； 即需要一条相对公平的上升通道给大家，让大家有盼头。 其次就需要打开信息通道，让公司战略更好的透传下去，执行力的关键还是信息量本身足不足。 员工能力 这个比较简单，无非就是选留育用大法。 选 清晰定位团队发展需要的人才要求，识别并积极吸纳支撑业务发展的优秀人才； 育 了解员工的能力现状，制定人才发展计划，投入精力辅导下属，打造可持续性人才梯队； 用 对团队人才进行合理配置，用人所长，达成团队最佳状态； 关键人才 加强对关键人才的关注，加强辅导沟通，了解其核心诉求，创造发展机会，提供有效激励，保留核心人才； 具体操作层面可以有以下动作： 和下属保持每两月不少于1次的重要沟通，了解员工动态、诉求； 建立新人培养机制，建立导师机制； 加强团队内学习氛围的搭建，主动带头参与部门、公司的培训授课、分享； 定期梳理团队人员的能力/潜力现状(可结合绩效评估)，动态调整人员配置； 团队内文化建设，组织团队活动，投入资源和精力到团队氛围建设中来； 员工治理 员工治理关注三点： 盘活团队资源，关注重点业务，合理进行调配； 提升组织效率，动态优化团队分工，提高组织运作效率，实现对业务发展的有效支持； 流程优化，优化业务流程及决策机制，深入挖掘用户需求，吸纳及优化建议的有效实施，保持产品/业务的快速迭代； 具体操作层面可以有以下动作： 根据业务发展情况，利用月度盘点机制，动态调整资源，做减法保证重点业务资源稳定； 鼓励员工轮岗和换岗，保证资源灵活调整配置; 员工治理核心是解决土壤问题，一方面释放更多资源，一方面激活资源利用率，具体来说就是解决能不能的问题，那么什么是能不能的问题呢？ 比如同学们加班到深夜，制度上不允许点加班餐，这就是能不能的问题； 比如同学们深夜发现一个事故，确实找不到人Code Review，那么就不能处理这个问题，这就是能不能的问题； 比如前端同学发现Bug，主动参与修复，结果追责时候前端同学变成了责任人，这就是能不能的问题。 结语 管理的核心是管人，管人的核心是解决其意愿和能力问题，让同学们想做并且能做，最后是解决环境问题，达到想做、能做、允许做甚至鼓励做的结果，总结如下： 需要建设信息通道，尽量透传战略； 需要建设上升通道，让有能力的人冒头； 管理层需要一套工具，知道所有人能做什么，做得怎样，并做客观评价； 原文链接 微信公众号 ： 叶小钗 ","tags":[],"title":"管理就是管人","link":"https://toomi.pages.dev/post/guan-li-jiu-shi-guan-ren/","stats":{"text":"4 min read","time":236000,"words":1180,"minutes":4},"dateFormat":"2022-10-25"},{"content":"file descriptor(以下简称fd)又叫文件描述符，他是一个抽象的指示符，用一个整数表示(非负整数)。它指向了由系统内核维护的一个file table中的某个条目(entry)。这个解释可能过于抽象，不过在正式详细介绍fd之前，有必要先了解用户程序和系统内核之间的工作过程。 注: 本文描述的所有场景仅限于类unix系统环境，在windows中这玩意叫file handle(臭名昭著的翻译: 句柄)。 User space &amp; Kernel space 现代操作系统会把内存划分为2个区域，分别为Use space(用户空间) 和 Kernel space(内核空间)。用户的程序在User space执行，系统内核在Kernel space中执行。 用户的程序没有权限直接访问硬件资源，但系统内核可以。比如读写本地文件需要访问磁盘，创建socket需要网卡等。因此用户程序想要读写文件，必须要向内核发起读写请求，这个过程叫system call。 内核收到用户程序system call时，负责访问硬件，并把结果返回给程序。 File Descriptor 上面简单介绍了User space和Kernel space，这对于理解fd有很大的帮助。fd会存在，就是因为用户程序无法直接访问硬件，因此当程序向内核发起system call打开一个文件时，在用户进程中必须有一个东西标识着打开的文件，这个东西就是fd。 file tables 和fd相关的一共有3张表，分别是file descriptor、file table、inode table，如下图所示。 file descriptors file descriptors table由用户进程所有，每个进程都有一个这样的表，这里记录了进程打开的文件所代表的fd，fd的值映射到file table中的条目(entry)。 另外，每个进程都会预留3个默认的fd: stdin、stdout、stderr;它们的值分别是0、1，2。 file table file table是全局唯一的表，由系统内核维护。这个表记录了所有进程打开的文件的状态(是否可读、可写等状态)，同时它也映射到inode table中的entry。 inode table inode table同样是全局唯一的，它指向了真正的文件地址(磁盘中的位置)，每个entry全局唯一。 流程 当程序向内核发起system call open()，内核将会 允许程序请求 创建一个entry插入到file table，并返回file descriptor 程序把fd插入到fds中。 当程序再次发起read()system call时，需要把相关的fd传给内核，内核定位到具体的文件(fd –&gt; file table –&gt; inode table)向磁盘发起读取请求，再把读取到的数据返回给程序处理 参考链接 https://wiyi.org/linux-file-descriptor.html ","tags":[],"title":"linux文件描述符","link":"https://toomi.pages.dev/post/linux-wen-jian-miao-shu-fu/","stats":{"text":"3 min read","time":159000,"words":709,"minutes":3},"dateFormat":"2022-10-24"},{"content":"1. Introduction There is a key new feature in redis 5: stream. Stream is a storage structure in the log form, and you can append data into it. It will generate a timestamp ID for each data. And stream also has a convenient model or reading data. So stream is suitable for message queues and time series storage. 2. Usage 2.1 Adding elements to stream redis:6379&gt; XADD mystream * sensor-id 1234 temperature 19.8 1531989605376-0 What we can know from the above are: mystream is the key of stream; the parameter at the location of * is the element ID, and * indicates that an element ID is generated by the system automatically; the added element contains 2 key-value pairs, sensor-id 1234 and temperature 19.8; the returned value is the ID of the newly added element, consisting of a timestamp and an incrementing number. You can also get the number of elements in the Stream: redis:6379&gt; XLEN mystream (integer) 1 2.2 Range query When we want to use a range query, we need to specify the start and end IDs, which is equivalent to giving a time range: redis:6379&gt; XRANGE mystream 1531989605376 1531989605377 1) 1) 1531989605376-0 2) 1) &quot;sensor-id&quot; 2) &quot;1234&quot; 3) &quot;temperature&quot; 4) &quot;19.8&quot; You can use - for the smallest ID and + for the biggest ID: redis:6379&gt; XRANGE mystream - + 1) 1) 1531989605376-0 2) 1) &quot;sensor-id&quot; 2) &quot;1234&quot; 3) &quot;temperature&quot; 4) &quot;19.8&quot; When there are too many elements returned, you can limit the number of returned results, which is just like the paging when querying the database and specify it by the COUNT parameter: redis:6379&gt; XRANGE mystream - + COUNT 2 1) 1) 1531989605376-0 2) 1) &quot;sensor-id&quot; 2) &quot;1234&quot; 3) &quot;temperature&quot; 4) &quot;19.8&quot; You can also use the XREVRANGE command to reverse the query, and the usage is the same as XRANGE. 2.3 Listening for new elements of stream redis:6379&gt; XREAD COUNT 2 STREAMS mystream 0 1) 1) &quot;mystream&quot; 2) 1) 1) 1531989605376-0 2) 1) &quot;sensor-id&quot; 2) &quot;1234&quot; 3) &quot;temperature&quot; 4) &quot;19.8&quot; The mystream after STREAMS specifies the key of the target stream; 0 is the smallest ID, and we need to obtain the elements that is greater than the specified ID in the specified stream; COUNT refers to the number of the elements we want to obtain. Multiple streams can be specified together, such as STREAMS mystream otherstream 0 0. 2.3.1 Blocking listener If you execute the following in client 1: redis:6379&gt; XREAD BLOCK 0 STREAMS mystream $ it will enter the waiting state. And if you add elements to client 2: redis:6379&gt; XADD mystream * test 1 the elements just added will be displayed in client 1: 1) 1) &quot;mystream&quot; 2) 1) 1) 1531994510562-0 2) 1) &quot;test&quot; 2) &quot;1&quot; 0 is the specified timeout, so 0 means never timeout here; $ means the maximum ID in the stream. 2.4 Consumer Group When the amount of stream is very large, or when the consumer processing is very time consuming, it'll under greater pressure if there is only one consumer. Thus redis stream provides the concept of the consumer group, allowing multiple consumers to process the same stream to implement load balancing. For example, if there are 3 consumers C1, C2, and C3, and there are 7 message elements in the stream, then the allocation for the consumers is: 1 -&gt; C1 2 -&gt; C2 3 -&gt; C3 4 -&gt; C1 5 -&gt; C2 6 -&gt; C3 7 -&gt; C1 原文连接 https://web.archive.org/web/20190217140100/https://www.tutorialdocs.com/article/redis-stream-tutorial.html https://dev.to/bajena/removing-n-oldest-entries-from-a-redis-stream-5aob ","tags":[],"title":"Redis Stream Tutorial In 20 Minutes","link":"https://toomi.pages.dev/post/redis-stream-tutorial-in-20-minutes/","stats":{"text":"5 min read","time":252000,"words":675,"minutes":5},"dateFormat":"2022-10-22"},{"content":"mozjpeg是一个来自Mozilla实验室的JPEG图像编码器项目，目标是在不降低图像质量且兼容主流的解码器的情况下，提供产品级的JPEG格式编码器来提高压缩率以减小JPEG文件的大小。 Mozilla指出，这些年来，网站的图片使用数量和大小都在与日俱增，而HTML、JS和CSS文件大小都相对减小了。也就是说在页面加载的过程中，图片占用了大量的网络流量。所以减小图片的大小可以显著优化页面加载速度。当然使用压缩比更高的替代图像格式（比如WebP）也是一种解决方案，但是这些新的图片格式都存在兼容性的问题。在内部讨论研究后，Mozilla认为JPEG仍可以进一步压缩，于是他们就开始了mozjpeg项目。 下载 二进制： cjpeg.exe -quality 5 banner.png &gt; xxx_1.jpg mozjpeg-v4.0.3-win-x64.zip gui mozjpeg-gui.exe ","tags":[],"title":"图片压缩工具","link":"https://toomi.pages.dev/post/tu-pian-ya-suo-gong-ju/","stats":{"text":"1 min read","time":55000,"words":253,"minutes":1},"dateFormat":"2022-10-22"},{"content":"Java 实现对象深拷贝的方式有两种 序列化 实现类的clone方法 下文介绍通过序列化实现通用对象深拷贝 引入依赖 需要依赖外部工具包 Apache Commons Lang &lt;dependency&gt; &lt;groupId&gt;commons-lang&lt;/groupId&gt; &lt;artifactId&gt;commons-lang&lt;/artifactId&gt; &lt;version&gt;2.6&lt;/version&gt; &lt;/dependency&gt; 范例 类对象,都需要实现Serializable public class Account implements Serializable { private static final long serialVersionUID = 1L; private List&lt;Email&gt; emails; public void setEmails(List&lt;Email&gt; emails) { this.emails = emails; } public List&lt;Email&gt; getEmails() { return emails; } } public class Email implements Serializable { private static final long serialVersionUID = 1L; private String email; public Email() { } public Email(String email) { this.email = email; } public String getEmail() { return email; } public void setEmail(String email) { this.email = email; } @Override public String toString() { return email; } // generated by Eclipse @Override public int hashCode() { final int prime = 31; int result = 1; result = prime * result + ((email == null) ? 0 : email.hashCode()); return result; } // generated by Eclipse @Override public boolean equals(Object obj) { if (this == obj) return true; if (obj == null) return false; if (getClass() != obj.getClass()) return false; Email other = (Email) obj; if (email == null) { if (other.email != null) return false; } else if (!email.equals(other.email)) return false; return true; } } 测试用例 @Test public void copy() { // 构建复杂对象1 Account accountForm = new Account(); List&lt;Email&gt; emailsSrc = new ArrayList&lt;Email&gt;(); emailsSrc.add(new Email(&quot;a@example.com&quot;)); emailsSrc.add(new Email(&quot;b@example.com&quot;)); emailsSrc.add(new Email(&quot;c@example.com&quot;)); accountForm.setEmails(emailsSrc); // 深复制出新对象2 Account account = SerializationUtils.clone(accountForm); // 修改原来的对象1属性值 emailsSrc.get(0).setEmail(&quot;toomi&quot;); // 后面能比对,原始对象1的属性被修改 assert(accountForm.getEmails().get(0).getEmail().equals(&quot;toomi&quot;)); // 复制出来的对象2属性没有修改，复制不是内存地址引用，前后对象是独立的 assert(account.getEmails().get(0).getEmail().equals(&quot;a@example.com&quot;)); } ","tags":[],"title":"java深拷贝","link":"https://toomi.pages.dev/post/java-shen-kao-bei/","stats":{"text":"3 min read","time":129000,"words":409,"minutes":3},"dateFormat":"2022-10-21"},{"content":"HTTP/0.9 1990年问世，那时的HTTP并没有作为正式的标准被建立，这时的HTTP其实含有HTTP/1.0之前版本的意思，那时候还有严重设计缺陷，只支持GET方法，很快被HTTP/1.0取代。 并且协议还规定，服务器只能回应HTML格式的字符串，不能回应别的格式，当服务器发送完毕，就关闭TCP连接。 HTTP/1.0 特点 任何格式的内容都可以发送。这使得互联网不仅可以传输文字，还能传输图像、视频、二进制文件。这为互联网的大发展奠定了基础。 除了GET命令，还引入了POST命令和HEAD命令，丰富了浏览器与服务器的互动手段。 HTTP请求和回应的格式也变了。除了数据部分，每次通信都必须包括头信息（HTTP header），用来描述一些元数据。 其他的新增功能还包括状态码（status code）、多字符集支持、多部分发送（multi-part type）、权限（authorization）、缓存（cache）、内容编码（content encoding）等。 不足 HTTP/1.0 版的主要缺点是，每个TCP连接只能发送一个请求。发送数据完毕，连接就关闭，如果还要请求其他资源，就必须再新建一个连接。TCP连接的新建成本很高，因为需要客户端和服务器三次握手，并且开始时发送速率较慢（slow start）。 所以，HTTP 1.0版本的性能比较差。随着网页加载的外部资源越来越多，这个问题就愈发突出了。 HTTP/1.1 特点 引入了持久连接（persistent connection），即TCP连接默认不关闭，可以被多个请求复用，不用声明Connection: keep-alive。客户端和服务器发现对方一段时间没有活动，就可以主动关闭连接。不过，规范的做法是，客户端在最后一个请求时，发送Connection: close，明确要求服务器关闭TCP连接 引入了管道机制（pipelining），即在同一个TCP连接里面，客户端可以同时发送多个请求。这样就进一步改进了HTTP协议的效率。举例来说，客户端需要请求两个资源。以前的做法是，在同一个TCP连接里面，先发送A请求，然后等待服务器做出回应，收到后再发出B请求。管道机制则是允许浏览器同时发出A请求和B请求，但是服务器还是按照顺序，先回应A请求，完成后再回应B请求。 将Content-length字段的作用进行扩充，即声明本次回应的数据长度（一个TCP连接现在可以传送多个回应，势必就要有一种机制，区分数据包是属于哪一个回应的） 采用分块传输编码，对于一些很耗时的动态操作，服务器需要等到所有操作完成，才能发送数据，显然这样的效率不高。更好的处理方法是，产生一块数据，就发送一块，采用&quot;流模式&quot;（stream）取代&quot;缓存模式&quot;（buffer） 1.1版还新增了许多动词方法：PUT、PATCH、HEAD、 OPTIONS、DELETE。另外，客户端请求的头信息新增了Host字段，用来指定服务器的域名 不足 虽然1.1版允许复用TCP连接，但是同一个TCP连接里面，所有的数据通信是按次序进行的。服务器只有处理完一个回应，才会进行下一个回应。要是前面的回应特别慢，后面就会有许多请求排队等着。这称为&quot;队头堵塞&quot;（Head-of-line blocking）。为了避免这个问题，只有两种方法：一是减少请求数，二是同时多开持久连接。这导致了很多的网页优化技巧，比如合并脚本和样式表、将图片嵌入CSS代码、域名分片（domain sharding）等等。如果HTTP协议设计得更好一些，这些额外的工作是可以避免的。 HTTP/2.0 简单来说，HTTP/2（超文本传输协议第2版，最初命名为HTTP2.0），是HTTP协议的第二个主要版本。HTTP/2是HTTP协议自1999年HTTP1.1发布后的首个更新，主要基于SPDY协议。 HTTP2.0的特点是：在不改动HTTP语义、方法、状态码、URI及首部字段的情况下，大幅度提高了web性能。 HTTP3.0 谷歌开发的QUIC协议，利用UDP实现可靠数据传输。 ","tags":[],"title":"http版本演变","link":"https://toomi.pages.dev/post/http-ban-ben-yan-bian/","stats":{"text":"5 min read","time":252000,"words":1162,"minutes":5},"dateFormat":"2022-10-20"},{"content":"定义 非对称加密算法指的是 加、解密使用不同的密钥，一把为公开的公钥，另一把为私钥。 公钥加密的内容只能由私钥进行解密，反之由私钥加密的内容只能由公钥进行解密。也就是说，这一对公钥、私钥都可以用来加密和解密，并且一方加密的内容只能由对方进行解密。 加密：公钥加密，私钥解密的过程，称为「加密」 因为公钥是公开的，任何公钥持有者都可以将想要发送给私钥持有者的信息进行加密后发送，而这个信息只有私钥持有者才能解密。 签名：私钥加密，公钥解密的过程，称为「签名」 它和加密有什么区别呢？因为公钥是公开的，所以任何持有公钥的人都能解密私钥加密过的密文，所以这个过程并不能保证消息的安全性，但是它却能保证消息来源的准确性和不可否认性，也就是说，如果使用公钥能正常解密某一个密文，那么就能证明这段密文一定是由私钥持有者发布的，而不是其他第三方发布的，并且私钥持有者不能否认他曾经发布过该消息。故此将该过程称为「签名」。 RSA是1977年由罗纳德·李维斯特（Ron Rivest）、阿迪·萨莫尔（Adi Shamir）和伦纳德·阿德曼（Leonard Adleman）一起提出的。当时他们三人都在麻省理工学院工作。RSA就是他们三人姓氏开头字母拼在一起组成的。1973年，在英国政府通讯总部工作的数学家克利福德·柯克斯（Clifford Cocks）在一个内部文件中提出了一个相同的算法，但他的发现被列入机密，一直到1997年才被发表。 对极大整数做因数分解的难度决定了RSA算法的可靠性。 换言之，对一极大整数做因数分解愈困难，RSA算法愈可靠。假如有人找到一种快速因数分解的算法的话，那么用RSA加密的信息的可靠性就肯定会极度下降。 但找到这样的算法的可能性是非常小的。今天只有短的RSA钥匙才可能被强力方式解破。到目前为止，世界上还没有任何可靠的攻击RSA算法的方式。只要其钥匙的长度足够长，用RSA加密的信息实际上是不能被解破的。 举一个通俗易懂的例子 看一个数学小魔术： 让A写下一个任意3位数，并将这个数和91相乘；然后将积的最后三位数告诉B，这样B就可以计算出A写下的是什么数字了,比如 : A写下的是123 (需加密内容)，并且A计算出123 * 91 (B的公钥加密)等于11193，并把结果的末三位193(加密结果)告诉B。 B只需要把193再乘以11 (B的私钥)，193 * 11 = 2123 末三位123（解密结果）就是A写下的数字了。 道理很简单，91乘以11等于1001，而任何一个三位数乘以1001后，末三位显然都不变（例如123乘以1001就等于123123）。 知道原理后，可以构造一个定义域和值域更大的加密解密系统。 例如： 任意一个数乘以400000001后，末8位都不变，而400000001 = 19801 * 20201。于是A来乘以19801，B来乘以20201，又一个加密解密不对称的系统就构造好了； 甚至可以构造得更大一些 4000000000000000000000000000001 = 1199481995446957 * 3334772856269093，这样我们就成功构造了一个30位的加密系统； 如果仅仅按照上面的思路，如果对方知道原理，非常容易穷举出400000001这个目标值；RSA算法使用的是指数和取模运算，本质上就是上面这套思想。 ","tags":[],"title":"非对称加密算法","link":"https://toomi.pages.dev/post/fei-dui-cheng-jia-mi-suan-fa/","stats":{"text":"4 min read","time":211000,"words":1014,"minutes":4},"dateFormat":"2022-10-18"},{"content":"相关背景知识 要说清楚 HTTPS 协议的实现原理，至少需要如下几个背景知识。 大致了解几个基本术语（HTTPS、SSL、TLS）的含义 大致了解 HTTP 和 TCP 的关系（尤其是“短连接”VS“长连接”） 大致了解加密算法的概念（尤其是“对称加密与非对称加密”的区别） 大致了解 CA 证书的用途 先澄清几个术语——HTTPS、SSL、TLS “HTTP”是干嘛用滴？ 首先，HTTP 是一个网络协议，是专门用来帮你传输 Web 内容滴。关于这个协议，就算你不了解，至少也听说过吧？比如你访问俺的博客的主页，浏览器地址栏会出现如下的网址： http://program-think.aa.com/ 俺加了粗体的部分就是指 HTTP 协议。大部分网站都是通过 HTTP 协议来传输 Web 页面、以及 Web 页面上包含的各种东东（图片、CSS 样式、JS 脚本）。 （注：当年写这篇的时候，Google 的 blogspot 博客平台【尚未】支持全站 HTTPS，所以在上述举例中，主页的网址以 http:// 开头） “SSL/TLS”是干嘛用滴？ SSL 是洋文“Secure Sockets Layer”的缩写，中文叫做“安全套接层”。它是在上世纪90年代中期，由网景公司设计的。（顺便插一句，网景公司不光发明了 SSL，还发明了很多 Web 的基础设施——比如“CSS 样式表”和“JS 脚本”） 为啥要发明 SSL 这个协议捏？因为原先互联网上使用的 HTTP 协议是明文的，存在很多缺点——比如传输内容会被偷窥（嗅探）和篡改。发明 SSL 协议，就是为了解决这些问题。 到了1999年，SSL 因为应用广泛，已经成为互联网上的事实标准。IETF 就在那年把 SSL 标准化。标准化之后的名称改为 TLS（是“Transport Layer Security”的缩写），中文叫做“传输层安全协议”。 很多相关的文章都把这两者并列称呼（SSL/TLS），因为这两者可以视作同一个东西的不同阶段。 3. “HTTPS”是啥意思？ 解释完 HTTP 和 SSL/TLS，现在就可以来解释 HTTPS 啦。咱们通常所说的 HTTPS 协议，说白了就是“HTTP 协议”和“SSL/TLS 协议”的组合。你可以把 HTTPS 大致理解为——“HTTP over SSL”或“HTTP over TLS”（反正 SSL 和 TLS 差不多，你可以把这俩当作同义词）。 再来说说 HTTP 协议的特点 作为背景知识介绍，还需要再稍微谈一下 HTTP 协议本身的特点。HTTP 本身有很多特点，考虑到篇幅有限，俺只谈那些和 HTTPS 相关的特点。 1. HTTP 的版本和历史 如今咱们用的 HTTP 协议，版本号是 1.1（也就是 HTTP 1.1）。这个 1.1 版本是1995年底开始起草的（技术文档是 RFC2068），并在1999年正式发布（技术文档是 RFC2616）。 在 1.1 之前，还有曾经出现过两个版本“0.9 和 1.0”，其中的 HTTP 0.9 【没有】被广泛使用，而 HTTP 1.0 被广泛使用过。 另外，据说明年（2015）IETF 就要发布 HTTP 2.0 的标准了。俺拭目以待。 2. HTTP 和 TCP 之间的关系 简单地说，TCP 协议是 HTTP 协议的基石——HTTP 协议需要依靠 TCP 协议来传输数据。 在网络分层模型中，TCP 被称为“传输层协议”，而 HTTP 被称为“应用层协议”。有很多常见的应用层协议是以 TCP 为基础的，比如“FTP、SMTP、POP、IMAP”等。 TCP 被称为“面向连接”的传输层协议。关于它的具体细节，俺就不展开了（否则篇幅又失控了）。你只需知道：传输层主要有两个协议，分别是 TCP 和 UDP。TCP 比 UDP 更可靠。你可以把 TCP 协议想象成某个水管，发送端这头进水，接收端那头就出水。并且 TCP 协议能够确保，先发送的数据先到达（与之相反，UDP 不保证这点）。 3. HTTP 协议如何使用 TCP 连接？ HTTP 对 TCP 连接的使用，分为两种方式：俗称“短连接”和“长连接”（“长连接”又称“持久连接”，洋文叫做“Keep-Alive”或“Persistent Connection”） 假设有一个网页，里面包含好多图片，还包含好多【外部的】CSS 文件和 JS 文件。在“短连接”的模式下，浏览器会先发起一个 TCP 连接，拿到该网页的 HTML 源代码（拿到 HTML 之后，这个 TCP 连接就关闭了）。然后，浏览器开始分析这个网页的源码，知道这个页面包含很多外部资源（图片、CSS、JS）。然后针对【每一个】外部资源，再分别发起一个个 TCP 连接，把这些文件获取到本地（同样的，每抓取一个外部资源后，相应的 TCP 就断开） 相反，如果是“长连接”的方式，浏览器也会先发起一个 TCP 连接去抓取页面。但是抓取页面之后，该 TCP 连接并不会立即关闭，而是暂时先保持着（所谓的“Keep-Alive”）。然后浏览器分析 HTML 源码之后，发现有很多外部资源，就用刚才那个 TCP 连接去抓取此页面的外部资源。 在 HTTP 1.0 版本，【默认】使用的是“短连接”（那时候是 Web 诞生初期，网页相对简单，“短连接”的问题不大）； 到了1995年底开始制定 HTTP 1.1 草案的时候，网页已经开始变得复杂（网页内的图片、脚本越来越多了）。这时候再用短连接的方式，效率太低下了（因为建立 TCP 连接是有“时间成本”和“CPU 成本”滴）。所以，在 HTTP 1.1 中，【默认】采用的是【Keep-Alive】的方式。 【HTTP Keep-Alive】有时候也叫做“HTTP persistent connection”或“HTTP connection reuse”。关于它的更多介绍，可以参见维基百科词条（在“这里”） 原文链接 https://program-think/2014/11/https-ssl-tls-1.html ","tags":[],"title":"HTTPS 和 SSL/TLS 协议[1]","link":"https://toomi.pages.dev/post/https-he-ssltls-xie-yi-1/","stats":{"text":"6 min read","time":336000,"words":1517,"minutes":6},"dateFormat":"2022-10-18"},{"content":"inurl:clash/proxies ","tags":[],"title":"节点搜索","link":"https://toomi.pages.dev/post/jie-dian-sou-suo/","stats":{"text":"1 min read","time":1000,"words":3,"minutes":1},"dateFormat":"2022-10-15"},{"content":"在这篇文章，将介绍从建立一个表单到发布到EBS服务器的过程。 Forms建立流程刚要 利用Forms Builder建立forms源文件 将forms源文件编译成可执行文件 将可执行文件注册到EBS应用服务器 a. 将表单与可执行文件关联 b. 建立一个功能，并将功能与表单关联 c. 将表单挂载到菜单 d. 将菜单分配给职责 e. 将职责分配给用户 基于TEMPLATE.fmb建立一个表单 TEMPLATE.fmb包含许多可调用的模块，基于该模板开发可使表单具有同样的外观，并加快开发速度。开发步骤如下： 打开Form Builder,选择菜单File-&gt;New-&gt;Form using Template 2. 找到模板所在的目录，并打开该模板。打开后，应把该表单另存其他名字，本次例子中把该表单命名为``EMP_FORM_MASTER_DET.fmb``这样才不会影响到模板文件。 3. 删除模板自带对象。 TEMPLATE自带的对象,比如Data Block里面的BLOCKNAME和DETAIL BLOCK，Canvas里面的BLOCKNAME,以及Windows里面的BLOCKNAME.通常开始时把他们都删除掉，以便对新的开发有影响，其实也可以不删除直接修改。这里都删除掉。 新建一个窗口(windows).删除掉无关的对象后，我们需要新建一个窗口，并命名为WIN_M.我们选中对象浏览器中Windows节点，然后点击工具栏中的+按钮。并设置它的subclass为WINDOW(propertyclass)，title为自己想显示的任意文字。 5. 新建一个画布(Canvas).选中Canvas节点来新建一个Canvas，然后点击工具栏中的+按钮。Canvas是放在Window中用来放置要显示的Item的，至少要有一个ITEM。设置画布相应的Name, Subclassinformation 和Window属性如下图。 子类信息如下 6. 现在回头修改刚才新建的Window上的Primary Canvas属性为刚才新建的Canvas(MAIN_C). 7. 修改Form level的trigger PRE-FORM的内容，来指定Form执行的时的第一个Window为我们刚才新建的WIN_M如下图。 8. 修改Program Unit中的CUSOTM package body中的close_windowprocedure，指定在关闭窗户时的处理。CUSOTM.close_window会被form level的CLOSE_WINDOW trigger调用。 9. 现在Form已经建立好了，但实际上上面啥都没有，现在需要建立Block和Item以便能在Cavas上显示出东西来。选中对象浏览器上的DataBlock节点，点击+新建一个Data Block,这里做一个不访问数据库的最简单的block，所以选择’Build a new block manually’,并设置属性如下 10. 添加数据项。在刚才建立的数据块中建立一个数据项，用来显示在画布上。 11. 查看画布。双击需要预览的画布，我们可以在进行本机预览。 将表单上传到EBS服务器进行编译。 在建立一个表单后，相当于编写了源代码。现需要把源代码编译成执行文件。因为表单具有平台相关性，我们需要把表单上传到EBS的Linux服务器进行编译。 用FTP软件将表单文件上传到服务器。 a. 上传到指定目录(Product_TOP) - $AU_TOP/forms/lang 代表forms源文件目录 - $AU_TOP/resource 代表库目录（pll/plx） - $FND_TOP/forms/lang 代表FND这个应用的forms可执行文件目录 用Telnet(SSH)软件登陆Linux后台，运行编译命令。(必须注意，先到$AU_TOP/forms/ZHS目录下执行编译命令，这样才能引用库文件，不然将出错) 编译命令的格式为 frmcmp_batch Module=fmb源文件所在路径 Userid=数据库用户名/数据库密码 Output_File =可执行文件存放路径 例如需要将$FND_TOP/forms/ZHS路径下的EMP_FORM_MASTER_DET.fmb文件编译并放到$FND_TOP/forms/ZHS下，EBS应用服务器对应的数据库应用名和密码都是apps则在$AU_TOP/forms/ZHS下运行如下命令 frmcmp_batch Module=$AU_TOP/forms/ZHS/EMP_FORM_MASTER_DET.fmb Userid=apps/apps Output_File =$FND_TOP/forms/ZHS/EMP_FORM_MASTER_DET.fmx 将表单注册到EBS上(功能名称等不要用中文，否则会出错) 刚仅仅是将可执行文件放到了EBS服务器文件目录上，从EBS界面上是看不见，无法调用该表单的，现需要把表单注册到EBS，以便在界面运行表单。 建立一个表单 路径：应用开发员--&gt;应用产品--&gt;表单 表单 刚编译的可执行程序EMP_FORM_MASTER_DET~~.fmx~~ 应用 fnd对应的程序名称即应用对象程序库(因为可执行文件放在$FND_TOP下面，所以这里选择FND对应的应用。如果放在$BHSC_TOP下应该选择BHSC对应的应用) 用户表单名 MASTER_DET_FOMRS（待会需要用到） 将表单与功能关联 路径：应用开发员--&gt;应用产品--&gt;功能 填写说明便签页信息 功能 主从表 (功能名称等不要用中文，否则会出错，这里仅仅作演示) 用户功能名 MASTER_DET_FUNCTION（待会用到） 填写特性标签页信息 功能 主从表 类型 表单 填写表单标签页信息 功能 主从表 表单 建立表单时填写的用户表单名 MASTER_DET_FOMRS 建立菜单 应用开发员--&gt;应用产品--&gt;菜单 菜单 表单测试菜单 用户菜单名 表单测试菜单1 功能 刚建立功能时的用户功能名 MASTER_DET_FUNCTION 将菜单分配给责任 系统管理员--&gt;安全性--&gt;定义 责任名 表单测试责任1 应用产品 应用对象程序库 菜单 刚定义的用户菜单名(表单测试菜单1) 最后就已经完成EBS表单注册。 参考 http://blog.csdn.net/tavor/article/details/18416375 ","tags":[],"title":"Oracle EBS 第一个Form表单","link":"https://toomi.pages.dev/post/oracle-ebs-di-yi-ge-form-biao-dan/","stats":{"text":"6 min read","time":317000,"words":1439,"minutes":6},"dateFormat":"2022-09-29"},{"content":"本文主要介绍Oracle EBS(Oracle E-Business Suite), 也成为 oracle application。 包括内容有 EBS系统结构 EBS系统导航，即在EBS界面进行操作 EBS表单客户化 EBS报表客户化 EBS系统结构 EBS系统使用了传统的三层架构模式(客户端、服务器端、数据库端)。 客户端：即我们所用的网页浏览器 服务器端：即EBS应用服务器 数据库端：即EBS数据库服务器 一般来说。EBS数据库服务器和EBS应用服务器安装在不同的主机上。普通用户操作EBS系统主要是使用浏览器访问EBS应用服务器，在应用服务器上进行数据查询等操作，EBS应用服务器向数据库服务器提交数据操作请求，数据库服务器请处理完EBS应用服务器的请求后将结果返回应用服务器，最后应用服务器把数据返回客户端，展现在用户界面上。 举个栗子： 假设EBS有一个表单，这个表单是建立于数据库的EMP表上，普通用户可以通过浏览器登陆EBS应用服务器，导航到那个表单，进行查询就可以查询到emp表的数据。但是对于开发人员，可以直接登陆到数据库服务器，然后对数据库进行查询操作。 EBS基础概念 在EBS中首先接触的概念是(用户、职责、菜单、功能)。用户就是登陆EBS时使用的用户名，EBS根据用户名识别一个用户。一个用户关联一个或多个职责，一个职责关联一个或多个菜单，一个菜单关联一个或多个功能。 例如，一个人事部门职员(用户)有人资管理职责，人资管理职责包含着人员管理和招聘管理菜单，人员管理菜单下有薪水管理和合同管理功能。 功能才是EBS系统的基本操作单位，其他都是一个逻辑概念，职能名称和菜单名称可以自命名，没有限制。就像职责是文件夹，菜单是子文件夹，功能才是我们能打开的文件。 当用户登录进ebs后，看见的界面类似下图 ","tags":[],"title":"oracle ebs 简介","link":"https://toomi.pages.dev/post/oracle-ebs-jian-jie/","stats":{"text":"3 min read","time":124000,"words":595,"minutes":3},"dateFormat":"2022-09-29"},{"content":"apex文件上传流程如下: 创建类型为 file browser 的页面项 提交页面后,apex自动将文件上传到临时表 APEX_APPLICATION_TEMP_FILES 中 将临时表APEX_APPLICATION_TEMP_FILES 中的数据报存到自己的正式表中 创建自定义表 创建一个自定义的表,用于保存上传的文件 create table bhsc_files as select * from APEX_APPLICATION_TEMP_FILES where 1=2; 创建页面项(page item) 创建页面项,type为 file browser Storage Type 设为 Table APEX_APPLICATION_TEMP_FILES Purge File at 设为 End of Request 创建页面Processing 下一步就是需要将APEX_APPLICATION_TEMP_FILES表中的数据保存起来,创建一个processing,语句如下 begin insert into bhsc_files select * from APEX_APPLICATION_TEMP_FILES; end; 创建提交页面的button 最后创建一个提交页面的按钮,提交页面后就保存到数据库了。 附件下载 跳转到共享组件(shared components) --&gt; Application Processes 创建一个名为 APP_FILE_ID的Application item , scope为 Application,Session State Protection为 Unrestricted 创建一个名为 APP_DOWNLOAD_FILE,Process Point为 Ajax Callback的过程，代码如下： begin for c1 in (select * from bhsc_files where id = :APP_FILE_ID) loop sys.htp.init; sys.owa_util.mime_header( c1.mime_type, FALSE ); sys.htp.p('Content-length: ' || sys.dbms_lob.getlength( c1.blob_content)); sys.htp.p('Content-Disposition: attachment; filename=&quot;' || c1.filename || '&quot;' ); sys.htp.p('Cache-Control: max-age=3600'); -- tell the browser to cache for one hour, adjust as necessary sys.owa_util.http_header_close; sys.wpg_docload.download_file( c1.blob_content ); apex_application.stop_apex_engine; end loop; end; 在页面上创建一个区域(region)，类型为 classic report,语句如下： select id,name,filename,null download_link from bhsc_files 设置download_link字段的类型为 link,target为 page in this application,page为 0,set item设置 APP_FILE_ID为 #ID#,advanced那里设置request值为 APPLICATION_PROCESS=APP_DOWNLOAD_FILE 具体信息如下 附件删除 附件删除的步骤与附件下载相同，一样是创建一个名为 APP_DELETE_FILE的Application Processes,代码如下 begin for c1 in (select * from bhsc_files where id = :APP_FILE_ID) loop delete from bhsc_files where id = :APP_FILE_ID; end loop; end; 在下载报表那里加多一个字段，字段设置为link，request值为 APPLICATION_PROCESS=APP_DELETE_FILE ","tags":[],"title":"Oracle apex 上传图片","link":"https://toomi.pages.dev/post/oracle-apex-shang-chuan-tu-pian/","stats":{"text":"3 min read","time":126000,"words":468,"minutes":3},"dateFormat":"2022-09-29"},{"content":"👏 历程 大学期间用过Jekyll，但是由于其基于 ruby ，环境安装麻烦，不适合多设备写作，后放弃 后续用hexo , 其基于 node js ， 环境安装依旧麻烦，后放弃 现在目前用的是Gridea ，其提供了 编辑器，服务发布 两项重要功能 目前确实让markdown写作更容易了，仅仅需要下载 Gridea ，并用git对 Gridea 源文件进行同步，就可以满足多设备写作 i am back ","tags":[],"title":"i am back ","link":"https://toomi.pages.dev/post/i-am-back/","stats":{"text":"1 min read","time":26000,"words":123,"minutes":1},"dateFormat":"2022-09-29"}]}
